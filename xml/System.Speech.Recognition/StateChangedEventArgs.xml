<Type Name="StateChangedEventArgs" FullName="System.Speech.Recognition.StateChangedEventArgs">
  <Metadata><Meta Name="ms.openlocfilehash" Value="a16e2c2067bef08c28919b4e0a2d158746ed385c" /><Meta Name="ms.sourcegitcommit" Value="b53d35b4a410c949742abd4c6a989d1af5357bca" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="de-DE" /><Meta Name="ms.lasthandoff" Value="07/21/2020" /><Meta Name="ms.locfileid" Value="86589929" /></Metadata><TypeSignature Language="C#" Value="public class StateChangedEventArgs : EventArgs" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit StateChangedEventArgs extends System.EventArgs" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.StateChangedEventArgs" />
  <TypeSignature Language="VB.NET" Value="Public Class StateChangedEventArgs&#xA;Inherits EventArgs" />
  <TypeSignature Language="C++ CLI" Value="public ref class StateChangedEventArgs : EventArgs" />
  <TypeSignature Language="F#" Value="type StateChangedEventArgs = class&#xA;    inherit EventArgs" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.EventArgs</BaseTypeName>
  </Base>
  <Interfaces />
  <Docs>
    <summary>Gibt Daten von dem <see cref="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />-Ereignis zurück.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Das- <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> Ereignis wird von der- <xref:System.Speech.Recognition.SpeechRecognizer> Klasse ausgelöst. <xref:System.Speech.Recognition.StateChangedEventArgs> wird von abgeleitet <xref:System.EventArgs> und für-Ereignisse an Handler übermittelt <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> .  
  
 <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> ist eine schreibgeschützte Eigenschaft. Der Zustand eines freigegebenen sprach Erkennungs Moduls kann nicht Programm gesteuert geändert werden. Benutzer können den Zustand der freigegebenen Spracherkennung mithilfe der sprach Erkennungs-Benutzeroberfläche oder über das **sprach Erkennungs** Element der Windows- **Systemsteuerung**ändern.  
  
 Die Einstellungen **on** und **Sleep** in der sprach Erkennungs Benutzeroberfläche entsprechen dem `Listening` Status. Die Einstellung **aus** in der Benutzeroberfläche für die Spracherkennung entspricht <xref:System.Speech.Recognition.RecognizerState.Stopped> .  
  
   
  
## Examples  
 Im folgenden Beispiel wird eine freigegebene Spracherkennung erstellt, und anschließend werden zwei Arten von Grammatiken zum Erkennen bestimmter Wörter und zum Akzeptieren der kostenlosen Diktat Erstellung erstellt. Im Beispiel werden alle erstellten Grammatiken asynchron in die Erkennung geladen.  Ein Handler für das- <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> Ereignis verwendet die- <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> Methode, um die Windows-Erkennung in den Modus "lauschen" zu versetzen.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted += new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized += new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Add a handler for the StateChanged event.  
      recognizer.StateChanged += new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Create "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yah}" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "nah" });  
      SemanticResultValue noValue = new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void  recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
     if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // Write the grammar name and the text of the recognized phrase to the console.  
    static void  recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
     Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void  recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
     string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
      }  
  
      // Add exception handling code here.  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  }  
}  
  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
    <altmember cref="T:System.Speech.Recognition.RecognizerState" />
  </Docs>
  <Members>
    <Member MemberName="RecognizerState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerState RecognizerState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.RecognizerState RecognizerState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.StateChangedEventArgs.RecognizerState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerState As RecognizerState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerState RecognizerState { System::Speech::Recognition::RecognizerState get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerState : System.Speech.Recognition.RecognizerState" Usage="System.Speech.Recognition.StateChangedEventArgs.RecognizerState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName Language="C#">[get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")]</AttributeName>
          <AttributeName Language="F#">[&lt;get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den aktuellen Zustand der freigegebenen Spracherkennungs-Engine in Windows ab.</summary>
        <value>Eine <see cref="T:System.Speech.Recognition.RecognizerState" />-Instanz, die angibt, ob der Zustand einer freigegebenen Spracherkennungs-Engine <see langword="Listening" /> oder <see langword="Stopped" /> ist.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Im folgenden Beispiel wird eine Anzeige auf der Grundlage der von einer-Instanz bereitgestellten Zustandsinformationen aktualisiert <xref:System.Speech.Recognition.RecognizerState> , die von der-Eigenschaft einer-Instanz abgerufen wird, die <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> <xref:System.Speech.Recognition.StateChangedEventArgs> an einen-Handler für ein-Ereignis weitergegeben wird <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged>  
  
```  
  
// Make sure that _recognizer and recognition start buttons are disabled if state is stopped.  
// Re-enable the start button to allow manual re-enable if the speech recognizer is listening.  
_recognizer.StateChanged +=  
  delegate(object sender, StateChangedEventArgs eventArgs)   
{  
  _recognizerStateLabel.Text = "Speech Recognizer State: " + eventArgs.RecognizerState.ToString();  
};  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
  </Members>
</Type>
