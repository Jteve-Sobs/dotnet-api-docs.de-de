<Type Name="RecognitionResult" FullName="System.Speech.Recognition.RecognitionResult">
  <Metadata>
    <Meta Name="ms.openlocfilehash" Value="58dc7fd01cd9580179bf403a5d3c0b372c8b28ee" />
    <Meta Name="ms.sourcegitcommit" Value="d31dc2ede16f6f7bc64e90d9f897ff54c4e3869b" />
    <Meta Name="ms.translationtype" Value="HT" />
    <Meta Name="ms.contentlocale" Value="de-DE" />
    <Meta Name="ms.lasthandoff" Value="04/03/2018" />
    <Meta Name="ms.locfileid" Value="30528864" />
  </Metadata>
  <TypeSignature Language="C#" Value="public sealed class RecognitionResult : System.Speech.Recognition.RecognizedPhrase, System.Runtime.Serialization.ISerializable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi serializable sealed beforefieldinit RecognitionResult extends System.Speech.Recognition.RecognizedPhrase implements class System.Runtime.Serialization.ISerializable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.RecognitionResult" />
  <TypeSignature Language="VB.NET" Value="Public NotInheritable Class RecognitionResult&#xA;Inherits RecognizedPhrase&#xA;Implements ISerializable" />
  <TypeSignature Language="C++ CLI" Value="public ref class RecognitionResult sealed : System::Speech::Recognition::RecognizedPhrase, System::Runtime::Serialization::ISerializable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Speech.Recognition.RecognizedPhrase</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.Runtime.Serialization.ISerializable</InterfaceName>
    </Interface>
  </Interfaces>
  <Attributes>
    <Attribute>
      <AttributeName>System.Diagnostics.DebuggerDisplay("{DebuggerDisplayString ()}")</AttributeName>
    </Attribute>
  </Attributes>
  <Docs>
    <summary>Enthält ausführliche Informationen zur Eingabe, die von Instanzen der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> oder <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> erkannt wurde.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Klasse leitet sich von <xref:System.Speech.Recognition.RecognizedPhrase> und enthält ausführliche Informationen zu Spracherkennung, u. a. folgende:  
  
-   Die <xref:System.Speech.Recognition.RecognizedPhrase.Grammar%2A> Eigenschaftenverweise der <xref:System.Speech.Recognition.Grammar> , dass die Erkennung verwendet, um die Spracherkennung zu identifizieren.  
  
-   Die <xref:System.Speech.Recognition.RecognizedPhrase.Text%2A> Eigenschaft enthält den normalisierten Text für den Ausdruck. Weitere Informationen zu textnormalisierung, finden Sie unter <xref:System.Speech.Recognition.ReplacementText>.  
  
-   Die <xref:System.Speech.Recognition.RecognizedPhrase.Semantics%2A> -Eigenschaft verweist auf die semantischen Informationen, die im Resultset enthalten sind. Die semantische Informationen ist ein Wörterbuch mit den Schlüsselnamen und zugehörige semantische Daten.  
  
-   Die <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> Eigenschaft enthält eine Auflistung von <xref:System.Speech.Recognition.RecognizedPhrase> Objekte, die andere Candidate Interpretationen der audio Eingabe darstellen. Weitere Informationen finden Sie unter <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>.  
  
-   Die <xref:System.Speech.Recognition.RecognizedPhrase.Words%2A> Eigenschaft enthält eine geordnete Auflistung von <xref:System.Speech.Recognition.RecognizedWordUnit> Objekte, die jeweils darstellen erkannt Wort in der Eingabe. Jede <xref:System.Speech.Recognition.RecognizedWordUnit> enthält Anzeigeformat lexikalischen Format und Aussprache-Informationen für das entsprechende Wort.  
  
 Bestimmte Member der <xref:System.Speech.Recognition.SpeechRecognitionEngine>, <xref:System.Speech.Recognition.SpeechRecognizer>, und <xref:System.Speech.Recognition.Grammar> Klassen können generieren eine <xref:System.Speech.Recognition.RecognitionResult>. Weitere Informationen finden Sie in der folgenden Methoden und Ereignisse.  
  
-   Methoden und Ereignisse von den <xref:System.Speech.Recognition.SpeechRecognitionEngine> Klasse:  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>  
  
-   Methoden und Ereignisse von den <xref:System.Speech.Recognition.SpeechRecognizer> Klasse:  
  
    -   <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized>  
  
-   Die <xref:System.Speech.Recognition.Grammar.SpeechRecognized> -Ereignis für die <xref:System.Speech.Recognition.Grammar> Klasse.  
  
 Weitere Informationen zur Deaktivierung von Ereignissen finden Sie unter [mit Spracherkennung Recognition Ereignissen](http://msdn.microsoft.com/library/01c598ca-2e0e-4e89-b303-cd1cef9e8482).  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Handler für das `SpeechRecognized` -Ereignis für ein <xref:System.Speech.Recognition.SpeechRecognitionEngine> oder <xref:System.Speech.Recognition.SpeechRecognizer> -Objekt, und einige der Informationen über die zugeordnete <xref:System.Speech.Recognition.RecognitionResult>.  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  // Add event handler code here.  
  
  // The following code illustrates some of the information available  
  // in the recognition result.  
  Console.WriteLine("Grammar({0}), {1}: {2}",  
    e.Result.Grammar.Name, e.Result.Audio.Duration, e.Result.Text);  
  
  // Display the semantic values in the recognition result.  
  foreach (KeyValuePair<String, SemanticValue> child in e.Result.Semantics)  
  {  
    Console.WriteLine(" {0} key: {1}",  
      child.Key, child.Value.Value ?? "null");  
  }  
  Console.WriteLine();  
  
  // Display information about the words in the recognition result.  
  foreach (RecognizedWordUnit word in e.Result.Words)  
  {  
    RecognizedAudio audio = e.Result.GetAudioForWordRange(word, word);  
    Console.WriteLine(" {0,-10} {1,-10} {2,-10} {3} ({4})",  
      word.Text, word.LexicalForm, word.Pronunciation,  
      audio.Duration, word.DisplayAttributes);  
  }  
  
  // Display the recognition alternates for the result.  
  foreach (RecognizedPhrase phrase in e.Result.Alternates)  
  {  
    Console.WriteLine(" alt({0}) {1}", phrase.Confidence, phrase.Text);  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
    <altmember cref="T:System.Speech.Recognition.RecognizedPhrase" />
    <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
    <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
    <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
    <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
    <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
    <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
    <altmember cref="E:System.Speech.Recognition.Grammar.SpeechRecognized" />
  </Docs>
  <Members>
    <Member MemberName="Alternates">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizedPhrase&gt; Alternates { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizedPhrase&gt; Alternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognitionResult.Alternates" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Alternates As ReadOnlyCollection(Of RecognizedPhrase)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizedPhrase ^&gt; ^ Alternates { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizedPhrase ^&gt; ^ get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizedPhrase&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die Auflistung der möglichen Übereinstimmungen als Eingabe für die Spracherkennung ab.</summary>
        <value>Eine schreibgeschützte Auflistung von Anerkennungsalternativen.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Erkennung <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> sind nach den Werten sortiert ihre <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> Eigenschaften. Die Vertrauenswert des angegebenen Ausdrucks gibt an, die Wahrscheinlichkeit, dass der Ausdruck die Eingabe übereinstimmt. Der Ausdruck mit dem höchsten Wert für die Zuverlässigkeit ist der Ausdruck, der sehr wahrscheinlich die Eingabe entspricht.  
  
 Jede <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> Wert ausgewertet werden soll, einzeln und ohne Verweis auf die Vertrauenswerte anderer <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>. Die Eigenschaften, die die <xref:System.Speech.Recognition.RecognitionResult> erbt von <xref:System.Speech.Recognition.RecognizedPhrase> bieten ausführliche Informationen zu den Ausdruck mit der höchsten vertrauensergebnis.  
  
 Eine Verwendung für die <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> bezieht sich auf automatisierte Fehlerkorrektur. Beispielsweise konnte beim Entwerfen eines Dialogfelds Verzeichnis eine Anwendung der Benutzer aufgefordert, überprüfen Sie, verfügt die Anwendung die richtige Informationen von einem Ereignis Recognition wie in "" Anna"sagen haben?" Wenn der Benutzer sagt "no", und klicken Sie dann die Anwendung konnte den Benutzer über alle Varianten, die hoch genug mussten Abfragen <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> Ergebnis.  
  
 Weitere Informationen zu Spracherkennung und die Verwendung von Erkennungsalternativen, finden Sie unter [Spracherkennung](http://msdn.microsoft.com/library/6a7dc524-07fc-4862-8d48-8c10dc64b919) und [mit Spracherkennung Recognition Ereignissen](http://msdn.microsoft.com/library/01c598ca-2e0e-4e89-b303-cd1cef9e8482).  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Handler für das `SpeechRecognized` Ereignis und einige der Informationen über die zugeordnete <xref:System.Speech.Recognition.RecognitionResult>.  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  // Add event handler code here.  
  
  // The following code illustrates some of the information available  
  // in the recognition result.  
  Console.WriteLine("Grammar({0}), {1}: {2}",  
    e.Result.Grammar.Name, e.Result.Audio.Duration, e.Result.Text);  
  
  // Display the semantic values in the recognition result.  
  foreach (KeyValuePair<String, SemanticValue> child in e.Result.Semantics)  
  {  
    Console.WriteLine(" {0} key: {1}",  
      child.Key, child.Value.Value ?? "null");  
  }  
  Console.WriteLine();  
  
  // Display information about the words in the recognition result.  
  foreach (RecognizedWordUnit word in e.Result.Words)  
  {  
    RecognizedAudio audio = e.Result.GetAudioForWordRange(word, word);  
    Console.WriteLine(" {0,-10} {1,-10} {2,-10} {3} ({4})",  
      word.Text, word.LexicalForm, word.Pronunciation,  
      audio.Duration, word.DisplayAttributes);  
  }  
  
  // Display the recognition alternates for the result.  
  foreach (RecognizedPhrase phrase in e.Result.Alternates)  
  {  
    Console.WriteLine(" alt({0}) {1}", phrase.Confidence, phrase.Text);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
        <altmember cref="T:System.Speech.Recognition.RecognizedPhrase" />
      </Docs>
    </Member>
    <Member MemberName="Audio">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizedAudio Audio { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizedAudio Audio" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognitionResult.Audio" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Audio As RecognizedAudio" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizedAudio ^ Audio { System::Speech::Recognition::RecognizedAudio ^ get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizedAudio</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Audio ab, das dem Erkennungsergebnis zugeordnet ist.</summary>
        <value>Das Audio, das dem Erkennungsergebnis oder der <see langword="null" /> zugeordnet ist, wenn die Erkennung das Ergebnis aus einem Aufruf der  <see langword="EmulateRecognize" /> oder der  <see langword="EmulateRecognizeAsync" />-Methoden einer <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> oder einer <see cref="T:System.Speech.Recognition.SpeechRecognizer" />-Instanz generiert.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Um einen Abschnitt der Audiodatei erhalten, die einen bestimmten Bereich von Wörtern in das Erkennungsergebnis zugeordnet ist, verwenden die <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Handler für das **SpeechRecognized** Ereignis und einige der Informationen über die zugeordnete <xref:System.Speech.Recognition.RecognitionResult>.  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  // Add event handler code here.  
  
  // The following code illustrates some of the information available  
  // in the recognition result.  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
      Console.WriteLine("Audio for result:");  
      Console.WriteLine("  Start time: "+ e.Result.Audio.StartTime);  
      Console.WriteLine("  Duration: " + e.Result.Audio.Duration);  
      Console.WriteLine("  Format: " + e.Result.Audio.Format.EncodingFormat);  
  
  // Display the semantic values in the recognition result.  
  foreach (KeyValuePair<String, SemanticValue> child in e.Result.Semantics)  
  {  
    Console.WriteLine(" {0} key: {1}",  
      child.Key, child.Value.Value ?? "null");  
  }  
  Console.WriteLine();  
  
  // Display information about the words in the recognition result.  
  foreach (RecognizedWordUnit word in e.Result.Words)  
  {  
    RecognizedAudio audio = e.Result.GetAudioForWordRange(word, word);  
    Console.WriteLine(" {0,-10} {1,-10} {2,-10} {3} ({4})",  
      word.Text, word.LexicalForm, word.Pronunciation,  
      audio.Duration, word.DisplayAttributes);  
  }  
  
  // Display the recognition alternates for the result.  
  foreach (RecognizedPhrase phrase in e.Result.Alternates)  
  {  
    Console.WriteLine(" alt({0}) {1}", phrase.Confidence, phrase.Text);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
        <altmember cref="T:System.Speech.Recognition.RecognizedPhrase" />
        <altmember cref="M:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange(System.Speech.Recognition.RecognizedWordUnit,System.Speech.Recognition.RecognizedWordUnit)" />
      </Docs>
    </Member>
    <Member MemberName="GetAudioForWordRange">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizedAudio GetAudioForWordRange (System.Speech.Recognition.RecognizedWordUnit firstWord, System.Speech.Recognition.RecognizedWordUnit lastWord);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognizedAudio GetAudioForWordRange(class System.Speech.Recognition.RecognizedWordUnit firstWord, class System.Speech.Recognition.RecognizedWordUnit lastWord) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange(System.Speech.Recognition.RecognizedWordUnit,System.Speech.Recognition.RecognizedWordUnit)" />
      <MemberSignature Language="VB.NET" Value="Public Function GetAudioForWordRange (firstWord As RecognizedWordUnit, lastWord As RecognizedWordUnit) As RecognizedAudio" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognizedAudio ^ GetAudioForWordRange(System::Speech::Recognition::RecognizedWordUnit ^ firstWord, System::Speech::Recognition::RecognizedWordUnit ^ lastWord);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizedAudio</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="firstWord" Type="System.Speech.Recognition.RecognizedWordUnit" />
        <Parameter Name="lastWord" Type="System.Speech.Recognition.RecognizedWordUnit" />
      </Parameters>
      <Docs>
        <param name="firstWord">Das erste Wort im Bereich.</param>
        <param name="lastWord">Das letzte Wort im Bereich.</param>
        <summary>Ruft einen Audioabschnitt ab, der einem bestimmten Bereich von Wörtern im Erkennungsergebnis zugeordnet ist.</summary>
        <returns>Der Audioabschnitt, der dem Wortbereich zugeordnet ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen der vollständigen Audio verknüpft sind, mit dem Erkennungsergebnis der <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> Eigenschaft.  
  
   
  
## Examples  
 Das folgende Beispiel erstellt eine Grammatik Namenseingabe akzeptiert und fügt es einen Handler für das `SpeechRecognized` Ereignis. Die Grammatik verwendet einen Platzhalter für das Namenselement des Ausdrucks. Der Ereignishandler verwendet die Audiodatei aus dem Platzhalter erstellen und eine Aufforderung Begrüßung wiedergegeben.  
  
```csharp  
  
private Grammar CreateNameInputGrammar()  
{  
  GrammarBuilder wildcardBuilder = new GrammarBuilder();  
  wildcardBuilder.AppendWildcard();  
  SemanticResultKey nameKey =  
    new SemanticResultKey("Name", wildcardBuilder);  
  
  GrammarBuilder nameBuilder =  
    new GrammarBuilder("My name is");  
  nameBuilder.Append(nameKey);  
  
  Grammar nameGrammar = new Grammar(nameBuilder);  
  nameGrammar.Name = "Name input";  
  
  nameGrammar.SpeechRecognized +=  
    new EventHandler<SpeechRecognizedEventArgs>(  
      NameInputHandler);  
  
  return nameGrammar;  
}  
  
// Handle the SpeechRecognized event for the name grammar.  
private void NameInputHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  SemanticValue semantics = e.Result.Semantics;  
  
  if (semantics.ContainsKey("Name"))  
  {  
    RecognizedAudio nameAudio =  
      result.GetAudioForWordRange(  
        result.Words[3], result.Words[result.Words.Count - 1]);  
  
    // Save the audio. Create a directory and file as necessary.  
    FileInfo fi = new FileInfo(@"C:\temp\temp.wav");  
    if (!fi.Directory.Exists)  
    {  
      fi.Directory.Create();  
    }  
    FileStream stream = new FileStream(fi.FullName, FileMode.Create);  
    nameAudio.WriteToWaveStream(stream);  
    stream.Close();  
  
    // Greet the person using the saved audio.  
    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  
    PromptBuilder builder = new PromptBuilder();  
    builder.AppendText("Hello");  
    builder.AppendAudio(fi.FullName);  
    synthesizer.Speak(builder);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.NullReferenceException">Die Erkennung generierte das Ergebnis aus einem Aufruf der <see langword="EmulateRecognize" />-Methode oder der <see langword="EmulateRecognizeAsync" />-Methode des <see cref="T:System.Speech.Recognition.SpeechRecognizer" />-Objekts oder des <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekts.</exception>
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
        <altmember cref="T:System.Speech.Recognition.RecognizedPhrase" />
        <altmember cref="T:System.Speech.Recognition.RecognizedWordUnit" />
        <altmember cref="P:System.Speech.Recognition.RecognitionResult.Audio" />
        <altmember cref="P:System.Speech.Recognition.RecognizedPhrase.Words" />
      </Docs>
    </Member>
    <Member MemberName="System.Runtime.Serialization.ISerializable.GetObjectData">
      <MemberSignature Language="C#" Value="void ISerializable.GetObjectData (System.Runtime.Serialization.SerializationInfo info, System.Runtime.Serialization.StreamingContext context);" />
      <MemberSignature Language="ILAsm" Value=".method hidebysig newslot virtual instance void System.Runtime.Serialization.ISerializable.GetObjectData(class System.Runtime.Serialization.SerializationInfo info, valuetype System.Runtime.Serialization.StreamingContext context) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognitionResult.System#Runtime#Serialization#ISerializable#GetObjectData(System.Runtime.Serialization.SerializationInfo,System.Runtime.Serialization.StreamingContext)" />
      <MemberSignature Language="VB.NET" Value="Sub GetObjectData (info As SerializationInfo, context As StreamingContext) Implements ISerializable.GetObjectData" />
      <MemberSignature Language="C++ CLI" Value=" virtual void System.Runtime.Serialization.ISerializable.GetObjectData(System::Runtime::Serialization::SerializationInfo ^ info, System::Runtime::Serialization::StreamingContext context) = System::Runtime::Serialization::ISerializable::GetObjectData;" />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.Runtime.Serialization.ISerializable.GetObjectData(System.Runtime.Serialization.SerializationInfo,System.Runtime.Serialization.StreamingContext)</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="info" Type="System.Runtime.Serialization.SerializationInfo" />
        <Parameter Name="context" Type="System.Runtime.Serialization.StreamingContext" />
      </Parameters>
      <Docs>
        <param name="info">Das mit Daten zu füllende Objekt.</param>
        <param name="context">Das Ziel für die Serialisierung.</param>
        <summary>Füllt eine <see cref="T:System.Runtime.Serialization.SerializationInfo" />-Instanz mit den Daten auf, die zum Serialisieren des Zielobjekts erforderlich sind.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Bei diesem Member handelt es sich um eine explizite Schnittstellenmemberimplementierung. Er kann nur verwendet werden, wenn die <xref:System.Speech.Recognition.RecognitionResult>-Instanz in eine <xref:System.Runtime.Serialization.ISerializable>-Schnittstelle umgewandelt wird.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Runtime.Serialization.ISerializable" />
        <altmember cref="T:System.Runtime.Serialization.SerializationInfo" />
        <altmember cref="T:System.Runtime.Serialization.StreamingContext" />
      </Docs>
    </Member>
  </Members>
</Type>