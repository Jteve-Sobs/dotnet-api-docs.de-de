<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata>
    <Meta Name="ms.openlocfilehash" Value="4a68990136b4c4a3e7465b21d6054268587b7843" />
    <Meta Name="ms.sourcegitcommit" Value="16d2d159872fd213cae4b8f371d7ae9c8b027c89" />
    <Meta Name="ms.translationtype" Value="HT" />
    <Meta Name="ms.contentlocale" Value="de-DE" />
    <Meta Name="ms.lasthandoff" Value="11/17/2018" />
    <Meta Name="ms.locfileid" Value="51894596" />
  </Metadata>
  <TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Stellt die Möglichkeit bereit, auf eine prozessinterne Spracherkennungs-Engine zuzugreifen und dieses zu verwalten.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine Instanz dieser Klasse für alle installierten Spracherkennungen erstellen. Rufen Sie Informationen darüber, welche Erkennungen installiert sind, verwenden Sie die statische <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
 Diese Klasse ist für die Speech Recognition-Engines in-Process-Ausführung und bietet die Kontrolle über verschiedene Aspekte der Spracherkennung, wie folgt:  
  
-   Um eine in-Process-Spracherkennung zu erstellen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> Konstruktoren.  
  
-   Verwenden Sie zum Verwalten der spracherkennungsgrammatiken der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> Methoden, und die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> Eigenschaft.  
  
-   Verwenden Sie zum Konfigurieren der Eingabe für die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> Methode.  
  
-   Verwenden Sie zum Ausführen der Spracherkennung die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methode.  
  
-   Verwenden Sie zum Ändern der Behandlung der Erkennung auf Ruhe- oder unerwartete Eingabe. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
-   Verwenden Sie zum Ändern der Anzahl der Varianten gibt, die Erkennung zurück der <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> Eigenschaft. Die Erkennung gibt Ergebnisse in einem <xref:System.Speech.Recognition.RecognitionResult> Objekt.  
  
-   Verwenden Sie zum Synchronisieren von Änderungen für die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode. Die Erkennung verwendet mehrere Threads, um Aufgaben auszuführen.  
  
-   Verwenden Sie zum emulieren Eingabe für die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekt ist der einzige des Prozesses verwenden, die das Objekt instanziiert. Im Gegensatz dazu die <xref:System.Speech.Recognition.SpeechRecognizer> gemeinsam eine einzelne Erkennung mit jeder Anwendung, die sie verwenden möchte.  
  
> [!NOTE]
>  Rufen Sie immer <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> bereits vor der Veröffentlichung des letzten Verweis auf die freigegebene Spracherkennung. Andernfalls werden die verwendeten Ressourcen werden nicht reserviert, bis der ruft der Garbage Collector des erkennerobjekts `Finalize` Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Da in diesem Beispiel verwendet die `Multiple` Modus der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> -Methode wird verwendet, bis Sie das Konsolenfenster zu schließen oder Beenden des Debuggens.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Initialisiert eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Kann eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> -Instanz anhand eines der folgenden:  
  
-   Die standardmäßige spracherkennungs-Engine für das system  
  
-   Eine bestimmte spracherkennungs-Engine, die Sie namentlich angeben.  
  
-   Die standardmäßige spracherkennungs-Engine für ein Gebietsschema an, die Sie angeben  
  
-   Einer bestimmten erkennungs-Engine an, die die Kriterien, die Sie angeben erfüllt, in einem <xref:System.Speech.Recognition.RecognizerInfo> Objekt.  
  
 Bevor die Spracherkennung erkennen kann, müssen Sie mindestens eine spracherkennungsgrammatik zu laden und konfigurieren die Eingabe für das Erkennungsmodul.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Verwenden Sie zum Konfigurieren der Audioeingabe einer der folgenden Methoden aus:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Initialisiert eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse mithilfe des angegebenen Standardspracherkennungsmoduls für das System.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Bevor die Spracherkennung Spracherkennung beginnen kann, müssen Sie Laden mindestens eine Erkennung Grammatik und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Verwenden Sie zum Konfigurieren der Audioeingabe einer der folgenden Methoden aus:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Das Gebietsschema, das die Spracherkennung unterstützen muss.</param>
        <summary>Initialisiert mithilfe des angegebenen Standardspracherkennungsmoduls für ein angegebenes Gebietsschema eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows und die "System.Speech"-API akzeptiert alle gültige Sprache / Land-Codes. Zum Ausführen der Spracherkennung, die mit der Sprache, die im angegebenen die `CultureInfo` Argument, das einer spracherkennungs-Engine, die unterstützt werden, dass Sprache / Land-Code installiert werden muss. Spracherkennungsmoduls an, die mit den im Lieferumfang von Microsoft Windows 7 arbeiten Sie mit der folgenden Sprache / Land-Codes.  
  
-   En-GB. Englisch (Vereinigtes Königreich)  
  
-   En-US. Englisch (USA)  
  
-   de-DE. Deutsch (Deutschland)  
  
-   es-ES. Spanisch (Spanien)  
  
-   fr-FR. Französisch (Frankreich)  
  
-   ja-JP. Japanisch (Japan)  
  
-   zh-CN. Chinesisch (China)  
  
-   Zh-TW. Chinesisch (Taiwan)  
  
 Zwei Buchstaben bestehenden Sprache Fehlercodes, wie z. B. "En", "fr", oder "es" sind ebenfalls zulässig.  
  
 Bevor die Spracherkennung erkennen kann, müssen Sie mindestens eine spracherkennungsgrammatik zu laden und konfigurieren die Eingabe für das Erkennungsmodul.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Verwenden Sie zum Konfigurieren der Audioeingabe einer der folgenden Methoden aus:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die veranschaulicht grundlegende Spracherkennung verwendet und eine Spracherkennung die Erkennung für das Gebietsschema En-US initialisiert.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Keine der installierten Spracherkennungen unterstützen das angegebene Gebietsschema, oder <paramref name="culture" /> ist die invariante Kultur.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Culture" /> ist <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">Die Informationen für die angegebene Spracherkennung.</param>
        <summary>Initialisiert eine neue Instanz <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> mithilfe der Informationen in einem <see cref="T:System.Speech.Recognition.RecognizerInfo" />-Objekt, um die zu verwendende Erkennung anzugeben.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine Instanz dieser Klasse für alle installierten Spracherkennungen erstellen. Rufen Sie Informationen darüber, welche Erkennungen installiert sind, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
 Bevor die Spracherkennung erkennen kann, müssen Sie mindestens eine spracherkennungsgrammatik zu laden und konfigurieren die Eingabe für das Erkennungsmodul.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Verwenden Sie zum Konfigurieren der Audioeingabe einer der folgenden Methoden aus:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die veranschaulicht grundlegende Spracherkennung und initialisiert eine Spracherkennung die Erkennung, die die englische Sprache unterstützt.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">Der Tokenname, den die Spracherkennung verwenden soll.</param>
        <summary>Initialisiert eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse mit einem Zeichenfolgenparameter, der den Namen der zu verwendenden Erkennung angibt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Der token-Namen der Erkennung ist der Wert des der <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> Eigenschaft der <xref:System.Speech.Recognition.RecognizerInfo> zurückgegebenes Objekt der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> Eigenschaft der Erkennung. Um eine Auflistung aller installierten Erkennungen abzurufen, verwenden Sie die statische <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
 Bevor die Spracherkennung erkennen kann, müssen Sie mindestens eine spracherkennungsgrammatik zu laden und konfigurieren die Eingabe für das Erkennungsmodul.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Verwenden Sie zum Konfigurieren der Audioeingabe einer der folgenden Methoden aus:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die veranschaulicht grundlegende Spracherkennung verwendet und erstellt eine Instanz von der Spracherkennung Erkennung 8.0 für Windows (Englisch - USA).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Keine Spracherkennung mit diesem Tokennamen ist installiert, oder <paramref name="recognizerId" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="recognizerId" /> ist <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Audioformat ab, das von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangen wird.</summary>
        <value>Das Audioformat bei der Eingabe in die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Instanz oder <see langword="null" />, wenn die Eingabe nicht für die NULL-Eingabe konfiguriert ist oder auf diese gesetzt wurde.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Konfigurieren der Audioeingabe einer der folgenden Methoden aus:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Im folgenden Beispiel wird <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> zum Abrufen und Anzeigen von Daten im audio-Format.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den Level des Audiosignals ab, die von der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangen wird.</summary>
        <value>Der Audiopegel der Eingabe an die Spracherkennung, von 0 bis 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Der Wert 0 stellt Ruhe und die maximale eingabevolumen Wert stellt 100 dar.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> die Ebene seiner Audioeingabe meldet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis mehrere Male pro Sekunde. Die Häufigkeit, mit der das Ereignis ausgelöst wird, hängt von dem Computer, auf dem die Anwendung ausgeführt wird.  
  
 Verwenden Sie zum Abrufen der Audiopegel zum Zeitpunkt des Ereignisses die <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> Eigenschaft des zugeordneten <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Verwenden Sie zum Abrufen der aktuellen Audiopegel der Eingabe für die Erkennung der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> Eigenschaft.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel fügt einen Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> Ereignis, um eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekt. Der Handler gibt die neue Audiodatei Ebene in der Konsole aus.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die aktuelle Position im Audiostream ab, die durch das Gerät generiert wird, das die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> mit Eingaben versorgt.</summary>
        <value>Die aktuelle Position im Audiostream, der durch das Eingabegerät generiert wird.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> -Eigenschaft verweist auf die Position des Eingabegeräts, in der generierten audio Stream. Im Gegensatz dazu die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> -Eigenschaft verweist auf die Erkennung der Position in der Audioeingabe. Diese Positionen können unterschiedlich sein. Z. B. wenn die Erkennung empfangen hat Eingabe nicht der Fall ist für die It noch generiert eine Erkennungsergebnis danach den Wert des der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> -Eigenschaft ist kleiner als der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Eigenschaft.  
  
   
  
## Examples  
 Im folgenden Beispiel verwendet die in-Process-Spracherkennung eine diktatgrammatik, um die Spracheingabe übereinzustimmen. Ein Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis an die Konsole schreibt die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> Wenn die freigegebene Spracherkennung. an der Eingabe erkennt.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ein Problem im Audiosignal erkennt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Rufen Sie die Fehler mit der <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> Eigenschaft des zugeordneten <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel definiert einen Ereignishandler, die Daten zum Sammeln einer <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> Ereignis.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den Status des von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangenen Audiosignals ab.</summary>
        <value>Der Zustand der Audioeingabe für die Spracherkennung.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> Eigenschaft darstellt, die audio Zustand mit einem Member mit dem <xref:System.Speech.Recognition.AudioState> Enumeration.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn sich der Zustand im Audio, das von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangen wird, ändert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen des audio Zustands zum Zeitpunkt des Ereignisses die <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> Eigenschaft des zugeordneten <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Rufen Sie den aktuellen audio-Zustand, der die Eingabe für die Erkennung mit der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> Eigenschaft. Weitere Informationen zum audio-Status finden Sie unter den <xref:System.Speech.Recognition.AudioState> Enumeration.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgenden Beispiel wird einen Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> Ereignis, um die Erkennung Schreiben der neuen <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> an die Konsole jedes Mal, wenn sie Änderungen, die mithilfe eines Members der <xref:System.Speech.Recognition.AudioState> Enumeration.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Zeitintervall ab oder legt dieses fest, während dessen eine <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Eingaben akzeptiert, welche nur Hintergrundgeräusche enthalten, bevor die Erkennung abgeschlossen wird.</summary>
        <value>Die Dauer des Zeitintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede Spracherkennung verfügt über einen Algorithmus, um unterscheiden Ruhe, Spracheingabe. Die Erkennung klassifiziert Hintergrundrauschen, die alle nicht-Ruheintervall Eingabe entspricht, die nicht die erste Regel eines der Erkennung geladen und der spracherkennungsgrammatiken aktiviert. Wenn die Erkennung nur Hintergrundgeräusche und Ruhe innerhalb des Timeoutintervalls Babble empfängt, werden für die Erkennung dieser Erkennungsvorgang abschließt.  
  
-   Für Vorgänge für asynchrone Erkennung, löst die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis, in denen die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> -Eigenschaft ist `true`, und die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> -Eigenschaft ist `null`.  
  
-   Synchrone Erkennungsvorgänge "und" Emulation ", die Erkennung gibt `null`, anstatt eine gültige <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Wenn das Zeitlimit Babble auf 0 festgelegt ist, führt die Erkennung keine Babble Timeout-Überprüfung aus. Das Timeoutintervall kann einen beliebigen Wert nicht negativ sein. Der Standardwert ist 0 Sekunden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, das grundlegende Spracherkennung veranschaulicht, die festlegt, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> vor dem Initiieren der Spracherkennung verwendet. Handler für der Spracherkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ausgabeereignisse Ereignisinformationen in die Konsole, um zu veranschaulichen wie das <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Erkennungsvorgänge auswirken.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Gibt das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt frei.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Gibt das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt frei.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">
          <see langword="true" />, um sowohl verwaltete als auch nicht verwaltete Ressourcen freizugeben, <see langword="false" />, um ausschließlich nicht verwaltete Ressourcen freizugeben.</param>
        <summary>Verwirft das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt und gibt Ressourcen frei, die während der Sitzung verwendet werden.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuliert die Eingabe für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden der Audioeingabe System zu umgehen, und geben Sie Text für die Erkennung als <xref:System.String> Objekte oder als ein Array von <xref:System.Speech.Recognition.RecognizedWordUnit> Objekte. Dies kann hilfreich sein, wenn Sie testen oder Debuggen einer Anwendung oder Grammatik. Sie können z. B. Emulation verwenden, um zu bestimmen, ob ein Wort in einer Grammatik ist und welche Semantik zurückgegeben werden, wenn das Wort erkannt wird. Verwenden der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> Methode, um die Audioeingabe für die spracherkennungs-Engine während der Emulation Vorgänge zu deaktivieren.  
  
 Die Spracherkennung Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Die Erkennung neuer Zeilen und zusätzliche Leerzeichen werden ignoriert und Satzzeichen als literal Eingabe behandelt.  
  
> [!NOTE]
>  Die <xref:System.Speech.Recognition.RecognitionResult> Objekt, das von der Spracherkennung als Reaktion auf emulierte Eingabe generiert wurde, einen Wert von `null` für seine <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> Eigenschaft.  
  
 Verwenden, um asynchrone Erkennung zu emulieren, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe für den Erkennungsvorgang.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird.  
  
 Die freihanderkennung, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren und Breite Zeichen, wenn Grammatikregeln für den auf der eingabebegriff angewendet. Weitere Informationen über diese Art von Vergleich finden Sie unter den <xref:System.Globalization.CompareOptions> Enumerationswerte <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> und <xref:System.Globalization.CompareOptions.IgnoreWidth>. Die freihanderkennung außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Satzzeichen als literal Eingabe behandelt.  
  
   
  
## Examples  
 Im folgenden Codebeispiel ist Teil einer Konsolenanwendung, die zeigt, emulierte Eingabe, die zugehörigen Erkennungsergebnisse und die zugehörigen Ereignisse, die von der Spracherkennung ausgelöst. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Ein Array von Worteinheiten, das die Eingabe für den Erkennungsvorgang enthält.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe bestimmter Wörter für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen den Wörtern und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird.  
  
 Die Erkennung verwendet `compareOptions` Wenn es der eingabebegriff Grammatikregeln anwendet. Die freihanderkennung, die mit Vista und Windows 7 ausgeliefert werden Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die Erkennung immer ignoriert die Zeichenbreite und werden nicht vom Typ Kana ignoriert. Die Erkennung wird auch neue Zeilen und zusätzliche Leerzeichen werden ignoriert und Satzzeichen als literal Eingabe behandelt. Weitere Informationen zu Zeichenbreite und der Kana-Zeichentyp finden Sie unter den <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> enthält mindestens ein <see langword="null" />-Element.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Der Eingabebegriff für den Erkennungsvorgang.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen dem Ausdruck und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird.  
  
 Die Erkennung verwendet `compareOptions` Wenn es der eingabebegriff Grammatikregeln anwendet. Die freihanderkennung, die mit Vista und Windows 7 ausgeliefert werden Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die Erkennung immer ignoriert die Zeichenbreite und werden nicht vom Typ Kana ignoriert. Die Erkennung wird auch neue Zeilen und zusätzliche Leerzeichen werden ignoriert und Satzzeichen als literal Eingabe behandelt. Weitere Informationen zu Zeichenbreite und der Kana-Zeichentyp finden Sie unter den <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuliert die Eingabe für die Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden der Audioeingabe System zu umgehen, und geben Sie Text für die Erkennung als <xref:System.String> Objekte oder als ein Array von <xref:System.Speech.Recognition.RecognizedWordUnit> Objekte. Dies kann hilfreich sein, wenn Sie testen oder Debuggen einer Anwendung oder Grammatik. Sie können z. B. Emulation verwenden, um zu bestimmen, ob ein Wort in einer Grammatik ist und welche Semantik zurückgegeben werden, wenn das Wort erkannt wird. Verwenden der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> Methode, um die Audioeingabe für die spracherkennungs-Engine während der Emulation Vorgänge zu deaktivieren.  
  
 Die Spracherkennung Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Erkennungsvorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis. Die Erkennung neuer Zeilen und zusätzliche Leerzeichen werden ignoriert und Satzzeichen als literal Eingabe behandelt.  
  
> [!NOTE]
>  Die <xref:System.Speech.Recognition.RecognitionResult> Objekt, das von der Spracherkennung als Reaktion auf emulierte Eingabe generiert wurde, einen Wert von `null` für seine <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> Eigenschaft.  
  
 Um synchronen Anerkennung zu emulieren, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe für den Erkennungsvorgang.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Erkennungsvorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
 Die freihanderkennung, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren und Breite Zeichen, wenn Grammatikregeln für den auf der eingabebegriff angewendet. Weitere Informationen über diese Art von Vergleich finden Sie unter den <xref:System.Globalization.CompareOptions> Enumerationswerte <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> und <xref:System.Globalization.CompareOptions.IgnoreWidth>. Die freihanderkennung außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Satzzeichen als literal Eingabe behandelt.  
  
   
  
## Examples  
 Im folgenden Codebeispiel ist Teil einer Konsolenanwendung, die asynchrone emulierte Eingabe, die zugehörigen Erkennungsergebnisse und die zugehörigen Ereignisse, die ausgelöst wird, von der Spracherkennung veranschaulicht. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken, oder die Erkennung bearbeitet einen asynchronen Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Ein Array von Worteinheiten, das die Eingabe für den Erkennungsvorgang enthält.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe bestimmter Wörter für die freigegebene Spracherkennung. Dabei wird ein Array von <see cref="T:System.Speech.Recognition.RecognizedWordUnit" />-Objekten statt Audio für die asynchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen den Wörtern und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Erkennungsvorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
 Die Erkennung verwendet `compareOptions` Wenn es der eingabebegriff Grammatikregeln anwendet. Die freihanderkennung, die mit Vista und Windows 7 ausgeliefert werden Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die freihanderkennung immer die Zeichenbreite ignoriert und nie den Kanatyp ignorieren. Die freihanderkennung außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Satzzeichen als literal Eingabe behandelt. Weitere Informationen zu Zeichenbreite und der Kana-Zeichentyp finden Sie unter den <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken, oder die Erkennung bearbeitet einen asynchronen Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> enthält mindestens ein <see langword="null" />-Element.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Der Eingabebegriff für den Erkennungsvorgang.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen dem Ausdruck und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Erkennungsvorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
 Die Erkennung verwendet `compareOptions` Wenn es der eingabebegriff Grammatikregeln anwendet. Die freihanderkennung, die mit Vista und Windows 7 ausgeliefert werden Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die freihanderkennung immer die Zeichenbreite ignoriert und nie den Kanatyp ignorieren. Die freihanderkennung außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Satzzeichen als literal Eingabe behandelt. Weitere Informationen zu Zeichenbreite und der Kana-Zeichentyp finden Sie unter den <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken, oder die Erkennung bearbeitet einen asynchronen Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> einen asynchronen Erkennungsvorgang einer emulierten Eingabe abschließt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> -Methode beginnt einen asynchronen Erkennungsvorgang. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis aus, wenn es die schließt den asynchronen Vorgang ab.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Vorgang auslösen kann die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis ist die letzte einem solchen Fall, dass die Erkennung für einen bestimmten Vorgang auslöst.  
  
 Wenn der emulierten Erkennung erfolgreich war, können Sie das Erkennungsergebnis, verwenden eine der folgenden zugreifen:  
  
-   Die <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> -Eigenschaft in der <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> Objekt in den Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft in der <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> Objekt in den Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis.  
  
 Wenn der emulierten Erkennung nicht erfolgreich war, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis wird nicht ausgelöst und die <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> NULL.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> wird von <xref:System.ComponentModel.AsyncCompletedEventArgs> abgeleitet.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> wird von <xref:System.Speech.Recognition.RecognitionEventArgs> abgeleitet.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung, die lädt eine spracherkennungsgrammatik aus, und zeigt, asynchrone emulierte Eingabe, die zugehörigen Erkennungsergebnisse und die zugehörigen Ereignisse, die von der Spracherkennung ausgelöst.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Ruheintervall auf oder legt dieses fest, welches die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> am Ende von eindeutiger Eingabe akzeptieren wird, bevor ein Erkennungsvorgang finalisiert wird.</summary>
        <value>Die Dauer des Ruheintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung verwendet dieses Timeoutintervall, wenn die Eingabe für die Erkennung eindeutig ist. Für eine spracherkennungsgrammatik, die Erkennung von entweder unterstützt beispielsweise "neue spielen." oder "Neues Spiel", "neue spielen Sie" ist eine eindeutige Eingabe, und "Neues Spiel" ist eine mehrdeutige Eingabe.  
  
 Diese Eigenschaft bestimmt, wie lange die spracherkennungs-Engine für zusätzliche Eingabe wartet, bevor ein Erkennungsvorgang finalisiert wird. Das Timeout-Intervall kann von 0 auf 10 Sekunden, einschließlich sein. Der Standardwert ist 150 Millisekunden.  
  
 Verwenden Sie zum Festlegen der Timeout-Intervall für mehrdeutigen Eingaben die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden oder größer als 10 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Ruheintervall auf oder legt dieses fest, welches die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> am Ende von mehrdeutiger Eingabe akzeptieren wird, bevor ein Erkennungsvorgang finalisiert wird.</summary>
        <value>Die Dauer des Ruheintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung verwendet dieses Timeoutintervall an, wenn die Eingabe für die Erkennung nicht eindeutig ist. Für eine spracherkennungsgrammatik, die Erkennung von entweder unterstützt beispielsweise "neue spielen." oder "Neues Spiel", "neue spielen Sie" ist eine eindeutige Eingabe, und "Neues Spiel" ist eine mehrdeutige Eingabe.  
  
 Diese Eigenschaft bestimmt, wie lange die spracherkennungs-Engine für zusätzliche Eingabe wartet, bevor ein Erkennungsvorgang finalisiert wird. Das Timeout-Intervall kann von 0 auf 10 Sekunden, einschließlich sein. Der Standardwert ist 500 Millisekunden.  
  
 Verwenden Sie zum Festlegen der Timeout-Intervall für eindeutiger Eingabe der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaft.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden oder größer als 10 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft eine Auflistung der <see cref="T:System.Speech.Recognition.Grammar" />-Objekte ab, die in diese <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Instanz geladen werden.</summary>
        <value>Die Auflistung von <see cref="T:System.Speech.Recognition.Grammar" />-Objekten.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Das folgende Beispiel gibt Informationen an die Konsole für jede spracherkennungsgrammatik, die derzeit von einer Spracherkennung geladen wird.  
  
> [!IMPORTANT]
>  Kopieren Sie die Grammatik-Auflistung, um Fehler zu vermeiden, wenn die Auflistung geändert wird, während diese Methode listet die Elemente der Auflistung.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Zeitintervall ab oder legt dieses fest, während eine <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Eingaben akzeptiert, welche keine Geräusche enthalten, bevor die Erkennung abgeschlossen wird.</summary>
        <value>Die Dauer des Ruheintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede Spracherkennung verfügt über einen Algorithmus, um unterscheiden Ruhe, Spracheingabe. Wenn die Eingabe für die Erkennung Pausen während der ursprünglichen ruhetimeout ist, werden für die Erkennung dieser Erkennungsvorgang abschließt.  
  
-   Für asynchrone Erkennungsvorgänge und emulieren, löst die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis, in denen die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> -Eigenschaft ist `true`, und die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> -Eigenschaft ist `null`.  
  
-   Synchrone Erkennungsvorgänge "und" Emulation ", die Erkennung gibt `null`, anstatt eine gültige <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Wenn das ursprüngliche Timeout Ruheintervall auf 0 festgelegt ist, führt die Erkennung keine erste Stille Timeout-Überprüfung aus. Das Timeoutintervall kann einen beliebigen Wert nicht negativ sein. Der Standardwert ist 0 Sekunden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> vor dem Initiieren der Spracherkennung verwendet. Handler für der Spracherkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ausgabeereignisse Ereignisinformationen in die Konsole, um zu veranschaulichen wie das <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Eigenschaften wirken sich auf Erkennungsvorgänge.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Gibt Informationen für alle installierten Spracherkennungen auf dem aktuellen System zurück.</summary>
        <returns>Eine schreibgeschützte Auflistung von <see cref="T:System.Speech.Recognition.RecognizerInfo" />-Objekten, die die installierten Erkennungen beschreiben.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen von Informationen über die aktuellen Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> Eigenschaft.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird die zurückgegebene Auflistung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode, um eine Spracherkennung die Erkennung zu suchen, die die englische Sprache unterstützt.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Das zu ladende Grammatikobjekt.</param>
        <summary>Lädt synchron ein <see cref="T:System.Speech.Recognition.Grammar" />-Objekt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennung wird eine Ausnahme ausgelöst, wenn die <xref:System.Speech.Recognition.Grammar> Objekt wurde bereits geladen, wird asynchron geladen wird, oder konnte nicht in jeder Erkennungsmodul geladen. Kann nicht geladen werden, die gleiche <xref:System.Speech.Recognition.Grammar> Objekt in mehreren Instanzen von <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Stattdessen erstellen Sie ein neues <xref:System.Speech.Recognition.Grammar> Objekt für die einzelnen <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz.  
  
 Wenn die Erkennung ausgeführt wird, müssen die Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> vor dem Laden, entladen, aktivieren oder deaktivieren eine Grammatik für die spracherkennungs-Engine angehalten.  
  
 Wenn Sie eine Grammatik laden, ist es standardmäßig aktiviert. Verwenden Sie zum Deaktivieren einer geladenen Grammatik der <xref:System.Speech.Recognition.Grammar.Enabled%2A> Eigenschaft.  
  
 Beim Laden einer <xref:System.Speech.Recognition.Grammar> -Objekts asynchron die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar> und in einer Spracherkennung geladen.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> ist in keinem gültigen Zustand.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Die zu ladende Spracherkennungsgrammatik.</param>
        <summary>Lädt asynchron eine Spracherkennungsgrammatik.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Beim Abschluss der Erkennung laden eine <xref:System.Speech.Recognition.Grammar> Objekt ist, löst es eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> Ereignis. Die Erkennung wird eine Ausnahme ausgelöst, wenn die <xref:System.Speech.Recognition.Grammar> Objekt wurde bereits geladen, wird asynchron geladen wird, oder konnte nicht in jeder Erkennungsmodul geladen. Kann nicht geladen werden, die gleiche <xref:System.Speech.Recognition.Grammar> Objekt in mehreren Instanzen von <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Stattdessen erstellen Sie ein neues <xref:System.Speech.Recognition.Grammar> Objekt für die einzelnen <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz.  
  
 Wenn die Erkennung ausgeführt wird, müssen die Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> vor dem Laden, entladen, aktivieren oder deaktivieren eine Grammatik für die spracherkennungs-Engine angehalten.  
  
 Wenn Sie eine Grammatik laden, ist es standardmäßig aktiviert. Verwenden Sie zum Deaktivieren einer geladenen Grammatik der <xref:System.Speech.Recognition.Grammar.Enabled%2A> Eigenschaft.  
  
 Um eine spracherkennungsgrammatik synchron laden, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> Methode.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> ist in keinem gültigen Zustand.</exception>
        <exception cref="T:System.OperationCanceledException">Der asynchrone Vorgang wurde abgebrochen.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> das asynchrone Laden eines <see cref="T:System.Speech.Recognition.Grammar" />-Objekts beendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode initiiert einen asynchronen Vorgang. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis, wenn sie den Vorgang abgeschlossen ist. Zum Abrufen der <xref:System.Speech.Recognition.Grammar> Objekt, das Erkennungsmodul geladen, verwenden Sie die <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> Eigenschaft des zugeordneten <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Abrufen des aktuellen <xref:System.Speech.Recognition.Grammar> Objekte, die die Erkennung geladen hat, verwendet der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> Eigenschaft.  
  
 Wenn die Erkennung ausgeführt wird, müssen die Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> vor dem Laden, entladen, aktivieren oder deaktivieren eine Grammatik für die spracherkennungs-Engine angehalten.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel erstellt ein in-Process-Spracherkennung und erstellt dann auf zwei Arten von Grammatiken für bestimmte Wörter erkennen, und akzeptieren freies Diktat. Das Beispiel erstellt eine <xref:System.Speech.Recognition.Grammar> Objekt aller den abgeschlossenen spracherkennungsgrammatiken, dann lädt asynchron die <xref:System.Speech.Recognition.Grammar> Objekte die <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz. Handler für der Erkennung des <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Schreiben von Ereignissen in der Konsole den Namen des der <xref:System.Speech.Recognition.Grammar> -Objekt, das verwendet wurde, um die Erkennung und der Text, der das Erkennungsergebnis jeweils ausführen.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die Höchstzahl alternativer Erkennungsergebnisse ab oder legt diese fest, welche die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> für jeden Erkennungsvorgang zurückgibt.</summary>
        <value>Die Anzahl alternativer Ergebnisse, die zurückgegeben werden sollen.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> Eigenschaft der <xref:System.Speech.Recognition.RecognitionResult> Klasse enthält die Auflistung der <xref:System.Speech.Recognition.RecognizedPhrase> Mögliche Interpretationen der Eingabe darstellende – Objekte.  
  
 Der Standardwert für <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> ist 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">
          <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> ist auf einen Wert unter 0 (null) festgelegt.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der Einstellung, die zurückgegeben werden soll.</param>
        <summary>Gibt die Werte von Einstellungen für das Erkennungsmodul zurück.</summary>
        <returns>Der Wert der Einstellung.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Einstellungen für die Erkennung können Zeichenfolge, 64-Bit-Ganzzahl oder Arbeitsspeicher-Adressdaten enthalten. Die folgende Tabelle beschreibt die Einstellungen, die für eine Microsoft Speech API (SAPI) definiert sind – Erkennung von kompatibel. Die folgenden Einstellungen müssen für jedes Erkennungsmodul den gleichen Bereich aufweisen, die diese Einstellung unterstützt. Eine SAPI kompatible Erkennung ist nicht erforderlich, um diese Einstellungen zu unterstützen und andere Einstellungen unterstützen.  
  
|name|Beschreibung |  
|----------|-----------------|  
|`ResourceUsage`|Gibt an, der Erkennung von CPU-Auslastung. Der Bereich liegt zwischen 0 und 100. Der Standardwert ist 50.|  
|`ResponseSpeed`|Gibt die Länge der Pause am Ende von eindeutiger Eingabe an, bevor ein Erkennungsvorgang für die Spracherkennung abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 Millisekunden (ms). Diese Einstellung entspricht der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaft.  Standard = 150ms.|  
|`ComplexResponseSpeed`|Gibt die Länge der Pause am Ende von mehrdeutiger Eingabe an, bevor ein Erkennungsvorgang für die Spracherkennung abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 ms. Diese Einstellung entspricht der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft. Standard = 500 ms.|  
|`AdaptationOn`|Gibt an, ob die Anpassung der das akustikmodell auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`). Der Standardwert ist `1` (ON).|  
|`PersistedBackgroundAdaptation`|Gibt an, ob die Anpassung der Hintergrund auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`), und die Einstellung in der Registrierung speichert. Der Standardwert ist `1` (ON).|  
  
 Verwenden Sie zum Aktualisieren einer Einstellung für die Erkennung eines der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden.  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung, die die Werte für eine Reihe von den Einstellungen für die Erkennung, die unterstützt das Gebietsschema En-US definiert ausgibt. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Die Erkennung verfügt über keine Einstellung mit diesem Namen.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Startet einen synchronen Spracherkennungsvorgang.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden führen eine einzelne, synchrone Erkennungsvorgang. Die Erkennung führt diesen Vorgang vor seiner geladenen und aktivierten spracherkennungsgrammatiken.  
  
 Während eines Aufrufs dieser Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung eine Eingabe erkennt, die sie als Sprache identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung ein Erkennungsvorgang finalisiert.  
  
 Die Erkennung wird nicht ausgelöst. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis aus, wenn es sich bei Verwendung eines der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden zurückgeben einer <xref:System.Speech.Recognition.RecognitionResult> Objekt oder `null` , wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.  
  
 Ein synchroner Erkennungsvorgang kann aus folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften oder für die `initialSilenceTimeout` Parameter, der die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methode.  
  
-   Die erkennungs-Engine, erkennt der Sprache, aber findet keine Übereinstimmungen in einem seiner geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Verwenden Sie zum Ändern, wie die Erkennung den Zeitpunkt der Sprache oder Pausen in Bezug auf Erkennung behandelt die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> benötigen mindestens einen <xref:System.Speech.Recognition.Grammar> Objekt geladen wird, bevor Sie Erkennung ausführen. Um eine spracherkennungsgrammatik zu laden, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die asynchrone Erkennung ausführen zu können, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Führt einen synchronen Spracherkennungsvorgang aus.</summary>
        <returns>Das Erkennungsergebnis für die Eingabe oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode führt eine einmalige Erkennungsvorgang. Die Erkennung führt diesen Vorgang vor seiner geladenen und aktivierten spracherkennungsgrammatiken.  
  
 Während eines Aufrufs dieser Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung eine Eingabe erkennt, die sie als Sprache identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung ein Erkennungsvorgang finalisiert.  
  
 Die Erkennung wird nicht ausgelöst. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis aus, wenn diese Methode verwenden.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> Methode gibt eine <xref:System.Speech.Recognition.RecognitionResult> Objekt oder `null` , wenn der Vorgang nicht erfolgreich ist.  
  
 Ein synchroner Erkennungsvorgang kann aus folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften.  
  
-   Die erkennungs-Engine, erkennt der Sprache, aber findet keine Übereinstimmungen in einem seiner geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Um die asynchrone Erkennung ausführen zu können, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und führt ein Erkennungsvorgang.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">Das Zeitspanne, in der eine Spracherkennung eine tonlose Eingabe akzeptiert, bevor die Erkennung abgeschlossen wird.</param>
        <summary>Führt einen synchronen Spracherkennungsvorgang mit einem angegebenen ursprünglichen Ruhetimeout aus.</summary>
        <returns>Das Erkennungsergebnis für die Eingabe oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn das Spracherkennungsmodul Spracherkennung innerhalb des durch angegebenen Zeitintervalls erkennt `initialSilenceTimeout` Argument <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> führt einen einzelnen Erkennungsvorgang und wird dann beendet.  Die `initialSilenceTimeout` Parameter hat Vorrang vor der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaft.  
  
 Während eines Aufrufs dieser Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung eine Eingabe erkennt, die sie als Sprache identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung ein Erkennungsvorgang finalisiert.  
  
 Die Erkennung wird nicht ausgelöst. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis aus, wenn diese Methode verwenden.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> Methode gibt eine <xref:System.Speech.Recognition.RecognitionResult> Objekt oder `null` , wenn der Vorgang nicht erfolgreich ist.  
  
 Ein synchroner Erkennungsvorgang kann aus folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder für die `initialSilenceTimeout` Parameter.  
  
-   Die erkennungs-Engine, erkennt der Sprache, aber findet keine Übereinstimmungen in einem seiner geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Um die asynchrone Erkennung ausführen zu können, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und führt ein Erkennungsvorgang.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Startet einen asynchronen Spracherkennungsvorgang.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden führen, einzelne oder mehrere asynchrone Erkennungsvorgänge. Die Erkennung führt jeder Vorgang vor seiner geladenen und aktivierten spracherkennungsgrammatiken.  
  
 Während eines Aufrufs dieser Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung eine Eingabe erkennt, die sie als Sprache identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung ein Erkennungsvorgang finalisiert.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Wird ausgelöst, wenn eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen ist.  
  
 Um das Ergebnis der einen asynchronen Erkennungsvorgang abzurufen, fügen Sie einen Ereignishandler für der Erkennung des <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Die Erkennung löst dieses Ereignis, wenn sie einen synchronen oder asynchronen Erkennungsvorgang erfolgreich abgeschlossen wurde. Wenn die Erkennung nicht erfolgreich war, die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> Eigenschaft <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> -Objekt, das Sie in den Handler für zugreifen können die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis werden `null`.  
  
 Ein asynchronen Erkennungsvorgang kann aus folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften.  
  
-   Die erkennungs-Engine, erkennt der Sprache, aber findet keine Übereinstimmungen in einem seiner geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte.  
  
-   Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> benötigen mindestens einen <xref:System.Speech.Recognition.Grammar> Objekt geladen wird, bevor Sie Erkennung ausführen. Um eine spracherkennungsgrammatik zu laden, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
-   Verwenden Sie zum Ändern, wie die Erkennung den Zeitpunkt der Sprache oder Pausen in Bezug auf Erkennung behandelt die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
-   Verwenden Sie zum Ausführen synchroner Erkennung eines der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Führt einen einzelnen, asynchronen Spracherkennungsvorgang aus.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode führt einen einzelnen, asynchronen Erkennungsvorgang. Die Erkennung führt den Vorgang für die geladenen und aktivierten spracherkennungsgrammatiken.  
  
 Während eines Aufrufs dieser Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung eine Eingabe erkennt, die sie als Sprache identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung ein Erkennungsvorgang finalisiert.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Wird ausgelöst, wenn eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen ist.  
  
 Um das Ergebnis der einen asynchronen Erkennungsvorgang abzurufen, fügen Sie einen Ereignishandler für der Erkennung des <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Die Erkennung löst dieses Ereignis, wenn sie einen synchronen oder asynchronen Erkennungsvorgang erfolgreich abgeschlossen wurde. Wenn die Erkennung nicht erfolgreich war, die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> Eigenschaft <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> -Objekt, das Sie in den Handler für zugreifen können die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis werden `null`.  
  
 Verwenden Sie zum Ausführen synchroner Erkennung eines der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die zeigt, grundlegende asynchrone Spracherkennung verwendet. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und führt einen asynchronen Erkennungsvorgang. Ereignishandler sind enthalten, um die Ereignisse zu veranschaulichen, die die Erkennung während des Vorgangs ausgelöst.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Gibt an, ob einer oder mehrere Erkennungsvorgänge ausgeführt werden.</param>
        <summary>Führt eine oder mehrere asynchrone Spracherkennungsvorgänge aus.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn `mode` ist <xref:System.Speech.Recognition.RecognizeMode.Multiple>, die Erkennung wird fortgesetzt, Ausführen von Vorgängen asynchrone Erkennung, bis die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> Methode wird aufgerufen.  
  
 Während eines Aufrufs dieser Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung eine Eingabe erkennt, die sie als Sprache identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung ein Erkennungsvorgang finalisiert.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Wird ausgelöst, wenn eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen ist.  
  
 Um das Ergebnis der einen asynchronen Erkennungsvorgang abzurufen, fügen Sie einen Ereignishandler für der Erkennung des <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Die Erkennung löst dieses Ereignis, wenn sie einen synchronen oder asynchronen Erkennungsvorgang erfolgreich abgeschlossen wurde. Wenn die Erkennung nicht erfolgreich war, die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> Eigenschaft <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> -Objekt, das Sie in den Handler für zugreifen können die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis werden `null`.  
  
 Ein asynchronen Erkennungsvorgang kann aus folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften.  
  
-   Die erkennungs-Engine, erkennt der Sprache, aber findet keine Übereinstimmungen in einem seiner geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Verwenden Sie zum Ausführen synchroner Erkennung eines der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die zeigt, grundlegende asynchrone Spracherkennung verwendet. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und mehrere asynchrone Erkennungsvorgänge führt. Die asynchronen Vorgänge werden nach 30 Sekunden abgebrochen. Ereignishandler sind enthalten, um die Ereignisse zu veranschaulichen, die die Erkennung während des Vorgangs ausgelöst.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Beendet die asynchrone Erkennung, ohne auf den Abschluss des aktuellen Erkennungsvorgangs zu warten.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode schließt die asynchrone Erkennung sofort ab. Wenn der aktuelle Vorgang für die asynchrone Erkennung Eingaben empfängt, wird die Eingabe ist abgeschnitten, und der Vorgang abgeschlossen ist, mit der vorhandenen Eingabe. Die Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis aus, wenn ein asynchroner Vorgang wird abgebrochen, und legt sie fest der <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> Eigenschaft der <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> zu `true`. Diese Methode bricht von eingeleiteten asynchronen Vorgänge ab. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Verwenden Sie zum Beenden asynchrone Erkennung ohne das Abschneiden der Eingabesets der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die Verwendung von veranschaulicht die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> Methode. Das Beispiel erstellt und lädt eine spracherkennungsgrammatik, initiiert einen Vorgang wird fortgesetzt, asynchronen Erkennungsvorgang und hält dann die 2 Sekunden, bevor sie den Vorgang abbricht. Die Erkennung Eingaben empfängt, aus der Datei c:\temp\audioinput\sample.wav. Ereignishandler sind enthalten, um die Ereignisse zu veranschaulichen, die die Erkennung während des Vorgangs ausgelöst.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Deaktiviert die asynchrone Erkennung, nachdem der aktuelle Erkennungsvorgang abgeschlossen ist.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode schließt die asynchrone Erkennung ohne das Abschneiden der Eingabe ab. Wenn der aktuelle Vorgang für die asynchrone Erkennung Eingaben empfängt, weiterhin die Erkennung Eingaben angenommen werden, bis der aktuelle Erkennungsvorgang abgeschlossen ist. Die Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis aus, wenn ein asynchroner Vorgang wird beendet, und legt sie fest der <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> Eigenschaft der <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> zu `true`. Diese Methode wird von eingeleiteten asynchronen Vorgänge beendet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Um sofort asynchrone Erkennung mit nur die vorhandene Eingabe zu stornieren, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die Verwendung von veranschaulicht die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> Methode. Das Beispiel erstellt und lädt eine spracherkennungsgrammatik, initiiert einen Vorgang wird fortgesetzt, asynchronen Erkennungsvorgang und hält dann die 2 Sekunden, bevor sie den Vorgang beendet. Die Erkennung Eingaben empfängt, aus der Datei c:\temp\audioinput\sample.wav. Ereignishandler sind enthalten, um die Ereignisse zu veranschaulichen, die die Erkennung während des Vorgangs ausgelöst.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> einen asynchronen Erkennungsvorgang abschließt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> des Objekts <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methode initiiert einen asynchronen Erkennungsvorgang. Wenn die Erkennung den asynchronen Vorgang abgeschlossen ist, wird dieses Ereignis ausgelöst.  
  
 Verwenden den Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis, die Sie zugreifen können die <xref:System.Speech.Recognition.RecognitionResult> in die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> Objekt. Wenn die Erkennung nicht erfolgreich war, <xref:System.Speech.Recognition.RecognitionResult> werden `null`. Um zu bestimmen, ob ein Timeout oder eine Unterbrechung in der Audioeingabe bis hin zu den Fehler verursacht, können Sie die Eigenschaften für zugreifen <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, oder <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  
  
 Weitere Informationen finden Sie in den Ausführungen zur <xref:System.Speech.Recognition.RecognizeCompletedEventArgs>-Klasse.  
  
 Um ausführliche Informationen zu den besten Kandidaten für die abgelehnte Anerkennung zu erhalten, fügen Sie einen Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erkennt Ausdrücke wie z. B. "Zum Anzeigen der Liste der Künstler in der Kategorie" jazz "" oder "Alben Gospel anzeigen". Im Beispiel wird einen Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis, um Informationen zu den Ergebnissen der Erkennung in der Konsole angezeigt.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die aktuelle Position von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> in der Audioeingabe ab, die verarbeitet wird.</summary>
        <value>Die Position der Erkennung in der Audioeingabe, die sie verarbeitet.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die audioposition ist spezifisch für jeden Spracherkennung. Der Wert eines Eingabedatenstroms wird hergestellt, wenn es aktiviert ist.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Eigenschaftenverweise der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Position des Objekts in seiner Audioeingabe. Im Gegensatz dazu die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> -Eigenschaft verweist auf die Position des Eingabegeräts, in der generierten audio Stream. Diese Positionen können unterschiedlich sein. Z. B. wenn die Erkennung empfangen hat Eingabe nicht der Fall ist für die It noch generiert eine Erkennungsergebnis danach den Wert des der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> -Eigenschaft ist kleiner als der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Eigenschaft.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft Informationen über die aktuelle Instanz von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ab.</summary>
        <value>Informationen über die aktuelle Spracherkennung.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Rufen Sie Informationen zu allen installierten Spracherkennungen für das aktuelle System mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
   
  
## Examples  
 Im folgende Beispiel ruft eine unvollständige Liste der Daten für die aktuelle prozessinterne spracherkennungs-Engine ab. Weitere Informationen finden Sie unter <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn ein aktives <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> angehalten wird, um Änderungen zu übernehmen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Anwendungen müssen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> zum Anhalten einer ausgeführten Instanz <xref:System.Speech.Recognition.SpeechRecognitionEngine> vor der Änderung der Einstellungen oder den zugehörigen <xref:System.Speech.Recognition.Grammar> Objekte. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis, wenn er bereit ist, Änderungen zu übernehmen.  
  
 Während beispielsweise die <xref:System.Speech.Recognition.SpeechRecognitionEngine> wird angehalten, Sie können zu laden, nicht entladen, aktivieren und deaktivieren Sie <xref:System.Speech.Recognition.Grammar> Objekte aus, und ändern Sie die Werte für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaften. Weitere Informationen finden Sie unter der Methode <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel zeigt eine Konsolenanwendung, die geladen und entladen wird <xref:System.Speech.Recognition.Grammar> Objekte. Die Anwendung verwendet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode zum Anfordern der spracherkennungs-Engine angehalten, damit sie ein Update empfangen kann. Die Anwendung dann lädt oder Entlädt eine <xref:System.Speech.Recognition.Grammar> Objekt.  
  
 Bei jeder Aktualisierung eines Handlers für <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis schreibt den Namen und die derzeit geladenen Status <xref:System.Speech.Recognition.Grammar> Objekte in der Konsole. Wie Grammatiken geladen und entladen werden, erkennt die Anwendung zunächst die Namen der Farm Tiere, die Namen der Farm Tiere und die Namen von Früchten und dann nur die Namen von Früchten.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie diese Methode beim Synchronisieren der Änderungen für die Erkennung. Z. B. Wenn Sie zu laden oder eine spracherkennungsgrammatik zu entladen, während die Erkennung Eingaben verarbeitet wird, verwenden Sie diese Methode und die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis, um das Verhalten Ihrer Anwendung mit dem Status der Erkennung zu synchronisieren.  
  
 Wenn diese Methode aufgerufen wird, die Erkennung anhält oder asynchrone Vorgänge abgeschlossen und generiert eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis. Ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignishandler können Sie den Status der Erkennung zwischen Erkennungsvorgänge ändern. Bei der Behandlung <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignisse, die Erkennung anhält, bis der Ereignishandler zurückkehrt.  
  
> [!NOTE]
>  Wenn die Eingabe für die Erkennung, bevor die Erkennung löst geändert wird die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis, die Anforderung wird verworfen.  
  
 Wenn diese Methode aufgerufen wird:  
  
-   Wenn die Erkennung nicht Eingabe verarbeitet wird, die Erkennung sofort generiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
-   Die Erkennung Eingaben verarbeitet, der Ruhe- oder Hintergrundrauschen besteht, wird die Erkennung anhält, den Erkennungsvorgang und generiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
-   Die Erkennung Eingaben verarbeitet, die nicht aus den Ruhe- oder Hintergrundrauschen besteht, wird die Erkennung den Erkennungsvorgang abgeschlossen und generiert dann die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
 Während die Erkennung verarbeitet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis:  
  
-   Die Erkennung verarbeitet keine Eingabe, und der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Eigenschaft bleibt unverändert.  
  
-   Die Erkennung zu erfassen, geben Sie den Wert des weiterhin die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Eigenschaft ändern kann.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Beim Generieren von der Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> -Ereignis, das <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> ist `null`.  
  
 Um ein Benutzertoken bereitzustellen, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode. Verwenden Sie zum Angeben eines audioposition-Offsets der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt eine Konsolenanwendung, die geladen und entladen wird <xref:System.Speech.Recognition.Grammar> Objekte. Die Anwendung verwendet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode zum Anfordern der spracherkennungs-Engine angehalten, damit sie ein Update empfangen kann. Die Anwendung dann lädt oder Entlädt eine <xref:System.Speech.Recognition.Grammar> Objekt.  
  
 Bei jeder Aktualisierung eines Handlers für <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis schreibt den Namen und die derzeit geladenen Status <xref:System.Speech.Recognition.Grammar> Objekte in der Konsole. Wie Grammatiken geladen und entladen werden, erkennt die Anwendung zunächst die Namen der Farm Tiere, die Namen der Farm Tiere und die Namen von Früchten und dann nur die Namen von Früchten.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Benutzerdefinierte Informationen, die Informationen für den Vorgang enthalten.</param>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren und stellt ein Benutzertoken für das zugeordnete Ereignis bereit.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennung beim Generieren der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis die <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> enthält den Wert des der `userToken` Parameter.  
  
 Verwenden Sie zum Angeben eines audioposition-Offsets der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Benutzerdefinierte Informationen, die Informationen für den Vorgang enthalten.</param>
        <param name="audioPositionAheadToRaiseUpdate">Der Offset von der aktuellen <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />, um die Anforderung zu verzögern.</param>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren und stellt ein Offset für das zugeordnete Ereignis bereit.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Initiiert die Erkennung nicht die Erkennung aktualisierungsanforderung bis der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> ist gleich der aktuellen <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> plus `audioPositionAheadToRaiseUpdate`.  
  
 Die Erkennung beim Generieren der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis die <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> enthält den Wert des der `userToken` Parameter.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Der Audioeingabestream.</param>
        <param name="audioFormat">Das Format der Audioeingabe.</param>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben von einem Audiostream zu empfangen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung bei der ein Erkennungsvorgang das Ende des Eingabestreams erreicht ist, werden die Erkennungsvorgang abschließt, mit der Eingabe zur Verfügung. Alle nachfolgenden Erkennungsvorgänge können eine Ausnahme generiert, es sei denn, Sie aktualisieren, dass die Eingabe für die Erkennung.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird die Eingabe aus einer Audiodatei example.wav, die die Ausdrücke enthält, "testen, testen eine zwei drei" und "gesprochen Cooper", die durch eine Pause getrennt. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben vom StandardAudiogerät zu empfangen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel verwendet eine Ausgabe aus dem Standard-Audiogeräts, führt Sie mehrere asynchrone Erkennungsvorgänge und endet, wenn ein Benutzer mit der Formulierung utters "exit".  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Deaktiviert die Eingabe zur Spracherkennung.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Konfigurieren der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekt ohne Eingabe bei Verwendung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden, oder wenn es sich bei eine erkennungs-Engine vorübergehend offline zu nehmen.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Der Pfad der Datei, die als Eingabe verwendet werden soll.</param>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben von einer Datei im Waveform-Audioformat (.wav) zu erhalten.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung das Ende der Eingabedatei bei der ein Erkennungsvorgang erreicht, werden die Erkennungsvorgang abschließt, mit der Eingabe zur Verfügung. Alle nachfolgenden Erkennungsvorgänge können eine Ausnahme generiert, es sei denn, Sie aktualisieren, dass die Eingabe für die Erkennung.  
  
   
  
## Examples  
 Im folgenden Beispiel führt Anerkennung für das Audio in eine WAV-Datei und den erkannten Text in die Konsole geschrieben.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Der Stream, der die Audiodaten enthält.</param>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben von einem Stream zu erhalten, der Daten im Waveform-Audioformat (.wav) enthält.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung bei der ein Erkennungsvorgang das Ende des Eingabestreams erreicht ist, werden die Erkennungsvorgang abschließt, mit der Eingabe zur Verfügung. Alle nachfolgenden Erkennungsvorgänge können eine Ausnahme generiert, es sei denn, Sie aktualisieren, dass die Eingabe für die Erkennung.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> eine Eingabe erkennt, die es als Sprache identifizieren kann.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede Spracherkennung verfügt über einen Algorithmus, um unterscheiden Ruhe, Spracheingabe. Wenn die <xref:System.Speech.Recognition.SpeechRecognitionEngine> führt einen spracherkennungsvorgang, löst es die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis aus, wenn der Algorithmus, mit die Eingabe als Sprache identifiziert. Die <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> Eigenschaft des zugeordneten <xref:System.Speech.Recognition.SpeechDetectedEventArgs> Objekt gibt an, Speicherort, in den Eingabestream, auf denen die Erkennung Sprache erkannt. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis aus, bevor es wird, eines ausgelöst der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignisse.  
  
 Weitere Informationen finden Sie unter den <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung zum Ursprung und Ziel Städten für einen Flug auswählen. Die Anwendung erkennt Ausdrücke an, wie z. B. "Ich möchte über Miami nach Chicago fliegen."  Im Beispiel wird die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignisse festlegen, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> jedes Mal Sprache erkannt wird.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ein Wort oder Wörter erkannt hat, die möglicherweise eine Komponente von mehreren vollständigen Ausdrücken in einer Grammatik sind.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> generiert zahlreiche <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> Ereignisse versucht wird, um einem eingegebenen Ausdruck zu identifizieren. Erreichen Sie den Text der teilweise erkannten Ausdrücken in der <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft der <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> Objekt in den Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> Ereignis. Die Bearbeitung dieser Ereignisse ist in der Regel nur beim Debuggen nützlich.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> wird von <xref:System.Speech.Recognition.RecognitionEventArgs> abgeleitet.  
  
 Weitere Informationen finden Sie unter den <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft und die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erkennt formulierungen wie z. B. "Der Liste der Künstler, in der Kategorie" jazz "anzeigen". Im Beispiel wird die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> Ereignis, um unvollständige Ausdruck Fragmente in der Konsole angezeigt wird, wie sie erkannt werden.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> eine Eingabe empfängt, die mit keinem seiner geladenen und aktivierten <see cref="T:System.Speech.Recognition.Grammar" />-Objekte übereinstimmt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennung dieses Ereignis auslöst, wenn er feststellt, dass die Eingabe nicht ausreichend zuverlässig eines seiner geladenen und aktivierten übereinstimmt <xref:System.Speech.Recognition.Grammar> Objekte. Die <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft der <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> enthält die abgelehnten <xref:System.Speech.Recognition.RecognitionResult> Objekt. Können Sie den Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis zum Abrufen der Erkennung <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> , die zurückgewiesen wurden und deren <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> Bewertungen.  
  
 Wenn Ihre Anwendung verwendet eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz können Sie ändern, den Vertrauensgrad, auf welche Sprache Eingabe akzeptiert oder abgelehnt, die mit einem, der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden. Sie können ändern, wie die Spracherkennung zu nicht-Spracheingabe mit reagiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erkennt Ausdrücke wie z. B. "Zum Anzeigen der Liste der Künstler in der Kategorie" jazz "" oder "Alben Gospel anzeigen". Im Beispiel wird einen Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis, um eine Benachrichtigung in der Konsole angezeigt wird, wenn die Sprache Eingabe kann nicht zugeordnet werden, auf den Inhalt der Grammatik mit ausreichenden <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> um eine erfolgreiche Erkennung zu erzeugen. Der Ereignishandler zeigt auch Erkennungsergebnis <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> , die aufgrund der niedrigen vertrauensergebnisse abgelehnt wurden.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> eine Eingabe empfängt, die mit einem seiner geladenen und aktivierten <see cref="T:System.Speech.Recognition.Grammar" />-Objekte übereinstimmt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können ein Erkennungsvorgang, der mit einem der Initiieren der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden. Die Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis, wenn er feststellt, dass die Eingabe entspricht, die eines der geladenen <xref:System.Speech.Recognition.Grammar> Objekte mit ausreichendem Maß Selbstvertrauen, um die Erkennung zu bilden. Die <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft der <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> enthält die zulässigen <xref:System.Speech.Recognition.RecognitionResult> Objekt. Handler <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse erhalten, dem erkannten Ausdruck als auch eine Liste der Erkennung <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> mit niedrigeren zuverlässigkeitsbewertung.  
  
 Wenn Ihre Anwendung verwendet eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz können Sie ändern, den Vertrauensgrad, auf welche Sprache Eingabe akzeptiert oder abgelehnt, die mit einem, der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden.  Sie können ändern, wie die Spracherkennung zu nicht-Spracheingabe mit reagiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 Wenn die Erkennung Eingaben empfängt, die eine Grammatik, entspricht die <xref:System.Speech.Recognition.Grammar> Objekt auslösen kann seine <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis. Die <xref:System.Speech.Recognition.Grammar> des Objekts <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis wird ausgelöst, bevor Sie der Spracherkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Alle Aufgaben, die spezifisch für eine bestimmte Grammatik sollte immer ausgeführt werden, von einem Handler für die <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu den Ereignishandlerdelegaten, finden Sie unter [Ereignisse und Delegaten](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung, die spracherkennungsgrammatik zu Konstrukte erstellt eine <xref:System.Speech.Recognition.Grammar> Objekt aus, und lädt sie in der <xref:System.Speech.Recognition.SpeechRecognitionEngine> um Erkennungsvorgänge auszuführen. Im Beispiel wird veranschaulicht, die Spracheingabe zu einem <xref:System.Speech.Recognition.SpeechRecognitionEngine>, die zugeordneten Erkennungsergebnisse und die zugehörigen Ereignisse, die von der Spracherkennung ausgelöst.  
  
 Eingabe gesprochen, wie z. B. "Ich möchte von Chicago nach Miami fliegen" ausgelöst werden, eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Sprechen den Ausdruck "Fliegen me von Houston nach Chicago" wird nicht ausgelöst. eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis.  
  
 Im Beispiel wird einen Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> erfolgreich für anzuzeigende Ereignis erkannt wird, Ausdrücke und die Semantik, die sie in der Konsole enthalten.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Entlädt alle <see cref="T:System.Speech.Recognition.Grammar" />-Objekte aus des Erkennungsmodul.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung gegenwärtig lädt eine <xref:System.Speech.Recognition.Grammar> asynchron, diese Methode wartet, bis die <xref:System.Speech.Recognition.Grammar> geladen ist, vor dem Entladen aller der <xref:System.Speech.Recognition.Grammar> Objekte aus der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz.  
  
 Verwenden Sie zum Entladen einer bestimmten Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die das synchrone laden und Entladen von spracherkennungsgrammatiken veranschaulicht.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Das zu entladene Grammatikobjekt.</param>
        <summary>Entlädt ein angegebenes <see cref="T:System.Speech.Recognition.Grammar" />-Objekt aus der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Instanz.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Anwendungen müssen verwenden, wenn die Erkennung ausgeführt wird, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> zum Anhalten der <xref:System.Speech.Recognition.SpeechRecognitionEngine> -Instanz vor dem Laden, entladen, aktivieren oder Deaktivieren einer <xref:System.Speech.Recognition.Grammar> Objekt. Alle entladen <xref:System.Speech.Recognition.Grammar> Objekte zu verwenden, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die das synchrone laden und Entladen von spracherkennungsgrammatiken veranschaulicht.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">Die Grammatik wird nicht in dieses Erkennungsmodul geladen, oder das Erkennungsmodul lädt gerade die Grammatik asynchron.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Aktualisiert den Wert einer Einstellung für das Erkennungsmodul.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Einstellungen für die Erkennung können Zeichenfolge, 64-Bit-Ganzzahl oder Arbeitsspeicher-Adressdaten enthalten. Die folgende Tabelle beschreibt die Einstellungen, die für eine Microsoft Speech API (SAPI) definiert sind – Erkennung von kompatibel. Die folgenden Einstellungen müssen für jedes Erkennungsmodul den gleichen Bereich aufweisen, die diese Einstellung unterstützt. Eine SAPI kompatible Erkennung ist nicht erforderlich, um diese Einstellungen zu unterstützen und andere Einstellungen unterstützen.  
  
|name|Beschreibung |  
|----------|-----------------|  
|`ResourceUsage`|Gibt an, der Erkennung von CPU-Auslastung. Der Bereich liegt zwischen 0 und 100. Der Standardwert ist 50.|  
|`ResponseSpeed`|Gibt die Länge der Pause am Ende von eindeutiger Eingabe an, bevor ein Erkennungsvorgang für die Spracherkennung abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 Millisekunden (ms). Diese Einstellung entspricht der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaft. Standard = 150ms.|  
|`ComplexResponseSpeed`|Gibt die Länge der Pause in Millisekunden (ms) am Ende von mehrdeutiger Eingabe an, bevor ein Erkennungsvorgang für die Spracherkennung abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 ms. Diese Einstellung entspricht der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft. Standard = 500 ms.|  
|`AdaptationOn`|Gibt an, ob die Anpassung der das akustikmodell auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`). Der Standardwert ist `1` (ON).|  
|`PersistedBackgroundAdaptation`|Gibt an, ob die Anpassung der Hintergrund auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`), und die Einstellung in der Registrierung speichert. Der Standardwert ist `1` (ON).|  
  
 Um eine der Einstellungen aus der Erkennung zurückzugeben, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> Methode.  
  
 Mit Ausnahme von `PersistedBackgroundAdaptation`, Eigenschaftswerte festgelegt, mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden bleiben wirksam, nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>, nach denen sie auf ihre Standardeinstellungen zurückgesetzt.  
  
 Sie können ändern, wie die Spracherkennung zu nicht-Spracheingabe mit reagiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der zu aktualisierenden Einstellung.</param>
        <param name="updatedValue">Der neue Wert der Einstellung.</param>
        <summary>Aktualisiert die angegebene Einstellung für die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> mit dem angegebenen ganzzahligen Wert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Mit Ausnahme von `PersistedBackgroundAdaptation`, Eigenschaftswerte festgelegt, mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methode bleiben wirksam, nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>, nach denen sie auf ihre Standardeinstellungen zurückgesetzt. Finden Sie unter <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> eine Beschreibung der unterstützten Einstellungen.  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung, die die Werte für eine Reihe von den Einstellungen für die Erkennung, die unterstützt das Gebietsschema En-US definiert ausgibt. Das Beispiel aktualisiert die Einstellungen der vertrauen, und dann eine Abfrage an die Erkennung, um die aktualisierten Werte zu überprüfen. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Die Erkennung verfügt über keine Einstellung mit diesem Namen.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der zu aktualisierenden Einstellung.</param>
        <param name="updatedValue">Der neue Wert der Einstellung.</param>
        <summary>Aktualisiert die angegebene Spracherkennungs-Engine-Einstellung mit dem angegebenen Zeichenfolgenwert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Mit Ausnahme von `PersistedBackgroundAdaptation`, Eigenschaftswerte festgelegt, mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methode bleiben wirksam, nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>, nach denen sie auf ihre Standardeinstellungen zurückgesetzt. Finden Sie unter <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> eine Beschreibung der unterstützten Einstellungen.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Die Erkennung verfügt über keine Einstellung mit diesem Namen.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>