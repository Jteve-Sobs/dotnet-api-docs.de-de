<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata>
    <Meta Name="ms.openlocfilehash" Value="713dc25516fc1274a2b0fdeb1e88295f3064fd9a" />
    <Meta Name="ms.sourcegitcommit" Value="6a0b904069161bbaec4ffd02aa7d9cf38c61e72e" />
    <Meta Name="ms.translationtype" Value="HT" />
    <Meta Name="ms.contentlocale" Value="de-DE" />
    <Meta Name="ms.lasthandoff" Value="06/24/2018" />
    <Meta Name="ms.locfileid" Value="36409731" />
  </Metadata>
  <TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Stellt die Möglichkeit bereit, auf eine prozessinterne Spracherkennungs-Engine zuzugreifen und dieses zu verwalten.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine Instanz dieser Klasse für keines der installierten Spracherkennung erstellen. Verwenden Sie zum Abrufen von Informationen über die Merkmale werden installiert die statische <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
 Diese Klasse ist für die Spracherkennung Recognition Module in-Process-Ausführung und ermöglicht die Steuerung über die verschiedenen Aspekte der Spracherkennung, wie folgt:  
  
-   Um eine in-Process-Spracherkennung zu erstellen, verwenden Sie eines der <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> Konstruktoren.  
  
-   Verwenden Sie zum Verwalten der Sprache Recognition Grammatiken der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> Methoden, und die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> Eigenschaft.  
  
-   Verwenden Sie zum Konfigurieren der Eingabe für die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> Methode.  
  
-   Um die Spracherkennung auszuführen, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methode.  
  
-   Verwenden Sie zum Bearbeiten der Behandlung der Erkennung auf Ruhe oder unerwarteter Eingaben die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
-   Um die Anzahl der Stellvertreter die Erkennung gibt zu ändern, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> Eigenschaft. Das Erkennungsmodul gibt Ergebnisse in einem <xref:System.Speech.Recognition.RecognitionResult> Objekt.  
  
-   Verwenden Sie zum Synchronisieren der Änderungen an die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode. Das Erkennungsmodul verwendet mehrere Threads, um Aufgaben auszuführen.  
  
-   Verwenden Sie zum Emulieren der Eingabe für die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekt ist für die alleinige des Prozesses verwenden, die der Instanziierung des Objekts. Im Gegensatz dazu, die <xref:System.Speech.Recognition.SpeechRecognizer> basiert auf eine einzelne Erkennung wie jede Anwendung, die sie verwenden möchte.  
  
> [!NOTE]
>  Rufen Sie immer <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> , bevor Sie den letzten Verweis auf die von der Spracherkennung freigeben. Andernfalls werden die verwendeten Ressourcen werden nicht reserviert, bis der Garbage Collector des Erkennungsmodul-Objekts aufruft `Finalize` Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Da in diesem Beispiel verwendet die `Multiple` Modus der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> -Methode, er führt Recognition, bis Sie das Konsolenfenster schließen oder Beenden des Debuggens.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Initialisiert eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Für das Konstruieren einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> -Instanz anhand einer der folgenden:  
  
-   Die Standardeinstellung Spracherkennungsmoduls für das system  
  
-   Eine bestimmte Spracherkennungsmoduls an die von Ihnen anhand des Namens  
  
-   Die Standardeinstellung Spracherkennungsmoduls für ein Gebietsschema, die Sie angeben  
  
-   Eine bestimmte Erkennungsmodul, die die Kriterien, die Sie angeben erfüllen, in einem <xref:System.Speech.Recognition.RecognizerInfo> Objekt.  
  
 Bevor die von der Spracherkennung ausführen kann, müssen Sie mindestens eine Sprache Recognition Grammatik laden und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Initialisiert eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse mithilfe des angegebenen Standardspracherkennungsmoduls für das System.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Bevor die von der Spracherkennung Spracherkennung beginnen kann, müssen Sie mindestens eine Anerkennung Grammatik laden und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Das Gebietsschema, das die Spracherkennung unterstützen muss.</param>
        <summary>Initialisiert mithilfe des angegebenen Standardspracherkennungsmoduls für ein angegebenes Gebietsschema eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Übernehmen alle gültigen Sprache / Land-Codes, Microsoft Windows und die System.Speech-API. Mit der Sprache, die im angegebenen Spracherkennung Ausführen der `CultureInfo` Argument, ein Spracherkennungsmodul, die Sprache / Land-Code installiert werden muss unterstützt. Die Spracherkennungsmoduls, die mit den im Lieferumfang von Microsoft Windows 7 arbeiten Sie mit der folgenden Sprache / Land-Codes.  
  
-   En-GB. Englisch (Großbritannien)  
  
-   En-US. Englisch (Vereinigte Staaten)  
  
-   de-DE. Deutsch (Deutschland)  
  
-   es-ES. Spanisch (Spanien)  
  
-   fr-FR. Französisch (Frankreich)  
  
-   ja-JP. Japanisch (Japan)  
  
-   zh-CN. Chinesisch (China)  
  
-   Zh-TW. Chinesisch (Taiwan)  
  
 Zwei Buchstaben bestehende Sprache codes, z. B. "En", "fr", oder ""es endenden"sind ebenfalls zulässig.  
  
 Bevor die von der Spracherkennung ausführen kann, müssen Sie mindestens eine Sprache Recognition Grammatik laden und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die demonstriert den grundlegenden Spracherkennung und initialisiert ein von der Spracherkennung für das Gebietsschema En-US.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Keine der installierten Spracherkennungen unterstützen das angegebene Gebietsschema, oder <paramref name="culture" /> ist die invariante Kultur.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Culture" /> ist <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">Die Informationen für die angegebene Spracherkennung.</param>
        <summary>Initialisiert eine neue Instanz <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> mithilfe der Informationen in einem <see cref="T:System.Speech.Recognition.RecognizerInfo" />-Objekt, um die zu verwendende Erkennung anzugeben.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine Instanz dieser Klasse für keines der installierten Spracherkennung erstellen. Verwenden Sie zum Abrufen von Informationen darüber, welche Merkmale werden installiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
 Bevor die von der Spracherkennung ausführen kann, müssen Sie mindestens eine Sprache Recognition Grammatik laden und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die demonstriert den grundlegenden Spracherkennung und initialisiert ein von der Spracherkennung die englische Sprache unterstützt.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">Der Tokenname, den die Spracherkennung verwenden soll.</param>
        <summary>Initialisiert eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse mit einem Zeichenfolgenparameter, der den Namen der zu verwendenden Erkennung angibt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die token-Namen der Erkennung ist der Wert des der <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizerInfo> zurückgegebenes Objekt die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> Eigenschaft der Erkennung. Um eine Auflistung aller installierten Prüfer abzurufen, verwenden Sie die statische <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
 Bevor die von der Spracherkennung ausführen kann, müssen Sie mindestens eine Sprache Recognition Grammatik laden und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht, und erstellt eine Instanz von der Spracherkennung Erkennungsmodul 8.0 für Windows (Englisch - USA).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Keine Spracherkennung mit diesem Tokennamen ist installiert, oder <paramref name="recognizerId" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="recognizerId" /> ist <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Audioformat ab, das von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangen wird.</summary>
        <value>Das Audioformat bei der Eingabe in die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Instanz oder <see langword="null" />, wenn die Eingabe nicht für die NULL-Eingabe konfiguriert ist oder auf diese gesetzt wurde.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Im folgenden Beispiel wird <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> abzurufen und Audioformat Daten anzuzeigen.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den Level des Audiosignals ab, die von der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangen wird.</summary>
        <value>Der Audiopegel der Eingabe an die Spracherkennung, von 0 bis 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Der Wert 0 stellt Ruhe und 100 steht für die maximale Eingabe Volume.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> die Ebene seiner Audioeingabe meldet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis mehrere Male pro Sekunde. Die Häufigkeit, mit der das Ereignis ausgelöst wird, hängt von dem Computer, auf dem die Anwendung ausgeführt wird.  
  
 Verwenden Sie zum Abrufen der audio Ebene zum Zeitpunkt des Ereignisses die <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Um die aktuelle audio Ebene der Eingabe für die Erkennung zu erhalten, verwenden Sie der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> Eigenschaft.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel fügt einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> Ereignis, um eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekt. Der Handler gibt die neue audio Ebene in der Konsole aus.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die aktuelle Position im Audiostream ab, die durch das Gerät generiert wird, das die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> mit Eingaben versorgt.</summary>
        <value>Die aktuelle Position im Audiostream, der durch das Eingabegerät generiert wird.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> -Eigenschaft verweist auf das Eingabegerät Position in der generierten Audiostream. Im Gegensatz dazu, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Eigenschaft verweist auf die Erkennung Position innerhalb der Audioeingabe. Diese Positionen können unterschiedlich sein. Z. B. wenn die Erkennung erhalten hat Eingabe nicht für die It hat noch erzeugt ein Erkennungsergebnis wird der Wert der die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> -Eigenschaft muss kleiner als der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Eigenschaft.  
  
   
  
## Examples  
 Im folgenden Beispiel verwendet die in-Process-Spracherkennung diktieren Grammatik Spracheingabe entsprechend an. Einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis in die Konsole schreibt die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> Wenn erkennt die von der Spracherkennung Sprache bei der Eingabe.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ein Problem im Audiosignal erkennt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen, welches Problem ist aufgetreten, der <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel definiert einen Ereignishandler, die sammelt Informationen über ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> Ereignis.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den Status des von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangenen Audiosignals ab.</summary>
        <value>Der Zustand der Audioeingabe für die Spracherkennung.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> Eigenschaft darstellt, das audio Zustand mit einem Mitglied der <xref:System.Speech.Recognition.AudioState> Enumeration.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn sich der Zustand im Audio, das von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangen wird, ändert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen des audio Zustands zum Zeitpunkt des Ereignisses die <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Um den aktuellen audio-Status, der die Eingabe für die Erkennung zu erhalten, verwenden Sie der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> Eigenschaft. Weitere Informationen zum audio-Status finden Sie unter der <xref:System.Speech.Recognition.AudioState> Enumeration.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgenden Beispiel wird einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> Ereignis, um die Erkennung Schreiben des neuen <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> an die Konsole jedes Mal, wenn sie Änderungen, die mithilfe eines Members der <xref:System.Speech.Recognition.AudioState> Enumeration.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Zeitintervall ab oder legt dieses fest, während dessen eine <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Eingaben akzeptiert, welche nur Hintergrundgeräusche enthalten, bevor die Erkennung abgeschlossen wird.</summary>
        <value>Die Dauer des Zeitintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede von der Spracherkennung verfügt über einen Algorithmus, um die Unterscheidung zwischen Ruhe und Sprache. Das Erkennungsmodul klassifiziert Hintergrundgeräuschen alle Ruhe-Eingabedatei, die die erste Regel aller der Erkennung nicht übereinstimmen geladen und der Sprache Recognition Grammatiken aktiviert. Wenn die Erkennung nur Hintergrundgeräuschen und Ruhe innerhalb des Timeoutintervalls Babble empfängt, schließt die Erkennung dieser Erkennungsvorgang ab.  
  
-   Für asynchrone Erkennungsvorgänge, löst die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> -Ereignis, in dem die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> Eigenschaft ist `true`, und die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> Eigenschaft ist `null`.  
  
-   Synchrone Erkennungsvorgänge Emulation und die Erkennung gibt `null`, anstatt eine gültige <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Wenn das Timeout für die Babble auf 0 festgelegt ist, führt die Erkennung keine Babble Timeout-Überprüfung aus. Das Timeoutintervall kann es sich um einen nicht negativen Wert annehmen. Der Standardwert beträgt 0 Sekunden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht, der festlegt der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> vor dem Initiieren der Spracherkennung. Handler für die Spracherkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignisse Ausgabe Ereignisinformationen in die Konsole zur Veranschaulichung der Funktion wie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> auf Recognition Vorgänge auswirken.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Gibt das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt frei.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Gibt das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt frei.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">
          <see langword="true" />, um sowohl verwaltete als auch nicht verwaltete Ressourcen freizugeben, <see langword="false" />, um ausschließlich nicht verwaltete Ressourcen freizugeben.</param>
        <summary>Gibt das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt und Ressourcen frei, die während der Sitzung verwendet werden.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuliert die Eingabe für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden der Audioeingabe System zu umgehen, und geben Sie Text an, um die Erkennung als <xref:System.String> Objekte oder als ein Array von <xref:System.Speech.Recognition.RecognizedWordUnit> Objekte. Dies kann hilfreich sein, wenn Sie testen oder Debuggen einer Anwendung oder Grammatik. Sie können z. B. Emulation verwenden, um zu bestimmen, ob ein Wort in einem Grammatik ist und welche Semantik zurückgegeben werden, wenn das Wort erkannt wird. Verwenden der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> Methode Audioeingabe an die Spracherkennungsmoduls während Emulation Vorgänge deaktiviert.  
  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Das Erkennungsmodul neue Zeilen und zusätzlichen Leerraum ignoriert und Interpunktion als literal Eingabe behandelt.  
  
> [!NOTE]
>  Die <xref:System.Speech.Recognition.RecognitionResult> Objekt, die von der Spracherkennung generiert wurde als Antwort auf "emuliert" Eingabe hat den Wert der `null` für seine <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> Eigenschaft.  
  
 Zum Emulieren der asynchronen Recognition verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe für den Erkennungsvorgang.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird.  
  
 Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren und Breite Zeichen, beim Anwenden der Grammatikregeln für den auf der input-Ausdruck. Weitere Informationen über diese Art von Vergleich finden Sie unter der <xref:System.Globalization.CompareOptions> Enumerationswerte <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> und <xref:System.Globalization.CompareOptions.IgnoreWidth>. Der Prüfer ist außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Interpunktion als literal Eingabe behandelt.  
  
   
  
## Examples  
 Im folgenden Codebeispiel ist Teil einer Konsolenanwendung, die "emuliert" Eingabe, die zugehörigen Erkennungsergebnisse und die zugehörigen Ereignisse ausgelöst, die für die von der Spracherkennung veranschaulicht. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Ein Array von Worteinheiten, das die Eingabe für den Erkennungsvorgang enthält.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe bestimmter Wörter für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen den Wörtern und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird.  
  
 Das Erkennungsmodul verwendet `compareOptions` Wenn der input-Ausdruck als Grammatikregeln anwendet. Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Das Erkennungsmodul immer die Zeichenbreite ignoriert und nie vom Typ Kana ignoriert. Das Erkennungsmodul auch neue Zeilen und zusätzliche Leerzeichen werden ignoriert und Interpunktion als literal Eingabe behandelt. Weitere Informationen zu der Zeichenbreite und Kanatyp, finden Sie unter der <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> enthält mindestens ein <see langword="null" />-Element.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Der Eingabebegriff für den Erkennungsvorgang.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen dem Ausdruck und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird.  
  
 Das Erkennungsmodul verwendet `compareOptions` Wenn der input-Ausdruck als Grammatikregeln anwendet. Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Das Erkennungsmodul immer die Zeichenbreite ignoriert und nie vom Typ Kana ignoriert. Das Erkennungsmodul auch neue Zeilen und zusätzliche Leerzeichen werden ignoriert und Interpunktion als literal Eingabe behandelt. Weitere Informationen zu der Zeichenbreite und Kanatyp, finden Sie unter der <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuliert die Eingabe für die Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden der Audioeingabe System zu umgehen, und geben Sie Text an, um die Erkennung als <xref:System.String> Objekte oder als ein Array von <xref:System.Speech.Recognition.RecognizedWordUnit> Objekte. Dies kann hilfreich sein, wenn Sie testen oder Debuggen einer Anwendung oder Grammatik. Sie können z. B. Emulation verwenden, um zu bestimmen, ob ein Wort in einem Grammatik ist und welche Semantik zurückgegeben werden, wenn das Wort erkannt wird. Verwenden der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> Methode Audioeingabe an die Spracherkennungsmoduls während Emulation Vorgänge deaktiviert.  
  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Recognition-Vorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis. Das Erkennungsmodul neue Zeilen und zusätzlichen Leerraum ignoriert und Interpunktion als literal Eingabe behandelt.  
  
> [!NOTE]
>  Die <xref:System.Speech.Recognition.RecognitionResult> Objekt, die von der Spracherkennung generiert wurde als Antwort auf "emuliert" Eingabe hat den Wert der `null` für seine <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> Eigenschaft.  
  
 Zum emulieren synchrone Recognition verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe für den Erkennungsvorgang.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Recognition-Vorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
 Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren und Breite Zeichen, beim Anwenden der Grammatikregeln für den auf der input-Ausdruck. Weitere Informationen über diese Art von Vergleich finden Sie unter der <xref:System.Globalization.CompareOptions> Enumerationswerte <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> und <xref:System.Globalization.CompareOptions.IgnoreWidth>. Der Prüfer ist außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Interpunktion als literal Eingabe behandelt.  
  
   
  
## Examples  
 Im folgenden Codebeispiel ist Teil einer Konsolenanwendung, die asynchrone emulierten Eingabe, die zugeordneten Erkennungsergebnisse und die zugehörigen Ereignisse ausgelöst, die für die von der Spracherkennung veranschaulicht. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken, oder die Erkennung bearbeitet einen asynchronen Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Ein Array von Worteinheiten, das die Eingabe für den Erkennungsvorgang enthält.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe bestimmter Wörter für die freigegebene Spracherkennung. Dabei wird ein Array von <see cref="T:System.Speech.Recognition.RecognizedWordUnit" />-Objekten statt Audio für die asynchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen den Wörtern und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Recognition-Vorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
 Das Erkennungsmodul verwendet `compareOptions` Wenn der input-Ausdruck als Grammatikregeln anwendet. Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Der Prüfer immer die Zeichenbreite ignoriert und nie den Kanatyp ignorieren. Der Prüfer ist außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Interpunktion als literal Eingabe behandelt. Weitere Informationen zu der Zeichenbreite und Kanatyp, finden Sie unter der <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken, oder die Erkennung bearbeitet einen asynchronen Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> enthält mindestens ein <see langword="null" />-Element.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Der Eingabebegriff für den Erkennungsvorgang.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen dem Ausdruck und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Recognition-Vorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
 Das Erkennungsmodul verwendet `compareOptions` Wenn der input-Ausdruck als Grammatikregeln anwendet. Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Der Prüfer immer die Zeichenbreite ignoriert und nie den Kanatyp ignorieren. Der Prüfer ist außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Interpunktion als literal Eingabe behandelt. Weitere Informationen zu der Zeichenbreite und Kanatyp, finden Sie unter der <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken, oder die Erkennung bearbeitet einen asynchronen Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> einen asynchronen Erkennungsvorgang einer emulierten Eingabe abschließt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methode startet einen asynchronen Recognition-Vorgang. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis aus, wenn sie den asynchronen Vorgang abschließt.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Vorgang auslösen kann die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis ist die letzte einem solchen Fall, dass die Erkennung für einen angegebenen Vorgang löst.  
  
 Wenn "emuliert" Erkennung erfolgreich war, können Sie die Erkennungsergebnis mithilfe einer der folgenden zugreifen:  
  
-   Die <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> Eigenschaft in der <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> Objekt im Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> -Eigenschaft in der <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> Objekt im Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis.  
  
 Wenn "emuliert" Erkennung nicht erfolgreich war, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis wird nicht ausgelöst, und die <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> wird null sein.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> wird von <xref:System.ComponentModel.AsyncCompletedEventArgs> abgeleitet.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> wird von <xref:System.Speech.Recognition.RecognitionEventArgs> abgeleitet.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung, die lädt eine Spracherkennung Recognition Grammatik und asynchrone emulierten Eingabe, die zugeordneten Erkennungsergebnisse und die zugehörigen Ereignisse ausgelöst, die für die von der Spracherkennung veranschaulicht.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Ruheintervall auf oder legt dieses fest, welches die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> am Ende von eindeutiger Eingabe akzeptieren wird, bevor ein Erkennungsvorgang finalisiert wird.</summary>
        <value>Die Dauer des Ruheintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die von der Spracherkennung verwendet dieses Timeoutintervall an, wenn die Eingabe Recognition eindeutig ist. Z. B. für eine Sprache Recognition-Grammatik, mit der Erkennung von beidem unterstützt "neue Bitte game" oder "new Game", "neue Bitte game" ist eine eindeutige Eingabe, und "new Game" ist eine mehrdeutige Eingabe.  
  
 Diese Eigenschaft bestimmt, wie lange die Spracherkennungsmoduls für zusätzliche Eingabe-gewartet wird, vor dem Abschließen eines Erkennungsvorgangs. Das Timeout-Intervall kann von 0 Sekunden auf 10 Sekunden, einschließlich sein. Der Standardwert beträgt 150 Millisekunden.  
  
 Um das Timeoutintervall für mehrdeutigen Eingaben festzulegen, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden oder größer als 10 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Ruheintervall auf oder legt dieses fest, welches die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> am Ende von mehrdeutiger Eingabe akzeptieren wird, bevor ein Erkennungsvorgang finalisiert wird.</summary>
        <value>Die Dauer des Ruheintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die von der Spracherkennung verwendet dieses Timeoutintervall an, wenn die Eingabe Recognition mehrdeutig ist. Z. B. für eine Sprache Recognition-Grammatik, mit der Erkennung von beidem unterstützt "neue Bitte game" oder "new Game", "neue Bitte game" ist eine eindeutige Eingabe, und "new Game" ist eine mehrdeutige Eingabe.  
  
 Diese Eigenschaft bestimmt, wie lange die Spracherkennungsmoduls für zusätzliche Eingabe-gewartet wird, vor dem Abschließen eines Erkennungsvorgangs. Das Timeout-Intervall kann von 0 Sekunden auf 10 Sekunden, einschließlich sein. Der Standardwert beträgt 500 Millisekunden.  
  
 Verwenden Sie zum Festlegen der Timeout-Intervall für die Eingabe eindeutig die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaft.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden oder größer als 10 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft eine Auflistung der <see cref="T:System.Speech.Recognition.Grammar" />-Objekte ab, die in diese <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Instanz geladen werden.</summary>
        <value>Eine Auflistung von <see cref="T:System.Speech.Recognition.Grammar" />-Objekten.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Das folgende Beispiel gibt Informationen an die Konsole für jede Sprache Recognition-Grammatik, die derzeit von einer Spracherkennung geladen wird.  
  
> [!IMPORTANT]
>  Kopieren Sie die Grammatik-Auflistung, um Fehler zu vermeiden, wenn die Auflistung geändert wird, während diese Methode listet die Elemente der Auflistung auf.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Zeitintervall ab oder legt dieses fest, während eine <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Eingaben akzeptiert, welche keine Geräusche enthalten, bevor die Erkennung abgeschlossen wird.</summary>
        <value>Die Dauer des Ruheintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede von der Spracherkennung verfügt über einen Algorithmus, um die Unterscheidung zwischen Ruhe und Sprache. Wenn die Eingabe für die Erkennung Ruhe innerhalb des Timeoutzeitraums anfängliche Ruhe ist, schließt die Erkennung dieser Erkennungsvorgang ab.  
  
-   Für asynchrone Erkennungsvorgänge und Emulation, löst die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> -Ereignis, in dem die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> Eigenschaft ist `true`, und die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> Eigenschaft ist `null`.  
  
-   Synchrone Erkennungsvorgänge Emulation und die Erkennung gibt `null`, anstatt eine gültige <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Wenn das erste Ruhe-Timeout-Intervall auf 0 festgelegt ist, führt die Erkennung keine anfängliche Ruhe-Timeout-Überprüfung aus. Das Timeoutintervall kann es sich um einen nicht negativen Wert annehmen. Der Standardwert beträgt 0 Sekunden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> vor dem Initiieren der Spracherkennung. Handler für die Spracherkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignisse Ausgabe Ereignisinformationen in die Konsole zu veranschaulichen, wie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften des eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Eigenschaften wirken sich auf Erkennungsvorgänge.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Gibt Informationen für alle installierten Spracherkennungen auf dem aktuellen System zurück.</summary>
        <returns>Eine schreibgeschützte Auflistung von <see cref="T:System.Speech.Recognition.RecognizerInfo" />-Objekten, die die installierten Erkennungen beschreiben.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen von Informationen über die aktuellen Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> Eigenschaft.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird die zurückgegebene Auflistung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode, um eine von der Spracherkennung zu ermitteln, die die englische Sprache unterstützt.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Das zu ladende Grammatikobjekt.</param>
        <summary>Lädt synchron ein <see cref="T:System.Speech.Recognition.Grammar" />-Objekt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Das Erkennungsmodul löst eine Ausnahme aus, wenn die <xref:System.Speech.Recognition.Grammar> Objekt bereits geladen wird, wird asynchron geladen oder konnte nicht in jeder Erkennungsmodul geladen. Kann nicht geladen werden, die gleiche <xref:System.Speech.Recognition.Grammar> Objekt in mehreren Instanzen von <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Erstellen Sie stattdessen ein neues <xref:System.Speech.Recognition.Grammar> -Objekt für jedes <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz.  
  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> der Spracherkennungsmoduls vor dem Laden, entladen, aktivieren oder deaktivieren eine Grammatik anhalten.  
  
 Wenn Sie eine Grammatik laden, ist es standardmäßig aktiviert. Verwenden Sie zum Deaktivieren einer geladenen Grammatik der <xref:System.Speech.Recognition.Grammar.Enabled%2A> Eigenschaft.  
  
 Beim Laden einer <xref:System.Speech.Recognition.Grammar> -Objekt asynchron, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar> und lädt sie in einer von der Spracherkennung.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> ist in keinem gültigen Zustand.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Die zu ladende Spracherkennungsgrammatik.</param>
        <summary>Lädt asynchron eine Spracherkennungsgrammatik.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Beim Abschluss der Erkennung Laden einer <xref:System.Speech.Recognition.Grammar> -Objekt, löst es eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> Ereignis. Das Erkennungsmodul löst eine Ausnahme aus, wenn die <xref:System.Speech.Recognition.Grammar> Objekt bereits geladen wird, wird asynchron geladen oder konnte nicht in jeder Erkennungsmodul geladen. Kann nicht geladen werden, die gleiche <xref:System.Speech.Recognition.Grammar> Objekt in mehreren Instanzen von <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Erstellen Sie stattdessen ein neues <xref:System.Speech.Recognition.Grammar> -Objekt für jedes <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz.  
  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> der Spracherkennungsmoduls vor dem Laden, entladen, aktivieren oder deaktivieren eine Grammatik anhalten.  
  
 Wenn Sie eine Grammatik laden, ist es standardmäßig aktiviert. Verwenden Sie zum Deaktivieren einer geladenen Grammatik der <xref:System.Speech.Recognition.Grammar.Enabled%2A> Eigenschaft.  
  
 Um eine Sprache Recognition Grammatik synchron zu laden, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> Methode.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> ist in keinem gültigen Zustand.</exception>
        <exception cref="T:System.OperationCanceledException">Der asynchrone Vorgang wurde abgebrochen.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> das asynchrone Laden eines <see cref="T:System.Speech.Recognition.Grammar" />-Objekts beendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Des Erkennungsmodul <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode initiiert einen asynchronen Vorgang. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis, wenn sie den Vorgang abgeschlossen ist. Zum Abrufen der <xref:System.Speech.Recognition.Grammar> Objekt, das die Erkennung laden, verwenden Sie die <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Abrufen des aktuellen <xref:System.Speech.Recognition.Grammar> Objekte, die die Erkennung geladen wurde, verwenden Sie der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> Eigenschaft.  
  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> der Spracherkennungsmoduls vor dem Laden, entladen, aktivieren oder deaktivieren eine Grammatik anhalten.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erstellt ein in-Process-Spracherkennung und erstellt dann auf zwei Arten von Grammatiken für die Erkennung von bestimmten Wörtern und kostenlose diktieren annimmt. Im Beispiel wird erstellt eine <xref:System.Speech.Recognition.Grammar> Objekt aus allen Grammatiken Recognition abgeschlossenen Spracherkennung asynchron lädt dann die <xref:System.Speech.Recognition.Grammar> -Objekte und die <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz. Handler für der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Schreiben von Ereignissen in der Konsole den Namen des dem <xref:System.Speech.Recognition.Grammar> -Objekt, das verwendet wurde, um die Erkennung und den Text des Ergebnisses Recognition bzw. ausführen.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die Höchstzahl alternativer Erkennungsergebnisse ab oder legt diese fest, welche die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> für jeden Erkennungsvorgang zurückgibt.</summary>
        <value>Die Anzahl alternativer Ergebnisse, die zurückgegeben werden sollen.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognitionResult> Klasse enthält die Auflistung der <xref:System.Speech.Recognition.RecognizedPhrase> Objekte, die mögliche Interpretationen der Eingabe darstellen.  
  
 Der Standardwert für <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> ist 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">
          <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> ist auf einen Wert unter 0 (null) festgelegt.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der Einstellung, die zurückgegeben werden soll.</param>
        <summary>Gibt die Werte von Einstellungen für das Erkennungsmodul zurück.</summary>
        <returns>Der Wert der Einstellung.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Erkennung Einstellungen können Zeichenfolge, 64-Bit-Ganzzahl oder Arbeitsspeicher-Adressdaten enthalten. Die folgende Tabelle beschreibt die Einstellungen, die für eine Microsoft-Speech-API (SAPI) definiert sind – Erkennung kompatibel. Die folgenden Einstellungen benötigen denselben Bereich für jeden Erkennungsmodul, die die Einstellung unterstützt. Eine SAPI-kompatible Erkennungsmodul ist nicht erforderlich, diese Einstellungen unterstützt und kann andere Einstellungen unterstützen.  
  
|Name|Beschreibung |  
|----------|-----------------|  
|`ResourceUsage`|Gibt die Erkennung CPU-Auslastung an. Der Bereich liegt zwischen 0 und 100. Der Standardwert ist 50.|  
|`ResponseSpeed`|Gibt die Länge der Pause am Ende der Eingabe eindeutig an, bevor die von der Spracherkennung ein Erkennungsvorgangs abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 Millisekunden (ms). Diese Einstellung bezieht sich auf der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaft.  Standard = 150ms.|  
|`ComplexResponseSpeed`|Gibt die Länge der Pause am Ende der mehrdeutigen Eingaben an, bevor die von der Spracherkennung ein Erkennungsvorgangs abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 ms. Diese Einstellung bezieht sich auf der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft. Standard = 500 ms.|  
|`AdaptationOn`|Gibt an, ob die Anpassung des Modells acoustic auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`). Der Standardwert ist `1` (ON).|  
|`PersistedBackgroundAdaptation`|Gibt an, ob die Anpassung im Hintergrund auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`), und speichert die Einstellung in der Registrierung. Der Standardwert ist `1` (ON).|  
  
 Um eine Einstellung für die Erkennung zu aktualisieren, verwenden Sie eines der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden.  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung, die die Werte für eine Reihe von den Einstellungen für die Erkennung, die das Gebietsschema En-US unterstützt definierten ausgibt. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Die Erkennung verfügt über keine Einstellung mit diesem Namen.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Startet einen synchronen Spracherkennungsvorgang.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden ausführen ein einzelnen, synchrone Erkennungsvorgangs an. Das Erkennungsmodul führt diesen Vorgang für seine geladen und aktiviert Speech Recognition Grammatiken.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
 Das Erkennungsmodul wird nicht ausgelöst. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis bei Verwendung einer der der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> -Methoden zurückgeben einer <xref:System.Speech.Recognition.RecognitionResult> -Objekt, oder `null` , wenn der Vorgang nicht erfolgreich ist oder die Erkennung nicht aktiviert.  
  
 Eine synchrone Erkennungsvorgang kann aus den folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften oder für die `initialSilenceTimeout` Parameter von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methode.  
  
-   Das Erkennungsmodul Spracherkennung erkennt jedoch findet keine Übereinstimmungen in einer seiner geladen und aktiviert <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Verwenden Sie zum Ändern, wie die Erkennung für die zeitliche Abfolge der Sprache oder Pausen in Bezug auf Erkennung behandelt, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> benötigen mindestens ein <xref:System.Speech.Recognition.Grammar> Objekt vor dem Ausführen von Recognition geladen. Um eine Sprache Recognition Grammatik zu laden, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um asynchrone Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Führt einen synchronen Spracherkennungsvorgang aus.</summary>
        <returns>Das Erkennungsergebnis für die Eingabe oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode führt eine einzelne Erkennungsvorgang. Das Erkennungsmodul führt diesen Vorgang für seine geladen und aktiviert Speech Recognition Grammatiken.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
 Das Erkennungsmodul wird nicht ausgelöst. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis bei Verwendung dieser Methode.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> Methode gibt ein <xref:System.Speech.Recognition.RecognitionResult> -Objekt, oder `null` , wenn der Vorgang nicht erfolgreich ist.  
  
 Eine synchrone Erkennungsvorgang kann aus den folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften.  
  
-   Das Erkennungsmodul Spracherkennung erkennt jedoch findet keine Übereinstimmungen in einer seiner geladen und aktiviert <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Um asynchrone Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und ein Erkennungsvorgang ausführt.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">Das Zeitspanne, in der eine Spracherkennung eine tonlose Eingabe akzeptiert, bevor die Erkennung abgeschlossen wird.</param>
        <summary>Führt einen synchronen Spracherkennungsvorgang mit einem angegebenen ursprünglichen Ruhetimeout aus.</summary>
        <returns>Das Erkennungsergebnis für die Eingabe oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Spracherkennungsmoduls Spracherkennung innerhalb des durch angegebenen Zeitintervalls erkennt `initialSilenceTimeout` Argument <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> eine einzelne Recognition durchgeführt und wird dann beendet.  Die `initialSilenceTimeout` Parameter hat Vorrang vor der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaft.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
 Das Erkennungsmodul wird nicht ausgelöst. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis bei Verwendung dieser Methode.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> Methode gibt ein <xref:System.Speech.Recognition.RecognitionResult> -Objekt, oder `null` , wenn der Vorgang nicht erfolgreich ist.  
  
 Eine synchrone Erkennungsvorgang kann aus den folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder für die `initialSilenceTimeout` Parameter.  
  
-   Das Erkennungsmodul Spracherkennung erkennt jedoch findet keine Übereinstimmungen in einer seiner geladen und aktiviert <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Um asynchrone Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und ein Erkennungsvorgang ausführt.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Startet einen asynchronen Spracherkennungsvorgang.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden führen Sie einzelne oder mehrere asynchrone Erkennungsvorgänge. Das Erkennungsmodul führt jeder Vorgang für seine geladen und aktiviert Speech Recognition Grammatiken.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Wird ausgelöst, wenn ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen ist.  
  
 Um das Ergebnis eines asynchronen Recognition Vorgangs abzurufen, fügen Sie einen Ereignishandler an des Erkennungsmodul <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Das Erkennungsmodul löst dieses Ereignis, wenn eine synchrone oder asynchrone Erkennungsvorgang erfolgreich abgeschlossen wird. Wenn Recognition nicht erfolgreich war, die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> Eigenschaft auf <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> -Objekt, das Sie, in der Handler für zugreifen können die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis werden `null`.  
  
 Eine asynchrone Erkennungsvorgang kann aus den folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften.  
  
-   Das Erkennungsmodul Spracherkennung erkennt jedoch findet keine Übereinstimmungen in einer seiner geladen und aktiviert <xref:System.Speech.Recognition.Grammar> Objekte.  
  
-   Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> benötigen mindestens ein <xref:System.Speech.Recognition.Grammar> Objekt vor dem Ausführen von Recognition geladen. Um eine Sprache Recognition Grammatik zu laden, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
-   Verwenden Sie zum Ändern, wie die Erkennung für die zeitliche Abfolge der Sprache oder Pausen in Bezug auf Erkennung behandelt, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
-   Um synchronen Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Führt einen einzelnen, asynchronen Spracherkennungsvorgang aus.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode führt ein einzelnes, asynchrones Erkennungsvorgang. Das Erkennungsmodul führt den Vorgang anhand seiner geladen und aktiviert Speech Recognition Grammatiken.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Wird ausgelöst, wenn ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen ist.  
  
 Um das Ergebnis eines asynchronen Recognition Vorgangs abzurufen, fügen Sie einen Ereignishandler an des Erkennungsmodul <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Das Erkennungsmodul löst dieses Ereignis, wenn eine synchrone oder asynchrone Erkennungsvorgang erfolgreich abgeschlossen wird. Wenn Recognition nicht erfolgreich war, die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> Eigenschaft auf <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> -Objekt, das Sie, in der Handler für zugreifen können die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis werden `null`.  
  
 Um synchronen Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende asynchrone Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und führt eine asynchrone Erkennungsvorgang. Ereignishandler sind angegeben, um zu veranschaulichen, die Ereignisse, die während des Vorgangs die Erkennung auslöst.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Gibt an, ob einer oder mehrere Erkennungsvorgänge ausgeführt werden.</param>
        <summary>Führt eine oder mehrere asynchrone Spracherkennungsvorgänge aus.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn `mode` ist <xref:System.Speech.Recognition.RecognizeMode.Multiple>, die Erkennung weiterhin Recognition asynchrone Vorgänge, bis die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> Methode wird aufgerufen.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Wird ausgelöst, wenn ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen ist.  
  
 Um das Ergebnis eines asynchronen Recognition Vorgangs abzurufen, fügen Sie einen Ereignishandler an des Erkennungsmodul <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Das Erkennungsmodul löst dieses Ereignis, wenn eine synchrone oder asynchrone Erkennungsvorgang erfolgreich abgeschlossen wird. Wenn Recognition nicht erfolgreich war, die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> Eigenschaft auf <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> -Objekt, das Sie, in der Handler für zugreifen können die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis werden `null`.  
  
 Eine asynchrone Erkennungsvorgang kann aus den folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften.  
  
-   Das Erkennungsmodul Spracherkennung erkennt jedoch findet keine Übereinstimmungen in einer seiner geladen und aktiviert <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Um synchronen Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende asynchrone Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und mehrere asynchrone Recognition Vorgänge ausführt. Die asynchronen Vorgänge sind nach 30 Sekunden abgebrochen. Ereignishandler sind angegeben, um zu veranschaulichen, die Ereignisse, die während des Vorgangs die Erkennung auslöst.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Beendet die asynchrone Erkennung, ohne auf den Abschluss des aktuellen Erkennungsvorgangs zu warten.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode schließt die asynchrone Recognition sofort ab. Wenn die aktuelle asynchrone Erkennungsvorgang Eingabe empfängt, die Eingabe ist abgeschnitten, und der Vorgang abgeschlossen ist, mit der Eingabe vorhandene. Die Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis aus, wenn ein asynchroner Vorgang wird abgebrochen, und legt sie fest der <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> auf `true`. Diese Methode bricht asynchrone Vorgänge gestartet, indem die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Verwenden, um asynchrone Recognition zu beenden, ohne das Abschneiden der Eingabe der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die Verwendung von veranschaulicht die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> Methode. Das Beispiel erstellt und lädt eine Spracherkennung Recognition-Grammatik, initiiert eine fortlaufende asynchrone Erkennungsvorgang und hält dann die 2 Sekunden, bevor sie den Vorgang abbricht. Das Erkennungsmodul empfängt Eingaben aus der Datei c:\temp\audioinput\sample.wav. Ereignishandler sind angegeben, um zu veranschaulichen, die Ereignisse, die während des Vorgangs die Erkennung auslöst.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Deaktiviert die asynchrone Erkennung, nachdem der aktuelle Erkennungsvorgang abgeschlossen ist.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode schließt die asynchrone Recognition ohne Abschneiden Eingabe ab. Wenn die aktuelle asynchrone Erkennungsvorgang Eingabe empfängt, wird die Erkennung fortgesetzt, Eingaben angenommen werden, bis der aktuelle Recognition-Vorgang abgeschlossen ist. Die Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis aus, wenn ein asynchroner Vorgang wird beendet, und legt sie fest der <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> auf `true`. Diese Methode beendet die asynchrone Vorgänge gestartet, indem die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Verwenden, um sofort abzubrechen asynchronen Ansatz mit nur den vorhandenen Eingabe der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die Verwendung von veranschaulicht die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> Methode. Das Beispiel erstellt und lädt eine Spracherkennung Recognition-Grammatik, initiiert eine fortlaufende asynchrone Erkennungsvorgang und hält dann die 2 Sekunden, bevor sie den Vorgang beendet. Das Erkennungsmodul empfängt Eingaben aus der Datei c:\temp\audioinput\sample.wav. Ereignishandler sind angegeben, um zu veranschaulichen, die Ereignisse, die während des Vorgangs die Erkennung auslöst.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> einen asynchronen Erkennungsvorgang abschließt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> des Objekts <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methode initiiert einen asynchronen Recognition-Vorgang. Wenn die Erkennung auf den asynchronen Vorgang abschließt, wird dieses Ereignis ausgelöst.  
  
 Der Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> -Ereignis, die Sie zugreifen können die <xref:System.Speech.Recognition.RecognitionResult> in die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> Objekt. Wenn die Erkennung nicht erfolgreich war, <xref:System.Speech.Recognition.RecognitionResult> werden `null`. Um zu bestimmen, ob ein Timeout oder eine dienstunterbrechung Audioeingabe Recognition Fehler verursacht, können Sie die Eigenschaften für zugreifen <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, oder <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  
  
 Weitere Informationen finden Sie in den Ausführungen zur <xref:System.Speech.Recognition.RecognizeCompletedEventArgs>-Klasse.  
  
 Um die Details für die besten Kandidaten für die abgelehnte Recognition zu erhalten, fügen Sie einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erkennt Ausdrücke wie z. B. "Zeigt eine Liste der Künstler in der Kategorie jazz" oder "Alben Gospel anzeigen". Im Beispiel wird einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis, um Informationen zu den Ergebnissen der Erkennung in der Konsole angezeigt.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die aktuelle Position von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> in der Audioeingabe ab, die verarbeitet wird.</summary>
        <value>Die Position der Erkennung in der Audioeingabe, die sie verarbeitet.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die audioposition bezieht sich auf jede von der Spracherkennung. Der Nullwert eines Eingabedatenstroms wird hergestellt, wenn diese Option aktiviert ist.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Eigenschaftenverweise der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Position des Objekts in seine Audioeingabe. Im Gegensatz dazu, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Eigenschaft verweist auf das Eingabegerät Position in der generierten Audiostream. Diese Positionen können unterschiedlich sein. Z. B. wenn die Erkennung erhalten hat Eingabe nicht für die It hat noch erzeugt ein Erkennungsergebnis wird der Wert der die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> -Eigenschaft muss kleiner als der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Eigenschaft.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft Informationen über die aktuelle Instanz von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ab.</summary>
        <value>Informationen über die aktuelle Spracherkennung.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen von Informationen für alle von der installierten Spracherkennung für das aktuelle System die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
   
  
## Examples  
 Im folgenden Beispiel wird eine Teilliste der Daten für die aktuelle prozessintern Spracherkennungsmoduls. Weitere Informationen finden Sie unter <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn ein aktives <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> angehalten wird, um Änderungen zu übernehmen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Anwendungen müssen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> anhalten eine laufende Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine> vor der Änderung der Einstellungen oder den zugehörigen <xref:System.Speech.Recognition.Grammar> Objekte. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis, wenn er Änderungen akzeptieren kann.  
  
 Während beispielsweise die <xref:System.Speech.Recognition.SpeechRecognitionEngine> wird angehalten, Sie können zu laden, entladen, aktivieren und deaktivieren <xref:System.Speech.Recognition.Grammar> Objekte aus, und ändern Sie die Werte für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaften. Weitere Informationen finden Sie unter der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>-Methode.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel zeigt eine Konsolenanwendung, die geladen und entladen wird <xref:System.Speech.Recognition.Grammar> Objekte. Die Anwendung verwendet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode zum Anfordern der Spracherkennungsmoduls anhalten, damit sie ein Update empfangen kann. Die Anwendung dann lädt oder Entlädt eine <xref:System.Speech.Recognition.Grammar> Objekt.  
  
 Bei jeder Aktualisierung, einen Handler für <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis schreibt den Namen und den Status der derzeit geladenen <xref:System.Speech.Recognition.Grammar> Objekte in die Konsole. Wie Grammatiken geladen und entladen werden, erkennt die Anwendung zuerst die Namen der Farm Tieren auf die Namen von Tieren sowie die Namen der Früchte und dann nur die Namen von Früchten.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie diese Methode, um Änderungen an die Erkennung zu synchronisieren. Beispielsweise, wenn Sie laden oder Entladen von Spracherkennung Recognition Grammatik während die Erkennung Eingabe verarbeitet wird, verwenden Sie diese Methode und die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis, um das Verhalten Ihrer Anwendung mit dem Status der Erkennung zu synchronisieren.  
  
 Wenn diese Methode aufgerufen wird, die Erkennung hält oder asynchrone Vorgänge abgeschlossen und generiert eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis. Ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> -Ereignishandler können Sie den Status der Erkennung zwischen Erkennungsvorgänge ändern. Bei der Verarbeitung von <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignisse, die Erkennung angehalten, bis die Ereignishandler zurückgibt.  
  
> [!NOTE]
>  Wenn die Eingabe für die Erkennung geändert wird, bevor die Erkennung löst ist die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis, die Anforderung wird verworfen.  
  
 Wenn diese Methode aufgerufen wird:  
  
-   Die Erkennung nicht Eingabe verarbeitet, generiert die Erkennung sofort die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
-   Das Erkennungsmodul Eingabe verarbeitet, der Pausen oder Hintergrundgeräuschen besteht, wird die Erkennung hält den Erkennungsvorgang und generiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
-   Das Erkennungsmodul Eingabe verarbeitet, die nicht der stille oder Hintergrundgeräuschen besteht, wird die Erkennung Erkennungsvorgang abgeschlossen und generiert dann das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
 Während die Erkennung verarbeitet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis:  
  
-   Das Erkennungsmodul verarbeitet keine Eingabe, und der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Eigenschaft bleibt unverändert.  
  
-   Das Erkennungsmodul weiterhin erfassen, die Eingabe, und der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> -Eigenschaft ändern.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Bei die Erkennung generiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> -Ereignis der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> ist `null`.  
  
 Um ein Benutzertoken bereitzustellen, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode. Verwenden Sie zum Angeben eines audioposition-Offsets der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt eine Konsolenanwendung, die geladen und entladen wird <xref:System.Speech.Recognition.Grammar> Objekte. Die Anwendung verwendet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode zum Anfordern der Spracherkennungsmoduls anhalten, damit sie ein Update empfangen kann. Die Anwendung dann lädt oder Entlädt eine <xref:System.Speech.Recognition.Grammar> Objekt.  
  
 Bei jeder Aktualisierung, einen Handler für <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis schreibt den Namen und den Status der derzeit geladenen <xref:System.Speech.Recognition.Grammar> Objekte in die Konsole. Wie Grammatiken geladen und entladen werden, erkennt die Anwendung zuerst die Namen der Farm Tieren auf die Namen von Tieren sowie die Namen der Früchte und dann nur die Namen von Früchten.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Benutzerdefinierte Informationen, die Informationen für den Vorgang enthalten.</param>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren und stellt ein Benutzertoken für das zugeordnete Ereignis bereit.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Bei die Erkennung generiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> -Ereignis der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> enthält den Wert des der `userToken` Parameter.  
  
 Verwenden Sie zum Angeben eines audioposition-Offsets der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Benutzerdefinierte Informationen, die Informationen für den Vorgang enthalten.</param>
        <param name="audioPositionAheadToRaiseUpdate">Der Offset von der aktuellen <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />, um die Anforderung zu verzögern.</param>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren und stellt ein Offset für das zugeordnete Ereignis bereit.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Das Erkennungsmodul initiiert die updateanforderung für die Erkennung nicht erst der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> ist gleich den aktuellen <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> plus `audioPositionAheadToRaiseUpdate`.  
  
 Bei die Erkennung generiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> -Ereignis der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> enthält den Wert des der `userToken` Parameter.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Der Audioeingabestream.</param>
        <param name="audioFormat">Das Format der Audioeingabe.</param>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben von einem Audiostream zu empfangen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung während eines Erkennungsvorgangs das Ende des Eingabestreams erreicht ist, schließt der Erkennungsvorgang ab, mit der Eingabe zur Verfügung. Alle nachfolgenden Erkennungsvorgänge können eine Ausnahme generiert, es sei denn, Sie aktualisieren, dass die Eingabe für die Erkennung.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird die Eingabe aus einer Audiodatei example.wav, die die Ausdrücke enthält, "testen, testen eine zwei drei" und "Mister Cooper", die durch eine Pause getrennt. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben vom StandardAudiogerät zu empfangen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel verwendet eine Ausgabe aus dem Standard-Audiogeräts, führt mehrere asynchrone Erkennungsvorgänge und wird beendet, wenn ein Benutzer den Ausdruck utters "exit".  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Deaktiviert die Eingabe zur Spracherkennung.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Konfigurieren der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekt für die keine Eingabe bei Verwendung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden, oder wenn ein Erkennungsmodul vorübergehend offline dauert.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Der Pfad der Datei, die als Eingabe verwendet werden soll.</param>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben von einer Datei im Waveform-Audioformat (.wav) zu erhalten.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn während eines Erkennungsvorgangs das Ende der Eingabedatei für die Erkennung erreicht hat, schließt der Erkennungsvorgang ab, mit der Eingabe zur Verfügung. Alle nachfolgenden Erkennungsvorgänge können eine Ausnahme generiert, es sei denn, Sie aktualisieren, dass die Eingabe für die Erkennung.  
  
   
  
## Examples  
 Im folgenden Beispiel führt eine Erkennung für das Audio in eine WAV-Datei und den erkannten Text in die Konsole geschrieben.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Der Stream, der die Audiodaten enthält.</param>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben von einem Stream zu erhalten, der Daten im Waveform-Audioformat (.wav) enthält.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung während eines Erkennungsvorgangs das Ende des Eingabestreams erreicht ist, schließt der Erkennungsvorgang ab, mit der Eingabe zur Verfügung. Alle nachfolgenden Erkennungsvorgänge können eine Ausnahme generiert, es sei denn, Sie aktualisieren, dass die Eingabe für die Erkennung.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> eine Eingabe erkennt, die es als Sprache identifizieren kann.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede von der Spracherkennung verfügt über einen Algorithmus, um die Unterscheidung zwischen Ruhe und Sprache. Wenn die <xref:System.Speech.Recognition.SpeechRecognitionEngine> führt ein Erkennungsvorgang Spracherkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis aus, wenn der Algorithmus die Eingabe als Sprache identifiziert. Die <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.SpeechDetectedEventArgs> Objekt gibt an, Speicherort im Eingabedatenstrom, auf denen die Erkennung Spracherkennung erkannt. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis vor es keines löst der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignisse.  
  
 Weitere Informationen finden Sie unter der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung für ein Flug Ursprungs- und Zielort Orte auswählen. Die Anwendung erkennt Ausdrücke an, wie z. B. "Ich möchte von Miami aus Chicago, fliegen."  Im Beispiel wird die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis Bericht die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> jedes Mal Sprache erkannt wird.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ein Wort oder Wörter erkannt hat, die möglicherweise eine Komponente von mehreren vollständigen Ausdrücken in einer Grammatik sind.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> generiert zahlreiche <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> Ereignisse, wie er versucht, einen input-Ausdruck zu identifizieren. Sie erreichen den Text der teilweise erkannten Zeichenfolgen in die <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft von der <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> Objekt im Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> Ereignis. Diese Ereignisse behandeln wird in der Regel nur zum Debuggen nützlich.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> wird von <xref:System.Speech.Recognition.RecognitionEventArgs> abgeleitet.  
  
 Weitere Informationen finden Sie unter der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft und die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erkennt Ausdrücke wie z. B. "Die Liste der Künstler in der Kategorie" jazz "anzeigen". Im Beispiel wird die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> Ereignis, um unvollständige Ausdruck Fragmente in der Konsole angezeigt, wie sie erkannt werden.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> eine Eingabe empfängt, die mit keinem seiner geladenen und aktivierten <see cref="T:System.Speech.Recognition.Grammar" />-Objekte übereinstimmt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Das Erkennungsmodul löst dieses Ereignis aus, wenn es feststellt, dass die Eingabe mit ausreichend geladen und aktiviert entspricht keiner <xref:System.Speech.Recognition.Grammar> Objekte. Die <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft von der <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> enthält die abgelehnte <xref:System.Speech.Recognition.RecognitionResult> Objekt. Können Sie den Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis abzurufenden Recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> , die abgelehnt wurden und ihre <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> Bewertungen.  
  
 Wenn die Anwendung Parallelität mit einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz können Sie den Vertrauensgrad, welche Sprache Eingabe akzeptiert oder abgelehnt wird, mit einem der, Ändern der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden. Sie können ändern, wie die Spracherkennung auf nicht-Sprache Eingaben mithilfe reagiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erkennt Ausdrücke wie z. B. "Zeigt eine Liste der Künstler in der Kategorie jazz" oder "Alben Gospel anzeigen". Im Beispiel wird einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis, um eine Benachrichtigung in der Konsole angezeigt, wenn die Spracherkennung Eingabe kann nicht zugeordnet werden, um den Inhalt der Grammatik mit ausreichenden <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> um einen erfolgreichen Erkennung zu erzeugen. Der Ereignishandler zeigt auch die Anerkennung Ergebnis <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> , die aufgrund von wenig vertrauensergebnisse abgelehnt wurden.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> eine Eingabe empfängt, die mit einem seiner geladenen und aktivierten <see cref="T:System.Speech.Recognition.Grammar" />-Objekte übereinstimmt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine mit einer der Erkennungsvorgang Initiieren der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden. Das Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> -Ereignis, wenn es feststellt, dass die Eingabe mindesten einem der geladenen <xref:System.Speech.Recognition.Grammar> Objekte mit einen ausreichenden Grad an Vertrauen Recognition bilden. Die <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft von der <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> enthält die akzeptierte <xref:System.Speech.Recognition.RecognitionResult> Objekt. Handler des <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse erhalten dem erkannten Ausdruck als auch eine Liste der Recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> mit niedrigeren vertrauensergebnisse.  
  
 Wenn die Anwendung Parallelität mit einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz können Sie den Vertrauensgrad, welche Sprache Eingabe akzeptiert oder abgelehnt wird, mit einem der, Ändern der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden.  Sie können ändern, wie die Spracherkennung auf nicht-Sprache Eingaben mithilfe reagiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 Wenn die Erkennung Eingabe erhält, die eine Grammatik entspricht der <xref:System.Speech.Recognition.Grammar> Objekt auslösen kann seine <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis. Die <xref:System.Speech.Recognition.Grammar> des Objekts <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis wird ausgelöst, bevor der Spracherkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Alle Aufgaben, die spezifisch für eine bestimmte Grammatik sollten immer ausgeführt werden, von einem Handler für das <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die Spracherkennung Recognition Grammatik Konstrukte erstellt eine <xref:System.Speech.Recognition.Grammar> Objekt, und lädt sie in der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Recognition ausführen. Im Beispiel wird veranschaulicht, Spracheingabe für eine <xref:System.Speech.Recognition.SpeechRecognitionEngine>, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignissen, die von der von der Spracherkennung ausgelöst.  
  
 Eingabe gesprochen, z. B. "Ich möchte von Chicago nach Miami fliegen" ausgelöst werden ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Sprechen den Ausdruck "Fliegen me aus Houston nach Chicago" wird nicht ausgelöst, eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis.  
  
 Im Beispiel wird einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis anzuzeigenden erfolgreich erkannt wird, Ausdrücke und die Semantik, die sie in der Konsole enthalten.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Entlädt alle <see cref="T:System.Speech.Recognition.Grammar" />-Objekte aus des Erkennungsmodul.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn zurzeit die Erkennung lädt einer <xref:System.Speech.Recognition.Grammar> asynchron ausgeführt wird, diese Methode wartet, bis die <xref:System.Speech.Recognition.Grammar> geladen wird, bevor alle Entladen der <xref:System.Speech.Recognition.Grammar> Objekte aus der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz.  
  
 Verwenden Sie zum Entladen einer bestimmten Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die das synchrone laden und Entladen von Spracherkennung Recognition Grammatiken veranschaulicht.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Das zu entladene Grammatikobjekt.</param>
        <summary>Entlädt ein angegebenes <see cref="T:System.Speech.Recognition.Grammar" />-Objekt aus der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Instanz.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung ausgeführt wird, müssen die Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Anhalten der <xref:System.Speech.Recognition.SpeechRecognitionEngine> -Instanz vor dem Laden, entladen, aktivieren oder Deaktivieren einer <xref:System.Speech.Recognition.Grammar> Objekt. Alle entladen <xref:System.Speech.Recognition.Grammar> Objekte, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die das synchrone laden und Entladen von Spracherkennung Recognition Grammatiken veranschaulicht.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">Die Grammatik wird nicht in dieses Erkennungsmodul geladen, oder das Erkennungsmodul lädt gerade die Grammatik asynchron.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Aktualisiert den Wert einer Einstellung für das Erkennungsmodul.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Erkennung Einstellungen können Zeichenfolge, 64-Bit-Ganzzahl oder Arbeitsspeicher-Adressdaten enthalten. Die folgende Tabelle beschreibt die Einstellungen, die für eine Microsoft-Speech-API (SAPI) definiert sind – Erkennung kompatibel. Die folgenden Einstellungen benötigen denselben Bereich für jeden Erkennungsmodul, die die Einstellung unterstützt. Eine SAPI-kompatible Erkennungsmodul ist nicht erforderlich, diese Einstellungen unterstützt und kann andere Einstellungen unterstützen.  
  
|Name|Beschreibung |  
|----------|-----------------|  
|`ResourceUsage`|Gibt die Erkennung CPU-Auslastung an. Der Bereich liegt zwischen 0 und 100. Der Standardwert ist 50.|  
|`ResponseSpeed`|Gibt die Länge der Pause am Ende der Eingabe eindeutig an, bevor die von der Spracherkennung ein Erkennungsvorgangs abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 Millisekunden (ms). Diese Einstellung bezieht sich auf der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaft. Standard = 150ms.|  
|`ComplexResponseSpeed`|Gibt die Länge der Pause in Millisekunden (ms) am Ende der mehrdeutigen Eingaben an, bevor die von der Spracherkennung ein Erkennungsvorgangs abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 ms. Diese Einstellung bezieht sich auf der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft. Standard = 500 ms.|  
|`AdaptationOn`|Gibt an, ob die Anpassung des Modells acoustic auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`). Der Standardwert ist `1` (ON).|  
|`PersistedBackgroundAdaptation`|Gibt an, ob die Anpassung im Hintergrund auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`), und speichert die Einstellung in der Registrierung. Der Standardwert ist `1` (ON).|  
  
 Um eine der Einstellungen für die Erkennung zurückzugeben, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> Methode.  
  
 Mit Ausnahme von `PersistedBackgroundAdaptation`, Eigenschaftswerte, die festlegen, mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden behalten ihre Gültigkeit nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>, nach denen sie auf die Standardeinstellungen zurückgesetzt.  
  
 Sie können ändern, wie die Spracherkennung auf nicht-Sprache Eingaben mithilfe reagiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der zu aktualisierenden Einstellung.</param>
        <param name="updatedValue">Der neue Wert der Einstellung.</param>
        <summary>Aktualisiert die angegebene Einstellung für die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> mit dem angegebenen ganzzahligen Wert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Mit Ausnahme von `PersistedBackgroundAdaptation`, Eigenschaftswerte, die festlegen, mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methode behalten ihre Gültigkeit nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>, nach denen sie auf die Standardeinstellungen zurückgesetzt. Finden Sie unter <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Beschreibungen der unterstützten Einstellungen.  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung, die die Werte für eine Reihe von den Einstellungen für die Erkennung, die das Gebietsschema En-US unterstützt definierten ausgibt. Im Beispiel aktualisiert die Einstellungen für die Zuverlässigkeit und fragt dann die Erkennung, um die aktualisierten Werte zu überprüfen. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Die Erkennung verfügt über keine Einstellung mit diesem Namen.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der zu aktualisierenden Einstellung.</param>
        <param name="updatedValue">Der neue Wert der Einstellung.</param>
        <summary>Aktualisiert die angegebene Spracherkennungs-Engine-Einstellung mit dem angegebenen Zeichenfolgenwert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Mit Ausnahme von `PersistedBackgroundAdaptation`, Eigenschaftswerte, die festlegen, mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methode behalten ihre Gültigkeit nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>, nach denen sie auf die Standardeinstellungen zurückgesetzt. Finden Sie unter <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Beschreibungen der unterstützten Einstellungen.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Die Erkennung verfügt über keine Einstellung mit diesem Namen.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>