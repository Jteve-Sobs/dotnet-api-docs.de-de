<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata><Meta Name="ms.openlocfilehash" Value="731a6e2969f49158fd92ed67ae07f8c0108806c8" /><Meta Name="ms.sourcegitcommit" Value="1b924db57b3a1cf768d98c21f9b988d6966a0f2b" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="de-DE" /><Meta Name="ms.lasthandoff" Value="03/06/2020" /><Meta Name="ms.locfileid" Value="78738344" /></Metadata><TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Stellt die Möglichkeit bereit, auf eine prozessinterne Spracherkennungs-Engine zuzugreifen und dieses zu verwalten.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine Instanz dieser Klasse für alle installierten Sprach Erkennungsmethoden erstellen. Um Informationen darüber zu erhalten, welche Erkennungs Tools installiert sind, verwenden Sie die statische <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>-Methode.  
  
 Diese Klasse dient zum Prozess internen Ausführen von sprach Erkennungs Modulen und bietet folgende Kontrolle über verschiedene Aspekte der Spracherkennung:  
  
-   Verwenden Sie zum Erstellen eines in-Process-sprach Erkennungs Moduls einen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> Konstruktoren.  
  
-   Verwenden Sie zum Verwalten von sprach Erkennungs Grammatiken die Methoden <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>und <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> und die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A>-Eigenschaft.  
  
-   Um die Eingabe für die Erkennung zu konfigurieren, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>-oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>-Methode.  
  
-   Verwenden Sie zum Durchführen der Spracherkennung die Methode <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>.  
  
-   Verwenden Sie die Eigenschaften <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>, um zu ändern, wie die Erkennung Ruhe oder unerwartete Eingaben behandelt.  
  
-   Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A>-Eigenschaft, um die Anzahl der vom Erkennungs Modul zurückgegebenen Alternativen zu ändern. Die Erkennung gibt Erkennungsergebnisse in einem <xref:System.Speech.Recognition.RecognitionResult> Objekt zurück.  
  
-   Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>-Methode, um Änderungen an der Erkennung zu synchronisieren. Die Erkennung verwendet mehr als einen Thread, um Aufgaben auszuführen.  
  
-   Um die Eingabe für die Erkennung zu emulieren, verwenden Sie die Methoden <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>.  
  
 Das <xref:System.Speech.Recognition.SpeechRecognitionEngine>-Objekt dient ausschließlich zum Verwenden des Prozesses, der das-Objekt instanziiert hat. Im Gegensatz dazu verwendet die <xref:System.Speech.Recognition.SpeechRecognizer> eine einzelne Erkennung für jede Anwendung, die Sie verwenden möchte.  
  
> [!NOTE]
>  Immer <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> aufgerufen, bevor Sie den letzten Verweis auf die Spracherkennung freigeben. Andernfalls werden die verwendeten Ressourcen nicht freigegeben, bis die Garbage Collector die `Finalize` Methode des Erkennungs Moduls aufruft.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht. Da in diesem Beispiel der `Multiple` Modus der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>-Methode verwendet wird, wird die Erkennung ausgeführt, bis Sie das Konsolenfenster schließen oder das Debuggen beenden.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Initialisiert eine neue Instanz der Klasse <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine <xref:System.Speech.Recognition.SpeechRecognitionEngine>-Instanz aus einer der folgenden Methoden erstellen:  
  
-   Die standardmäßige Spracherkennungs-Engine für das System  
  
-   Eine bestimmte Spracherkennungs-Engine, die Sie nach Namen angeben  
  
-   Die standardmäßige Spracherkennungs-Engine für ein Gebiets Schema, das Sie angeben.  
  
-   Eine bestimmte Erkennungs-Engine, die die Kriterien erfüllt, die Sie in einem <xref:System.Speech.Recognition.RecognizerInfo> Objekt angeben.  
  
 Bevor die Spracherkennung mit der Erkennung beginnen kann, müssen Sie mindestens eine sprach Erkennungs Grammatik laden und die Eingabe für die Erkennung konfigurieren.  
  
 Um eine Grammatik zu laden, müssen Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>-oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>-Methode aufzurufen.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Initialisiert eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse mithilfe des angegebenen Standardspracherkennungsmoduls für das System.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Bevor die Spracherkennung die Spracherkennung beginnen kann, müssen Sie mindestens eine Erkennungs Grammatik laden und die Eingabe für die Erkennung konfigurieren.  
  
 Um eine Grammatik zu laden, müssen Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>-oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>-Methode aufzurufen.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Das Gebietsschema, das die Spracherkennung unterstützen muss.</param>
        <summary>Initialisiert mithilfe des angegebenen Standardspracherkennungsmoduls für ein angegebenes Gebietsschema eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows und die System. Speech-API akzeptieren alle gültigen sprach Ländercodes. Um die Spracherkennung mithilfe der im `CultureInfo`-Argument angegebenen Sprache durchzuführen, muss eine sprach Erkennungs-Engine installiert werden, die diesen Sprachen-Ländercode unterstützt. Die Spracherkennungs-Engines, die mit Microsoft Windows 7 ausgeliefert wurden, funktionieren mit den folgenden Programmiersprachen.  
  
-   en-GB. Walisisch (Großbritannien)  
  
-   en-US. Englisch (USA)  
  
-   de-de. Deutsch (Deutschland)  
  
-   es-es. Spanisch (Spanien)  
  
-   fr-FR. Französisch (Frankreich)  
  
-   ja-JP. Japanisch (Japan)  
  
-   zh-CN. Chinesisch (China)  
  
-   zh-tw. Chinesisch (Taiwan)  
  
 Sprachcodes mit zwei Buchstaben, z. b. "en", "fr" oder "es", sind ebenfalls zulässig.  
  
 Bevor die Spracherkennung mit der Erkennung beginnen kann, müssen Sie mindestens eine sprach Erkennungs Grammatik laden und die Eingabe für die Erkennung konfigurieren.  
  
 Um eine Grammatik zu laden, müssen Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>-oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>-Methode aufzurufen.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht und eine Spracherkennung für das Gebiets Schema "en-US" initialisiert.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Keine der installierten Spracherkennungen unterstützen das angegebene Gebietsschema, oder <paramref name="culture" /> ist die invariante Kultur.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="Culture" /> ist <see langword="null" /></exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerInfo As RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">Die Informationen für die angegebene Spracherkennung.</param>
        <summary>Initialisiert eine neue Instanz <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> mithilfe der Informationen in einem <see cref="T:System.Speech.Recognition.RecognizerInfo" />-Objekt, um die zu verwendende Erkennung anzugeben.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine Instanz dieser Klasse für alle installierten Sprach Erkennungsmethoden erstellen. Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>-Methode, um Informationen darüber zu erhalten, welche erkenerer installiert sind.  
  
 Bevor die Spracherkennung mit der Erkennung beginnen kann, müssen Sie mindestens eine sprach Erkennungs Grammatik laden und die Eingabe für die Erkennung konfigurieren.  
  
 Um eine Grammatik zu laden, müssen Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>-oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>-Methode aufzurufen.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht und eine Spracherkennung initialisiert, die die englische Sprache unterstützt.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">Der Tokenname, den die Spracherkennung verwenden soll.</param>
        <summary>Initialisiert eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse mit einem Zeichenfolgenparameter, der den Namen der zu verwendenden Erkennung angibt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Der Tokenname der Erkennung ist der Wert der <xref:System.Speech.Recognition.RecognizerInfo.Id%2A>-Eigenschaft des <xref:System.Speech.Recognition.RecognizerInfo> Objekts, das von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A>-Eigenschaft der Erkennung zurückgegeben wurde. Verwenden Sie die statische <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>-Methode, um eine Auflistung aller installierten erkenungen zu erhalten.  
  
 Bevor die Spracherkennung mit der Erkennung beginnen kann, müssen Sie mindestens eine sprach Erkennungs Grammatik laden und die Eingabe für die Erkennung konfigurieren.  
  
 Um eine Grammatik zu laden, müssen Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>-oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>-Methode aufzurufen.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht und eine Instanz der Spracherkennung 8,0 für Windows (Englisch-US) erstellt.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Keine Spracherkennung mit diesem Tokennamen ist installiert, oder <paramref name="recognizerId" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="recognizerId" /> ist <see langword="null" /></exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Audioformat ab, das von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangen wird.</summary>
        <value>Das Audioformat bei der Eingabe in die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Instanz oder <see langword="null" />, wenn die Eingabe nicht für die NULL-Eingabe konfiguriert ist oder auf diese gesetzt wurde.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Im folgenden Beispiel wird <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> verwendet, um audioformatdaten abzurufen und anzuzeigen.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den Level des Audiosignals ab, die von der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangen wird.</summary>
        <value>Der Audiopegel der Eingabe an die Spracherkennung, von 0 bis 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Der Wert 0 steht für Stille, und 100 stellt das maximale Eingabe Volume dar.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> die Ebene seiner Audioeingabe meldet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Das <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis mehrmals pro Sekunde aus. Die Häufigkeit, mit der das Ereignis ausgelöst wird, hängt von dem Computer ab, auf dem die Anwendung ausgeführt wird.  
  
 Um die Audioebene zum Zeitpunkt des Ereignisses zu erhalten, verwenden Sie die <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A>-Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Um die aktuelle Audioebene der Eingabe für die Erkennung zu erhalten, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A>-Eigenschaft des Erkennungs Moduls.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel wird einem <xref:System.Speech.Recognition.SpeechRecognitionEngine>-Objekt ein Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>-Ereignis hinzugefügt. Der Handler gibt die neue Audioebene an der Konsole aus.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die aktuelle Position im Audiostream ab, die durch das Gerät generiert wird, das die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> mit Eingaben versorgt.</summary>
        <value>Die aktuelle Position im Audiostream, der durch das Eingabegerät generiert wird.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>-Eigenschaft verweist auf die Position des Eingabe Geräts im generierten Audiodatenstrom. Im Gegensatz dazu verweist die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>-Eigenschaft in der Audioeingabe auf die Position des Erkennungs Moduls. Diese Positionen können unterschiedlich sein. Wenn die Erkennung z. b. Eingaben erhalten hat, für die Sie noch kein Erkennungs Ergebnis generiert hat, ist der Wert der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>-Eigenschaft kleiner als der Wert der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>-Eigenschaft.  
  
   
  
## Examples  
 Im folgenden Beispiel verwendet die in-Process-Spracherkennung eine Diktat Grammatik, um Spracheingaben abzugleichen. Ein Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-Ereignis, das die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>und <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> in die Konsole schreibt, wenn die Spracherkennung bei der Eingabe eine Sprache erkennt.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ein Problem im Audiosignal erkennt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Um das Problem zu beheben, verwenden Sie die <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A>-Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel wird ein Ereignishandler definiert, der Informationen zu einem <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>-Ereignis sammelt.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den Status des von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangenen Audiosignals ab.</summary>
        <value>Der Zustand der Audioeingabe für die Spracherkennung.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>-Eigenschaft stellt den audiozustand mit einem Member der <xref:System.Speech.Recognition.AudioState> Enumeration dar.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn sich der Zustand im Audio, das von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfangen wird, ändert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Um den audiozustand zum Zeitpunkt des Ereignisses zu erhalten, verwenden Sie die <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A>-Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Um den aktuellen audiozustand der Eingabe für die Erkennung zu erhalten, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>-Eigenschaft des Erkennungs Moduls. Weitere Informationen zum Audiostatus finden Sie in der <xref:System.Speech.Recognition.AudioState>-Enumeration.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel wird ein Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>-Ereignis verwendet, um bei jeder Änderung die neue <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> der Erkennung in die Konsole zu schreiben. dabei wird ein Member der <xref:System.Speech.Recognition.AudioState>-Enumeration verwendet.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Zeitintervall ab oder legt dieses fest, während dessen eine <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Eingaben akzeptiert, welche nur Hintergrundgeräusche enthalten, bevor die Erkennung abgeschlossen wird.</summary>
        <value>Die Dauer des Zeitintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede Spracherkennung verfügt über einen Algorithmus, um zwischen Ruhe und Sprache zu unterscheiden. Die Erkennung klassifiziert als Hintergrundrauschen alle nicht-stillen Eingaben, die nicht mit der ursprünglichen Regel der geladenen und aktivierten sprach Erkennungs Grammatiken der Erkennung identisch sind. Wenn die Erkennung nur Hintergrundgeräusche und Ruhe Werte innerhalb des getrautem Timeout Intervalls empfängt, schließt die Erkennung diesen Erkennungs Vorgang ab.  
  
-   Für asynchrone Erkennungs Vorgänge löst die Erkennung das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Ereignis aus, bei dem die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType>-Eigenschaft `true`ist und die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType>-Eigenschaft `null`ist.  
  
-   Bei synchronen Erkennungs Vorgängen und Emulationen gibt die Erkennung `null`anstelle eines gültigen <xref:System.Speech.Recognition.RecognitionResult>zurück.  
  
 Wenn das geplappertimeout-Timeout für den Wert 0 festgelegt ist, führt die Erkennung keine Überprüfung der Überprüfung durch. Das Timeout Intervall kann ein beliebiger nicht negativer Wert sein. Der Standardwert ist 0 Sekunden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht, die die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften eines <xref:System.Speech.Recognition.SpeechRecognitionEngine> festlegt, bevor die Spracherkennung initiiert wird. Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignisse der Spracherkennung geben Ereignis Informationen an die Konsole aus, um zu veranschaulichen, wie sich die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> auf Erkennungs Vorgänge auswirken.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Gibt das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt frei.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Gibt das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt frei.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing"><see langword="true" />, um sowohl verwaltete als auch nicht verwaltete Ressourcen freizugeben. <see langword="false" />, um ausschließlich nicht verwaltete Ressourcen freizugeben.</param>
        <summary>Verwirft das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt und gibt Ressourcen frei, die während der Sitzung verwendet werden.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuliert die Eingabe für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden umgehen die systemaudioeingabe und stellen Text für die Erkennung als <xref:System.String> Objekte oder als Array von <xref:System.Speech.Recognition.RecognizedWordUnit> Objekten bereit. Dies kann hilfreich sein, wenn Sie eine Anwendung oder eine Grammatik testen oder Debuggen. Beispielsweise können Sie mithilfe der Emulation ermitteln, ob sich ein Wort in einer Grammatik befindet und welche Semantik zurückgegeben wird, wenn das Wort erkannt wird. Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>-Methode, um Audioeingaben für die sprach Erkennungs-Engine während Emulations Vorgängen zu deaktivieren.  
  
 Die Spracherkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse aus, als ob der Erkennungs Vorgang nicht emuliert wird. Die Erkennung ignoriert neue Zeilen und zusätzliche Leerzeichen und behandelt Interpunktions Zeichen als literaleingaben.  
  
> [!NOTE]
>  Das <xref:System.Speech.Recognition.RecognitionResult> Objekt, das von der Spracherkennung als Antwort auf die emulierten Eingaben generiert wurde, hat den Wert `null` für seine <xref:System.Speech.Recognition.RecognitionResult.Audio%2A>-Eigenschaft.  
  
 Um die asynchrone Erkennung zu emulieren, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>-Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe für den Erkennungsvorgang.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse aus, als ob der Erkennungs Vorgang nicht emuliert wird.  
  
 Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren groß-und Kleinschreibung und Zeichenbreite beim Anwenden von Grammatikregeln auf den Eingabe Ausdruck. Weitere Informationen zu diesem Vergleichstyp finden Sie in den <xref:System.Globalization.CompareOptions> Enumerationswerte <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> und <xref:System.Globalization.CompareOptions.IgnoreWidth>. Die Erkennungs Tools ignorieren auch neue Zeilen und zusätzliche Leerzeichen und behandeln Interpunktions Zeichen als literaleingaben.  
  
   
  
## Examples  
 Das folgende Codebeispiel ist Teil einer Konsolenanwendung, die die emulierten Eingaben, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignisse veranschaulicht, die von der Spracherkennung ausgelöst werden. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> ist <see langword="null" /></exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> ist die leere Zeichenfolge („“).</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (wordUnits As RecognizedWordUnit(), compareOptions As CompareOptions) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Ein Array von Worteinheiten, das die Eingabe für den Erkennungsvorgang enthält.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe bestimmter Wörter für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen den Wörtern und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse aus, als ob der Erkennungs Vorgang nicht emuliert wird.  
  
 Die Erkennung verwendet `compareOptions`, wenn Sie Grammatikregeln auf den Eingabe Ausdruck anwendet. Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren den Fall, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die Erkennung ignoriert immer die Zeichenbreite und ignoriert den Kana-Typ nie. Die Erkennung ignoriert auch neue Zeilen und zusätzliche Leerzeichen und behandelt Interpunktions Zeichen als literaleingaben. Weitere Informationen zu Zeichenbreite und Kana-Typ finden Sie in der <xref:System.Globalization.CompareOptions>-Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> ist <see langword="null" /></exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> enthält mindestens ein <see langword="null" />-Element.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String, compareOptions As CompareOptions) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Der Eingabebegriff für den Erkennungsvorgang.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen dem Ausdruck und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse aus, als ob der Erkennungs Vorgang nicht emuliert wird.  
  
 Die Erkennung verwendet `compareOptions`, wenn Sie Grammatikregeln auf den Eingabe Ausdruck anwendet. Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren den Fall, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die Erkennung ignoriert immer die Zeichenbreite und ignoriert den Kana-Typ nie. Die Erkennung ignoriert auch neue Zeilen und zusätzliche Leerzeichen und behandelt Interpunktions Zeichen als literaleingaben. Weitere Informationen zu Zeichenbreite und Kana-Typ finden Sie in der <xref:System.Globalization.CompareOptions>-Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> ist <see langword="null" /></exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> ist die leere Zeichenfolge („“).</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuliert die Eingabe für die Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden umgehen die systemaudioeingabe und stellen Text für die Erkennung als <xref:System.String> Objekte oder als Array von <xref:System.Speech.Recognition.RecognizedWordUnit> Objekten bereit. Dies kann hilfreich sein, wenn Sie eine Anwendung oder eine Grammatik testen oder Debuggen. Beispielsweise können Sie mithilfe der Emulation ermitteln, ob sich ein Wort in einer Grammatik befindet und welche Semantik zurückgegeben wird, wenn das Wort erkannt wird. Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>-Methode, um Audioeingaben für die sprach Erkennungs-Engine während Emulations Vorgängen zu deaktivieren.  
  
 Die Spracherkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse aus, als ob der Erkennungs Vorgang nicht emuliert wird. Wenn die Erkennung den asynchronen Erkennungs Vorgang abschließt, löst Sie das <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Ereignis aus. Die Erkennung ignoriert neue Zeilen und zusätzliche Leerzeichen und behandelt Interpunktions Zeichen als literaleingaben.  
  
> [!NOTE]
>  Das <xref:System.Speech.Recognition.RecognitionResult> Objekt, das von der Spracherkennung als Antwort auf die emulierten Eingaben generiert wurde, hat den Wert `null` für seine <xref:System.Speech.Recognition.RecognitionResult.Audio%2A>-Eigenschaft.  
  
 Verwenden Sie zum Emulieren der synchronen Erkennung die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>-Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe für den Erkennungsvorgang.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse aus, als ob der Erkennungs Vorgang nicht emuliert wird. Wenn die Erkennung den asynchronen Erkennungs Vorgang abschließt, löst Sie das <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Ereignis aus.  
  
 Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren groß-und Kleinschreibung und Zeichenbreite beim Anwenden von Grammatikregeln auf den Eingabe Ausdruck. Weitere Informationen zu diesem Vergleichstyp finden Sie in den <xref:System.Globalization.CompareOptions> Enumerationswerte <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> und <xref:System.Globalization.CompareOptions.IgnoreWidth>. Die Erkennungs Tools ignorieren auch neue Zeilen und zusätzliche Leerzeichen und behandeln Interpunktions Zeichen als literaleingaben.  
  
   
  
## Examples  
 Das folgende Codebeispiel ist Teil einer Konsolenanwendung, die asynchrone emulierten Eingaben, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignisse veranschaulicht, die von der Spracherkennung ausgelöst werden. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken, oder die Erkennung bearbeitet einen asynchronen Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> ist <see langword="null" /></exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> ist die leere Zeichenfolge („“).</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (wordUnits As RecognizedWordUnit(), compareOptions As CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Ein Array von Worteinheiten, das die Eingabe für den Erkennungsvorgang enthält.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe bestimmter Wörter für die freigegebene Spracherkennung. Dabei wird ein Array von <see cref="T:System.Speech.Recognition.RecognizedWordUnit" />-Objekten statt Audio für die asynchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen den Wörtern und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse aus, als ob der Erkennungs Vorgang nicht emuliert wird. Wenn die Erkennung den asynchronen Erkennungs Vorgang abschließt, löst Sie das <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Ereignis aus.  
  
 Die Erkennung verwendet `compareOptions`, wenn Sie Grammatikregeln auf den Eingabe Ausdruck anwendet. Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren den Fall, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die Erkennungs Tools ignorieren immer die Zeichenbreite und ignorieren den Kana-Typ nie. Die Erkennungs Tools ignorieren auch neue Zeilen und zusätzliche Leerzeichen und behandeln Interpunktions Zeichen als literaleingaben. Weitere Informationen zu Zeichenbreite und Kana-Typ finden Sie in der <xref:System.Globalization.CompareOptions>-Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken, oder die Erkennung bearbeitet einen asynchronen Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> ist <see langword="null" /></exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> enthält mindestens ein <see langword="null" />-Element.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String, compareOptions As CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Der Eingabebegriff für den Erkennungsvorgang.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen dem Ausdruck und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse aus, als ob der Erkennungs Vorgang nicht emuliert wird. Wenn die Erkennung den asynchronen Erkennungs Vorgang abschließt, löst Sie das <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Ereignis aus.  
  
 Die Erkennung verwendet `compareOptions`, wenn Sie Grammatikregeln auf den Eingabe Ausdruck anwendet. Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren den Fall, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die Erkennungs Tools ignorieren immer die Zeichenbreite und ignorieren den Kana-Typ nie. Die Erkennungs Tools ignorieren auch neue Zeilen und zusätzliche Leerzeichen und behandeln Interpunktions Zeichen als literaleingaben. Weitere Informationen zu Zeichenbreite und Kana-Typ finden Sie in der <xref:System.Globalization.CompareOptions>-Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung hat keine geladenen Spracherkennungsgrammatiken, oder die Erkennung bearbeitet einen asynchronen Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> ist <see langword="null" /></exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> ist die leere Zeichenfolge („“).</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> enthält das Flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> oder <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> einen asynchronen Erkennungsvorgang einer emulierten Eingabe abschließt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methode startet einen asynchronen Erkennungs Vorgang. Der <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst das <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Ereignis aus, wenn der asynchrone Vorgang abgeschlossen wird.  
  
 Durch den <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Vorgang können die Ereignisse <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> erhöht werden. Das <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis ist das letzte Ereignis, das die Erkennung für einen bestimmten Vorgang auslöst.  
  
 Wenn die emulierten Erkennung erfolgreich war, können Sie auf das Erkennungs Ergebnis zugreifen, indem Sie eine der folgenden Optionen verwenden:  
  
-   Die <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A>-Eigenschaft im <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs>-Objekt im-Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Ereignis.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>-Eigenschaft im <xref:System.Speech.Recognition.SpeechRecognizedEventArgs>-Objekt im-Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>-Ereignis.  
  
 Wenn die emulierten Erkennung nicht erfolgreich war, wird das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>-Ereignis nicht ausgelöst, und der <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> wird NULL sein.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> wird von <xref:System.ComponentModel.AsyncCompletedEventArgs> abgeleitet.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> wird von <xref:System.Speech.Recognition.RecognitionEventArgs> abgeleitet.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die eine sprach Erkennungs Grammatik lädt und die asynchrone emulierten Eingaben, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignisse veranschaulicht, die von der Spracherkennung ausgelöst werden.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call matches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Ruheintervall auf oder legt dieses fest, welches die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> am Ende von eindeutiger Eingabe akzeptieren wird, bevor ein Erkennungsvorgang finalisiert wird.</summary>
        <value>Die Dauer des Ruheintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung verwendet dieses Timeout Intervall, wenn die Erkennungs Eingabe eindeutig ist. Beispielsweise ist für eine sprach Erkennungs Grammatik, die das Erkennen von "New Game gefällt" oder "New Game" unterstützt, "New Game bitte" eine eindeutige Eingabe, und "New Game" ist eine mehrdeutige Eingabe.  
  
 Diese Eigenschaft bestimmt, wie lange die sprach Erkennungs-Engine auf zusätzliche Eingaben wartet, bevor ein Erkennungs Vorgang abgeschlossen wird. Das Timeout Intervall kann zwischen 0 und 10 Sekunden (einschließlich) liegen. Der Standardwert ist 150 Millisekunden.  
  
 Um das Timeout Intervall für mehrdeutige Eingaben festzulegen, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>-Eigenschaft.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden oder größer als 10 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Ruheintervall auf oder legt dieses fest, welches die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> am Ende von mehrdeutiger Eingabe akzeptieren wird, bevor ein Erkennungsvorgang finalisiert wird.</summary>
        <value>Die Dauer des Ruheintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung verwendet dieses Timeout Intervall, wenn die Erkennungs Eingabe mehrdeutig ist. Beispielsweise ist für eine sprach Erkennungs Grammatik, die das Erkennen von "New Game gefällt" oder "New Game" unterstützt, "New Game bitte" eine eindeutige Eingabe, und "New Game" ist eine mehrdeutige Eingabe.  
  
 Diese Eigenschaft bestimmt, wie lange die sprach Erkennungs-Engine auf zusätzliche Eingaben wartet, bevor ein Erkennungs Vorgang abgeschlossen wird. Das Timeout Intervall kann zwischen 0 und 10 Sekunden (einschließlich) liegen. Der Standardwert ist 500 Millisekunden.  
  
 Um das Timeout Intervall für eindeutige Eingaben festzulegen, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>-Eigenschaft.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden oder größer als 10 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft eine Auflistung der <see cref="T:System.Speech.Recognition.Grammar" />-Objekte ab, die in diese <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Instanz geladen werden.</summary>
        <value>Die Auflistung der <see cref="T:System.Speech.Recognition.Grammar" />-Objekte.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Im folgenden Beispiel werden Informationen für jede sprach Erkennungs Grammatik, die derzeit von einer Spracherkennung geladen wird, an die Konsole ausgegeben.  
  
> [!IMPORTANT]
>  Kopieren Sie die Grammatik Auflistung, um Fehler zu vermeiden, wenn die Auflistung geändert wird, während diese Methode die Elemente der Auflistung auflistet.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Zeitintervall ab oder legt dieses fest, während eine <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Eingaben akzeptiert, welche keine Geräusche enthalten, bevor die Erkennung abgeschlossen wird.</summary>
        <value>Die Dauer des Ruheintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede Spracherkennung verfügt über einen Algorithmus, um zwischen Ruhe und Sprache zu unterscheiden. Wenn die Erkennungs Eingabe während des anfänglichen Ruhe Zeitlimits Ruhe ist, schließt die Erkennung diesen Erkennungs Vorgang ab.  
  
-   Für asynchrone Erkennungs Vorgänge und Emulationen löst die Erkennung das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Ereignis aus, bei dem die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType>-Eigenschaft `true`ist und die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType>-Eigenschaft `null`ist.  
  
-   Bei synchronen Erkennungs Vorgängen und Emulationen gibt die Erkennung `null`anstelle eines gültigen <xref:System.Speech.Recognition.RecognitionResult>zurück.  
  
 Wenn das anfängliche Timeout Intervall auf 0 (null) festgelegt ist, führt die Erkennung keine anfängliche Überprüfung des Ruhe Zeitlimits durch. Das Timeout Intervall kann ein beliebiger nicht negativer Wert sein. Der Standardwert ist 0 Sekunden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht. Im Beispiel werden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften eines <xref:System.Speech.Recognition.SpeechRecognitionEngine> vor dem Initiieren der Spracherkennung festgelegt. Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignisse der Spracherkennung geben Ereignis Informationen an die Konsole aus, um zu veranschaulichen, wie sich die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> Eigenschaften auf Erkennungs Vorgänge auswirken.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft wird auf einen Wert kleiner als 0 Sekunden festgelegt.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Gibt Informationen für alle installierten Spracherkennungen auf dem aktuellen System zurück.</summary>
        <returns>Eine schreibgeschützte Auflistung von <see cref="T:System.Speech.Recognition.RecognizerInfo" />-Objekten, die die installierten Erkennungen beschreiben.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Um Informationen über die aktuelle Erkennung zu erhalten, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A>-Eigenschaft.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht. Das Beispiel verwendet die-Auflistung, die von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>-Methode zurückgegeben wird, um eine Spracherkennung zu suchen, die die englische Sprache unterstützt  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="VB.NET" Value="Public Sub LoadGrammar (grammar As Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Das zu ladende Grammatikobjekt.</param>
        <summary>Lädt synchron ein <see cref="T:System.Speech.Recognition.Grammar" />-Objekt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennung löst eine Ausnahme aus, wenn das <xref:System.Speech.Recognition.Grammar> Objekt bereits geladen wurde, asynchron geladen wird oder nicht in eine Erkennung geladen werden konnte. Das gleiche <xref:System.Speech.Recognition.Grammar> Objekt kann nicht in mehrere Instanzen von <xref:System.Speech.Recognition.SpeechRecognitionEngine>geladen werden. Erstellen Sie stattdessen ein neues <xref:System.Speech.Recognition.Grammar>-Objekt für jede <xref:System.Speech.Recognition.SpeechRecognitionEngine>-Instanz.  
  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> verwenden, um die sprach Erkennungs-Engine anzuhalten, bevor Sie eine Grammatik laden, entladen, aktivieren oder deaktivieren.  
  
 Wenn Sie eine Grammatik laden, ist Sie standardmäßig aktiviert. Verwenden Sie die <xref:System.Speech.Recognition.Grammar.Enabled%2A>-Eigenschaft, um eine geladene Grammatik zu deaktivieren.  
  
 Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>-Methode, um ein <xref:System.Speech.Recognition.Grammar>-Objekt asynchron zu laden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird eine <xref:System.Speech.Recognition.DictationGrammar> erstellt und in eine Spracherkennung geladen.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> ist <see langword="null" /></exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" /> ist in keinem gültigen Zustand.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="VB.NET" Value="Public Sub LoadGrammarAsync (grammar As Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Die zu ladende Spracherkennungsgrammatik.</param>
        <summary>Lädt asynchron eine Spracherkennungsgrammatik.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn das Erkennungs Modul das Laden eines <xref:System.Speech.Recognition.Grammar> Objekts abschließt, löst es ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>-Ereignis aus. Die Erkennung löst eine Ausnahme aus, wenn das <xref:System.Speech.Recognition.Grammar> Objekt bereits geladen wurde, asynchron geladen wird oder nicht in eine Erkennung geladen werden konnte. Das gleiche <xref:System.Speech.Recognition.Grammar> Objekt kann nicht in mehrere Instanzen von <xref:System.Speech.Recognition.SpeechRecognitionEngine>geladen werden. Erstellen Sie stattdessen ein neues <xref:System.Speech.Recognition.Grammar>-Objekt für jede <xref:System.Speech.Recognition.SpeechRecognitionEngine>-Instanz.  
  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> verwenden, um die sprach Erkennungs-Engine anzuhalten, bevor Sie eine Grammatik laden, entladen, aktivieren oder deaktivieren.  
  
 Wenn Sie eine Grammatik laden, ist Sie standardmäßig aktiviert. Verwenden Sie die <xref:System.Speech.Recognition.Grammar.Enabled%2A>-Eigenschaft, um eine geladene Grammatik zu deaktivieren.  
  
 Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>-Methode, um eine sprach Erkennungs Grammatik synchron zu laden.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> ist <see langword="null" /></exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" /> ist in keinem gültigen Zustand.</exception>
        <exception cref="T:System.OperationCanceledException">Der asynchrone Vorgang wurde abgebrochen.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> das asynchrone Laden eines <see cref="T:System.Speech.Recognition.Grammar" />-Objekts beendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>-Methode der Erkennungsmethode initiiert einen asynchronen Vorgang. Das <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis aus, wenn der Vorgang abgeschlossen ist. Um das <xref:System.Speech.Recognition.Grammar> Objekt zu erhalten, das von der Erkennung geladen wurde, verwenden Sie die <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A>-Eigenschaft der zugeordneten <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Um die aktuellen <xref:System.Speech.Recognition.Grammar> Objekte zu erhalten, die die Erkennung geladen hat, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A>-Eigenschaft des Erkennungs Moduls.  
  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> verwenden, um die sprach Erkennungs-Engine anzuhalten, bevor Sie eine Grammatik laden, entladen, aktivieren oder deaktivieren.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel wird eine Prozess interne Spracherkennung erstellt, und anschließend werden zwei Arten von Grammatiken zum Erkennen bestimmter Wörter und zum Akzeptieren der kostenlosen Diktat Erstellung erstellt. In diesem Beispiel wird ein <xref:System.Speech.Recognition.Grammar>-Objekt aus jedem der abgeschlossenen sprach Erkennungs Grammatiken erstellt. Anschließend werden die <xref:System.Speech.Recognition.Grammar>-Objekte asynchron in die <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz geladen. Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>-und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse der Erkennung schreiben in die Konsole den Namen des <xref:System.Speech.Recognition.Grammar> Objekts, das zum Durchführen der Erkennung verwendet wurde, bzw. den Text des Erkennungs Ergebnisses.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die Höchstzahl alternativer Erkennungsergebnisse ab oder legt diese fest, welche die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> für jeden Erkennungsvorgang zurückgibt.</summary>
        <value>Die Anzahl alternativer Ergebnisse, die zurückgegeben werden sollen.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>-Eigenschaft der <xref:System.Speech.Recognition.RecognitionResult>-Klasse enthält die Auflistung von <xref:System.Speech.Recognition.RecognizedPhrase>-Objekten, die mögliche Interpretationen der Eingabe darstellen.  
  
 Der Standardwert für <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> ist 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException"><see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> ist auf einen Wert unter 0 (null) festgelegt.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der zurückzugebenden Einstellung.</param>
        <summary>Gibt die Werte von Einstellungen für das Erkennungsmodul zurück.</summary>
        <returns>Der Wert der Einstellung.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennungseinstellungen können Zeichen folgen-, 64-Bit-ganzzahlige oder Speicher Adressdaten enthalten. In der folgenden Tabelle werden die Einstellungen beschrieben, die für eine SAPI-kompatible Erkennung (Microsoft Speech API) definiert sind. Die folgenden Einstellungen müssen für jede Erkennung, die die-Einstellung unterstützt, denselben Bereich aufweisen. Ein SAPI-kompatibles Erkennungs Modul ist nicht erforderlich, um diese Einstellungen zu unterstützen, und kann andere Einstellungen unterstützen.  
  
|Name|BESCHREIBUNG|  
|----------|-----------------|  
|`ResourceUsage`|Gibt die CPU-Auslastung des Erkennungs Moduls an. Der Bereich liegt zwischen 0 und 100. Der Standardwert lautet "50".|  
|`ResponseSpeed`|Gibt die Länge der Stille am Ende der eindeutigen Eingabe an, bevor die Spracherkennung einen Erkennungs Vorgang abschließt. Der Bereich liegt zwischen 0 und 10.000 Millisekunden (MS). Diese Einstellung entspricht der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>-Eigenschaft der Erkennung.  Standardwert: 150 ms.|  
|`ComplexResponseSpeed`|Gibt die Länge der Stille am Ende der mehrdeutigen Eingabe an, bevor die Spracherkennung einen Erkennungs Vorgang abschließt. Der Bereich liegt zwischen 0 und 10.000 ms. Diese Einstellung entspricht der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>-Eigenschaft der Erkennung. Standardwert = 500 ms.|  
|`AdaptationOn`|Gibt an, ob die Anpassung des Akustik Modells on (Value = `1`) oder Off (Value = `0`) erfolgt. Der Standardwert ist `1` (on).|  
|`PersistedBackgroundAdaptation`|Gibt an, ob die Hintergrund Anpassung on (Value = `1`) oder Off (Value = `0`) ist, und speichert die Einstellung in der Registrierung. Der Standardwert ist `1` (on).|  
  
 Um eine Einstellung für die Erkennung zu aktualisieren, verwenden Sie eine der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die die Werte für eine Reihe von Einstellungen ausgibt, die für die Erkennung definiert sind, die das Gebiets Schema "en-US" unterstützt. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> ist <see langword="null" /></exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> ist die leere Zeichenfolge („“).</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Die Erkennung verfügt über keine Einstellung mit diesem Namen.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Startet einen synchronen Spracherkennungsvorgang.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden führen einen einzelnen synchronen Erkennungs Vorgang aus. Die Erkennung führt diesen Vorgang für die geladenen und aktivierten sprach Erkennungs Grammatiken aus.  
  
 Während eines Aufrufes dieser Methode kann die Erkennung die folgenden Ereignisse abrufen:  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>).  Wird ausgelöst, wenn die Erkennung Eingaben erkennt, die Sie als Sprache identifizieren kann.  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>).  Wird ausgelöst, wenn die Eingabe eine mehrdeutige Entsprechung mit einer der aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Wird ausgelöst, wenn die Erkennung einen Erkennungs Vorgang einschließt.  
  
 Die Erkennung gibt das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Ereignis nicht aus, wenn eine der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>-Methoden verwendet wird.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>-Methoden geben ein <xref:System.Speech.Recognition.RecognitionResult> Objekt zurück oder `null`, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.  
  
 Ein synchroner Erkennungs Vorgang kann aus folgenden Gründen fehlschlagen:  
  
-   Die Sprache wird nicht erkannt, bevor die Timeout Intervalle für die Eigenschaften <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> oder für den `initialSilenceTimeout`-Parameter der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>-Methode ablaufen.  
  
-   Die Erkennungs-Engine erkennt die Sprache, findet jedoch keine Übereinstimmungen in einem Ihrer geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Verwenden Sie die Eigenschaften "<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>", "<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>", "<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>" und "<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>", um zu ändern, wie die Erkennung die zeitliche Steuerung oder Stille in Bezug auf die Erkennung behandelt.  
  
 Der <xref:System.Speech.Recognition.SpeechRecognitionEngine> muss mindestens ein <xref:System.Speech.Recognition.Grammar> Objekt vor dem Durchführen der Erkennung geladen haben. Um eine sprach Erkennungs Grammatik zu laden, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>-oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>-Methode.  
  
 Verwenden Sie zum Ausführen einer asynchronen Erkennung eine der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Führt einen synchronen Spracherkennungsvorgang aus.</summary>
        <returns>Das Erkennungsergebnis für die Eingabe oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode führt einen einzelnen Erkennungs Vorgang aus. Die Erkennung führt diesen Vorgang für die geladenen und aktivierten sprach Erkennungs Grammatiken aus.  
  
 Während eines Aufrufes dieser Methode kann die Erkennung die folgenden Ereignisse abrufen:  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>).  Wird ausgelöst, wenn die Erkennung Eingaben erkennt, die Sie als Sprache identifizieren kann.  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>).  Wird ausgelöst, wenn die Eingabe eine mehrdeutige Entsprechung mit einer der aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Wird ausgelöst, wenn die Erkennung einen Erkennungs Vorgang einschließt.  
  
 Die Erkennung gibt das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Ereignis nicht aus, wenn diese Methode verwendet wird.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize>-Methode gibt ein <xref:System.Speech.Recognition.RecognitionResult> Objekt zurück oder `null`, wenn der Vorgang nicht erfolgreich ist.  
  
 Ein synchroner Erkennungs Vorgang kann aus folgenden Gründen fehlschlagen:  
  
-   Die Sprache wird nicht erkannt, bevor die Timeout Intervalle für die Eigenschaften <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> ablaufen.  
  
-   Die Erkennungs-Engine erkennt die Sprache, findet jedoch keine Übereinstimmungen in einem Ihrer geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Verwenden Sie zum Ausführen einer asynchronen Erkennung eine der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird eine <xref:System.Speech.Recognition.DictationGrammar>erstellt, in eine Prozess interne Spracherkennung geladen und ein Erkennungs Vorgang durchführt.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">Das Zeitspanne, in der eine Spracherkennung eine tonlose Eingabe akzeptiert, bevor die Erkennung abgeschlossen wird.</param>
        <summary>Führt einen synchronen Spracherkennungsvorgang mit einem angegebenen ursprünglichen Ruhetimeout aus.</summary>
        <returns>Das Erkennungsergebnis für die Eingabe oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder die Erkennung nicht aktiviert ist.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Spracherkennungs-Engine Sprache innerhalb des Zeitintervalls erkennt, das durch `initialSilenceTimeout`-Argument angegeben wird, führt <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> einen einzelnen Erkennungs Vorgang aus und wird dann beendet.  Der `initialSilenceTimeout`-Parameter ersetzt die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>-Eigenschaft des Erkennungs Moduls.  
  
 Während eines Aufrufes dieser Methode kann die Erkennung die folgenden Ereignisse abrufen:  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>).  Wird ausgelöst, wenn die Erkennung Eingaben erkennt, die Sie als Sprache identifizieren kann.  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>).  Wird ausgelöst, wenn die Eingabe eine mehrdeutige Entsprechung mit einer der aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Wird ausgelöst, wenn die Erkennung einen Erkennungs Vorgang einschließt.  
  
 Die Erkennung gibt das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Ereignis nicht aus, wenn diese Methode verwendet wird.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize>-Methode gibt ein <xref:System.Speech.Recognition.RecognitionResult> Objekt zurück oder `null`, wenn der Vorgang nicht erfolgreich ist.  
  
 Ein synchroner Erkennungs Vorgang kann aus folgenden Gründen fehlschlagen:  
  
-   Die Sprache wird nicht erkannt, bevor die Timeout Intervalle für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder für den `initialSilenceTimeout` Parameter ablaufen.  
  
-   Die Erkennungs-Engine erkennt die Sprache, findet jedoch keine Übereinstimmungen in einem Ihrer geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Verwenden Sie zum Ausführen einer asynchronen Erkennung eine der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird eine <xref:System.Speech.Recognition.DictationGrammar>erstellt, in eine Prozess interne Spracherkennung geladen und ein Erkennungs Vorgang durchführt.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Startet einen asynchronen Spracherkennungsvorgang.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden führen einzelne oder mehrere asynchrone Erkennungs Vorgänge aus. Die Erkennung führt jeden Vorgang anhand der geladenen und aktivierten sprach Erkennungs Grammatiken aus.  
  
 Während eines Aufrufes dieser Methode kann die Erkennung die folgenden Ereignisse abrufen:  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>).  Wird ausgelöst, wenn die Erkennung Eingaben erkennt, die Sie als Sprache identifizieren kann.  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>).  Wird ausgelöst, wenn die Eingabe eine mehrdeutige Entsprechung mit einer der aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Wird ausgelöst, wenn die Erkennung einen Erkennungs Vorgang einschließt.  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>). Wird ausgelöst, wenn ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen wird.  
  
 Um das Ergebnis eines asynchronen Erkennungs Vorgangs abzurufen, fügen Sie einen Ereignishandler an das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis der Erkennung an. Die Erkennung löst dieses Ereignis aus, wenn Sie einen synchronen oder asynchronen Erkennungs Vorgang erfolgreich abgeschlossen hat. Wenn die Erkennung nicht erfolgreich war, wird die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A>-Eigenschaft für <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> Objekt, auf das Sie im Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Ereignis zugreifen können, `null`.  
  
 Ein asynchroner Erkennungs Vorgang kann aus folgenden Gründen fehlschlagen:  
  
-   Die Sprache wird nicht erkannt, bevor die Timeout Intervalle für die Eigenschaften <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> ablaufen.  
  
-   Die Erkennungs-Engine erkennt die Sprache, findet jedoch keine Übereinstimmungen in einem Ihrer geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte.  
  
-   Der <xref:System.Speech.Recognition.SpeechRecognitionEngine> muss mindestens ein <xref:System.Speech.Recognition.Grammar> Objekt vor dem Durchführen der Erkennung geladen haben. Um eine sprach Erkennungs Grammatik zu laden, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>-oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>-Methode.  
  
-   Verwenden Sie die Eigenschaften "<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>", "<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>", "<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>" und "<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>", um zu ändern, wie die Erkennung die zeitliche Steuerung oder Stille in Bezug auf die Erkennung behandelt.  
  
-   Verwenden Sie zum Durchführen einer synchronen Erkennung eine der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Führt einen einzelnen, asynchronen Spracherkennungsvorgang aus.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode führt einen einzelnen asynchronen Erkennungs Vorgang aus. Die Erkennung führt den Vorgang für die geladenen und aktivierten sprach Erkennungs Grammatiken aus.  
  
 Während eines Aufrufes dieser Methode kann die Erkennung die folgenden Ereignisse abrufen:  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>).  Wird ausgelöst, wenn die Erkennung Eingaben erkennt, die Sie als Sprache identifizieren kann.  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>).  Wird ausgelöst, wenn die Eingabe eine mehrdeutige Entsprechung mit einer der aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Wird ausgelöst, wenn die Erkennung einen Erkennungs Vorgang einschließt.  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>). Wird ausgelöst, wenn ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen wird.  
  
 Um das Ergebnis eines asynchronen Erkennungs Vorgangs abzurufen, fügen Sie einen Ereignishandler an das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis der Erkennung an. Die Erkennung löst dieses Ereignis aus, wenn Sie einen synchronen oder asynchronen Erkennungs Vorgang erfolgreich abgeschlossen hat. Wenn die Erkennung nicht erfolgreich war, wird die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A>-Eigenschaft für <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> Objekt, auf das Sie im Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Ereignis zugreifen können, `null`.  
  
 Verwenden Sie zum Durchführen einer synchronen Erkennung eine der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende asynchrone Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt Sie in eine Prozess interne Spracherkennung und führt einen asynchronen Erkennungs Vorgang aus. Ereignishandler sind enthalten, um die Ereignisse zu veranschaulichen, die die Erkennung während des Vorgangs auslöst.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Gibt an, ob einer oder mehrere Erkennungsvorgänge ausgeführt werden.</param>
        <summary>Führt eine oder mehrere asynchrone Spracherkennungsvorgänge aus.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn `mode` <xref:System.Speech.Recognition.RecognizeMode.Multiple>ist, führt die Erkennung weiterhin asynchrone Erkennungs Vorgänge aus, bis die Methode <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> aufgerufen wird.  
  
 Während eines Aufrufes dieser Methode kann die Erkennung die folgenden Ereignisse abrufen:  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>).  Wird ausgelöst, wenn die Erkennung Eingaben erkennt, die Sie als Sprache identifizieren kann.  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>).  Wird ausgelöst, wenn die Eingabe eine mehrdeutige Entsprechung mit einer der aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Wird ausgelöst, wenn die Erkennung einen Erkennungs Vorgang einschließt.  
  
-   [https://login.microsoftonline.com/consumers/](<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>). Wird ausgelöst, wenn ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen wird.  
  
 Um das Ergebnis eines asynchronen Erkennungs Vorgangs abzurufen, fügen Sie einen Ereignishandler an das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis der Erkennung an. Die Erkennung löst dieses Ereignis aus, wenn Sie einen synchronen oder asynchronen Erkennungs Vorgang erfolgreich abgeschlossen hat. Wenn die Erkennung nicht erfolgreich war, wird die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A>-Eigenschaft für <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> Objekt, auf das Sie im Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Ereignis zugreifen können, `null`.  
  
 Ein asynchroner Erkennungs Vorgang kann aus folgenden Gründen fehlschlagen:  
  
-   Die Sprache wird nicht erkannt, bevor die Timeout Intervalle für die Eigenschaften <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> ablaufen.  
  
-   Die Erkennungs-Engine erkennt die Sprache, findet jedoch keine Übereinstimmungen in einem Ihrer geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Verwenden Sie zum Durchführen einer synchronen Erkennung eine der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende asynchrone Spracherkennung veranschaulicht. Im Beispiel wird eine <xref:System.Speech.Recognition.DictationGrammar>erstellt, in eine Prozess interne Spracherkennung geladen, und es werden mehrere asynchrone Erkennungs Vorgänge durchführt. Die asynchronen Vorgänge werden nach 30 Sekunden abgebrochen. Ereignishandler sind enthalten, um die Ereignisse zu veranschaulichen, die die Erkennung während des Vorgangs auslöst.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Beendet die asynchrone Erkennung, ohne auf den Abschluss des aktuellen Erkennungsvorgangs zu warten.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode schließt die asynchrone Erkennung sofort ab. Wenn der aktuelle asynchrone Erkennungs Vorgang Eingaben empfängt, wird die Eingabe abgeschnitten, und der Vorgang wird mit der vorhandenen Eingabe abgeschlossen. Die Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Ereignis aus, wenn ein asynchroner Vorgang abgebrochen wird, und legt die <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A>-Eigenschaft der <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> auf `true`fest. Diese Methode bricht asynchrone Vorgänge ab, die von den Methoden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> initiiert werden.  
  
 Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A>-Methode, um die asynchrone Erkennung ohne Abschneiden der Eingabe zu verhindern.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die Verwendung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A>-Methode veranschaulicht. Im Beispiel wird eine sprach Erkennungs Grammatik erstellt und geladen, ein fortlaufender asynchroner Erkennungs Vorgang initiiert und dann zwei Sekunden angehalten, bevor der Vorgang abgebrochen wird. Die Erkennung empfängt Eingaben von der Datei c:\temp\audioinput\sample.wav. Ereignishandler sind enthalten, um die Ereignisse zu veranschaulichen, die die Erkennung während des Vorgangs auslöst.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Deaktiviert die asynchrone Erkennung, nachdem der aktuelle Erkennungsvorgang abgeschlossen ist.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode schließt die asynchrone Erkennung ab, ohne Eingaben zu kürzen. Wenn der aktuelle asynchrone Erkennungs Vorgang Eingaben empfängt, akzeptiert die Erkennung weiterhin Eingaben, bis der aktuelle Erkennungs Vorgang abgeschlossen ist. Die Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Ereignis aus, wenn ein asynchroner Vorgang beendet wird, und legt die <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A>-Eigenschaft der <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> auf `true`fest. Diese Methode beendet asynchrone Vorgänge, die von den Methoden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> initiiert werden.  
  
 Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A>-Methode, um die asynchrone Erkennung nur mit der vorhandenen Eingabe sofort abzubrechen.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die Verwendung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A>-Methode veranschaulicht. Im Beispiel wird eine sprach Erkennungs Grammatik erstellt und geladen, ein fortlaufender asynchroner Erkennungs Vorgang initiiert und dann zwei Sekunden angehalten, bevor der Vorgang beendet wird. Die Erkennung empfängt Eingaben von der Datei c:\temp\audioinput\sample.wav. Ereignishandler sind enthalten, um die Ereignisse zu veranschaulichen, die die Erkennung während des Vorgangs auslöst.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> einen asynchronen Erkennungsvorgang abschließt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>-Methode des <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekts initiiert einen asynchronen Erkennungs Vorgang. Wenn die Erkennung den asynchronen Vorgang schließt, löst Sie dieses Ereignis aus.  
  
 Mithilfe des-Handlers für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Ereignis können Sie auf die <xref:System.Speech.Recognition.RecognitionResult> im <xref:System.Speech.Recognition.RecognizeCompletedEventArgs>-Objekt zugreifen. Wenn die Erkennung nicht erfolgreich war, werden <xref:System.Speech.Recognition.RecognitionResult> `null`. Um zu ermitteln, ob ein Timeout oder eine Unterbrechung der Audioeingabe zu einem Fehler bei der Erkennung geführt hat, können Sie auf die Eigenschaften für <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>oder <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>zugreifen.  
  
 Weitere Informationen finden Sie in den Ausführungen zur <xref:System.Speech.Recognition.RecognizeCompletedEventArgs>-Klasse.  
  
 Zum Abrufen von Details zu den besten abgelehnten Erkennungs Kandidaten fügen Sie einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis an.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel werden Ausdrücke wie "Anzeigen der Liste der Künstler in der Kategorie" Jazz "oder" Anzeigen von Alben (Gospel) "erkannt. Im Beispiel wird ein Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis verwendet, um Informationen zu den Ergebnissen der Erkennung in der-Konsole anzuzeigen.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die aktuelle Position von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> in der Audioeingabe ab, die verarbeitet wird.</summary>
        <value>Die Position der Erkennung in der Audioeingabe, die sie verarbeitet.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die AudioPosition ist spezifisch für jede Spracherkennung. Der Wert 0 (null) eines Eingabedaten Stroms wird festgelegt, wenn er aktiviert ist.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>-Eigenschaft verweist in der Audioeingabe auf die Position des <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekts. Im Gegensatz dazu verweist die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>-Eigenschaft im generierten Audiostream auf die Position des Eingabe Geräts. Diese Positionen können unterschiedlich sein. Wenn die Erkennung z. b. Eingaben erhalten hat, für die Sie noch kein Erkennungs Ergebnis generiert hat, ist der Wert der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>-Eigenschaft kleiner als der Wert der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>-Eigenschaft.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft Informationen über die aktuelle Instanz von <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ab.</summary>
        <value>Informationen über die aktuelle Spracherkennung.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>-Methode, um Informationen zu allen installierten Sprach Erkennungsprogrammen für das aktuelle System zu erhalten.  
  
   
  
## Examples  
 Im folgenden Beispiel wird eine partielle Liste von Daten für die aktuelle in-Process-Spracherkennungs-Engine abgerufen. Weitere Informationen finden Sie unter <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn ein aktives <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> angehalten wird, um Änderungen zu übernehmen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Anwendungen müssen <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> verwenden, um eine laufende Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine> anzuhalten, bevor Sie Ihre Einstellungen oder <xref:System.Speech.Recognition.Grammar> Objekte ändern. Das <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis aus, wenn es bereit ist, Änderungen zu akzeptieren.  
  
 Wenn der <xref:System.Speech.Recognition.SpeechRecognitionEngine> beispielsweise angehalten ist, können Sie <xref:System.Speech.Recognition.Grammar> Objekte laden, entladen, aktivieren und deaktivieren und Werte für die Eigenschaften <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> ändern. Weitere Informationen finden Sie unter der Methode <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt eine Konsolenanwendung, die <xref:System.Speech.Recognition.Grammar>-Objekte lädt und lädt. Die Anwendung verwendet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>-Methode, um die Spracherkennungs-Engine aufzufordern, anzuhalten, damit ein Update empfangen werden kann. Die Anwendung lädt dann ein <xref:System.Speech.Recognition.Grammar> Objekt oder entlädt es.  
  
 Bei jedem Update schreibt ein Handler für <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis den Namen und den Status der aktuell geladenen <xref:System.Speech.Recognition.Grammar> Objekte in die Konsole. Wenn Grammatiken geladen und entladen werden, erkennt die Anwendung zuerst die Namen von Farm-animals, dann die Namen der Farm-und die Namen der Früchte und dann nur die Namen der Früchte.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie diese Methode, um Änderungen an der Erkennung zu synchronisieren. Wenn Sie z. b. eine sprach Erkennungs Grammatik laden oder entladen, während die Erkennung Eingaben verarbeitet, verwenden Sie diese Methode und das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>-Ereignis, um das Anwendungsverhalten mit dem Zustand der Erkennung zu synchronisieren.  
  
 Wenn diese Methode aufgerufen wird, wird die Erkennung angehalten oder beendet, und es wird ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis generiert. Ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignishandler kann dann den Zustand der Erkennung in zwischen Erkennungs Vorgängen ändern. Bei der Behandlung von <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignissen wird die Erkennung angehalten, bis der Ereignishandler zurückgibt.  
  
> [!NOTE]
>  Wenn die Eingabe für die Erkennung geändert wird, bevor die Erkennung das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>-Ereignis auslöst, wird die Anforderung verworfen.  
  
 Wenn diese Methode aufgerufen wird:  
  
-   Wenn die Erkennung keine Eingaben verarbeitet, generiert die Erkennung sofort das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
-   Wenn die Erkennung Eingaben verarbeitet, die aus Ruhe-oder Hintergrundrauschen bestehen, hält die Erkennung den Erkennungs Vorgang an und generiert das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
-   Wenn die Erkennung Eingaben verarbeitet, die nicht aus Ruhe-oder Hintergrundrauschen bestehen, schließt die Erkennung den Erkennungs Vorgang ab und generiert dann das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
 Während die Erkennung das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis behandelt:  
  
-   Die Erkennung verarbeitet die Eingabe nicht, und der Wert der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>-Eigenschaft bleibt unverändert.  
  
-   Die Erkennung erfasst weiterhin Eingaben, und der Wert der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>-Eigenschaft kann sich ändern.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis generiert, wird die <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A>-Eigenschaft der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> `null`.  
  
 Um ein Benutzer Token bereitzustellen, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>-oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>-Methode. Um einen audiopositions Offset anzugeben, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>-Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt eine Konsolenanwendung, die <xref:System.Speech.Recognition.Grammar>-Objekte lädt und lädt. Die Anwendung verwendet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>-Methode, um die Spracherkennungs-Engine aufzufordern, anzuhalten, damit ein Update empfangen werden kann. Die Anwendung lädt dann ein <xref:System.Speech.Recognition.Grammar> Objekt oder entlädt es.  
  
 Bei jedem Update schreibt ein Handler für <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis den Namen und den Status der aktuell geladenen <xref:System.Speech.Recognition.Grammar> Objekte in die Konsole. Wenn Grammatiken geladen und entladen werden, erkennt die Anwendung zuerst die Namen von Farm-animals, dann die Namen der Farm-und die Namen der Früchte und dann nur die Namen der Früchte.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Benutzerdefinierte Informationen, die Informationen für den Vorgang enthalten.</param>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren und stellt ein Benutzertoken für das zugeordnete Ereignis bereit.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>-Ereignis generiert, enthält die <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A>-Eigenschaft des <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> den Wert des `userToken`-Parameters.  
  
 Um einen audiopositions Offset anzugeben, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>-Methode.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Benutzerdefinierte Informationen, die Informationen für den Vorgang enthalten.</param>
        <param name="audioPositionAheadToRaiseUpdate">Der Offset von der aktuellen <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />, um die Anforderung zu verzögern.</param>
        <summary>Fordert an, dass die Erkennung anhält, um den Zustand zu aktualisieren und stellt ein Offset für das zugeordnete Ereignis bereit.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennung initiiert die Aktualisierungs Anforderung des Erkennungs Moduls erst, wenn die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> des Erkennungs Moduls der aktuellen <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Plus `audioPositionAheadToRaiseUpdate`entspricht.  
  
 Wenn die Erkennung das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>-Ereignis generiert, enthält die <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A>-Eigenschaft des <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> den Wert des `userToken`-Parameters.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Der Audioeingabestream.</param>
        <param name="audioFormat">Das Format der Audioeingabe.</param>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben von einem Audiostream zu empfangen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung beim Erkennungs Vorgang das Ende des Eingabedaten Stroms erreicht, wird der Erkennungs Vorgang mit der verfügbaren Eingabe abgeschlossen. Alle nachfolgenden Erkennungs Vorgänge können eine Ausnahme generieren, es sei denn, Sie aktualisieren die Eingabe in die Erkennung.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht. Das Beispiel verwendet Eingaben aus einer Audiodatei, z. b. wav, die die Ausdrücke "Test Test 1 2 3" und "Mister Cooper" durch eine Pause enthält. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben vom StandardAudiogerät zu empfangen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die grundlegende Spracherkennung veranschaulicht. In dem Beispiel wird die Ausgabe des standardaudiogeräts verwendet. dabei werden mehrere asynchrone Erkennungs Vorgänge durchführt, und die Ausführung wird beendet, wenn ein Benutzer den Ausdruck "Exit" verwendet.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Deaktiviert die Eingabe zur Spracherkennung.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Konfigurieren Sie das <xref:System.Speech.Recognition.SpeechRecognitionEngine>-Objekt für keine Eingabe, wenn Sie die Methoden <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> verwenden, oder wenn Sie eine Erkennungs-Engine vorübergehend offline schalten.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Der Pfad der Datei, die als Eingabe verwendet werden soll.</param>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben von einer Datei im Waveform-Audioformat (.wav) zu erhalten.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung beim Erkennungs Vorgang das Ende der Eingabedatei erreicht, wird der Erkennungs Vorgang mit der verfügbaren Eingabe abgeschlossen. Alle nachfolgenden Erkennungs Vorgänge können eine Ausnahme generieren, es sei denn, Sie aktualisieren die Eingabe in die Erkennung.  
  
   
  
## Examples  
 Im folgenden Beispiel wird die-Audioerkennung in einer WAV-Datei durchführt, und der erkannte Text wird in die Konsole geschrieben.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Der Stream, der die Audiodaten enthält.</param>
        <summary>Konfiguriert das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt, um Eingaben von einem Stream zu erhalten, der Daten im Waveform-Audioformat (.wav) enthält.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung beim Erkennungs Vorgang das Ende des Eingabedaten Stroms erreicht, wird der Erkennungs Vorgang mit der verfügbaren Eingabe abgeschlossen. Alle nachfolgenden Erkennungs Vorgänge können eine Ausnahme generieren, es sei denn, Sie aktualisieren die Eingabe in die Erkennung.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> eine Eingabe erkennt, die es als Sprache identifizieren kann.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede Spracherkennung verfügt über einen Algorithmus, um zwischen Ruhe und Sprache zu unterscheiden. Wenn die <xref:System.Speech.Recognition.SpeechRecognitionEngine> einen sprach Erkennungs Vorgang ausführt, löst Sie das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-Ereignis aus, wenn der Algorithmus die Eingabe als Sprache identifiziert. Die <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A>-Eigenschaft des zugeordneten <xref:System.Speech.Recognition.SpeechDetectedEventArgs> Objekts gibt den Speicherort im Eingabestream an, in dem die Erkennung eine Sprache erkannt hat. Der <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-Ereignis aus, bevor es die Ereignisse <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> auslöst.  
  
 Weitere Informationen finden Sie in den Methoden <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, mit der die Ursprungs-und Zielstädte für einen Flug ausgewählt werden. Die Anwendung erkennt Ausdrücke, z. b. "Ich möchte von Miami nach Chicago fliegen."  Das Beispiel verwendet das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-Ereignis, um die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> bei jedem erkennen der Sprache zu melden.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ein Wort oder Wörter erkannt hat, die möglicherweise eine Komponente von mehreren vollständigen Ausdrücken in einer Grammatik sind.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Der <xref:System.Speech.Recognition.SpeechRecognitionEngine> generiert zahlreiche <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> Ereignisse, während er versucht, einen Eingabe Ausdruck zu identifizieren. Sie können auf den Text von teilweise erkannten Ausdrücken in der <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>-Eigenschaft des <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs>-Objekts im-Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-Ereignis zugreifen. In der Regel ist die Verarbeitung dieser Ereignisse nur für das Debuggen hilfreich.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> wird von <xref:System.Speech.Recognition.RecognitionEventArgs> abgeleitet.  
  
 Weitere Informationen finden Sie in der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>-Eigenschaft und in den Methoden <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel werden Ausdrücke wie "Anzeigen der Liste der Künstler in der Kategorie" Jazz "erkannt. Das Beispiel verwendet das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-Ereignis, um unvollständige Ausdrucks Fragmente in der-Konsole anzuzeigen, wenn Sie erkannt werden.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> eine Eingabe empfängt, die mit keinem seiner geladenen und aktivierten <see cref="T:System.Speech.Recognition.Grammar" />-Objekte übereinstimmt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Das Erkennungs Modul löst dieses Ereignis aus, wenn es feststellt, dass die Eingabe nicht mit dem ausreichenden Vertrauen eines der geladenen und aktivierten <xref:System.Speech.Recognition.Grammar> Objekte in Einklang steht. Die <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>-Eigenschaft des <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> enthält das abgelehnte <xref:System.Speech.Recognition.RecognitionResult>-Objekt. Sie können den-Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-Ereignis verwenden, um Erkennungs <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> und deren <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> Ergebnisse abzurufen.  
  
 Wenn Ihre Anwendung eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz verwendet, können Sie den Vertrauensgrad ändern, mit dem Spracheingaben mit einer der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden akzeptiert oder abgelehnt werden. Mithilfe der Eigenschaften <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> können Sie ändern, wie die Spracherkennung auf nicht Spracheingaben antwortet.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel werden Ausdrücke wie "Anzeigen der Liste der Künstler in der Kategorie" Jazz "oder" Anzeigen von Alben (Gospel) "erkannt. Im Beispiel wird ein Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis verwendet, um eine Benachrichtigung in der-Konsole anzuzeigen, wenn die Spracheingabe nicht mit dem Inhalt der Grammatik übereinstimmen kann, der ausreichend <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> ist, um erfolgreich erkannt zu werden. Der Handler zeigt auch Erkennungs Ergebnis <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> an, die aufgrund von niedrigen Vertrauens Bewertungen abgelehnt wurden.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> eine Eingabe empfängt, die mit einem seiner geladenen und aktivierten <see cref="T:System.Speech.Recognition.Grammar" />-Objekte übereinstimmt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können einen Erkennungs Vorgang mit einer der Methoden <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> initiieren. Die Erkennung löst das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>-Ereignis aus, wenn ermittelt wird, dass die Eingabe einem der geladenen <xref:System.Speech.Recognition.Grammar>-Objekte mit einem ausreichenden Maß an Vertrauen entspricht, um die Erkennung zu gewährleisten. Die <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>-Eigenschaft der-<xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> die das akzeptierte <xref:System.Speech.Recognition.RecognitionResult>-Objekt enthält. Handler von <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignissen können den erkannten Ausdruck und eine Liste von Erkennungs <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> mit geringeren Vertrauens Bewertungen abrufen.  
  
 Wenn Ihre Anwendung eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz verwendet, können Sie den Vertrauensgrad ändern, mit dem Spracheingaben mit einer der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden akzeptiert oder abgelehnt werden.  Mithilfe der Eigenschaften <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> können Sie ändern, wie die Spracherkennung auf nicht Spracheingaben antwortet.  
  
 Wenn die Erkennung Eingaben empfängt, die mit einer Grammatik übereinstimmen, kann das <xref:System.Speech.Recognition.Grammar> Objekt das <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis erhöhen. Das <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis des <xref:System.Speech.Recognition.Grammar> Objekts wird vor dem <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis der Spracherkennung ausgelöst. Alle für eine bestimmte Grammatik spezifischen Aufgaben sollten immer von einem Handler für das <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis ausgeführt werden.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die sprach Erkennungs Grammatik erstellt, ein <xref:System.Speech.Recognition.Grammar> Objekt erstellt und es in die <xref:System.Speech.Recognition.SpeechRecognitionEngine> lädt, um die Erkennung auszuführen. Das Beispiel veranschaulicht die Spracheingabe für eine <xref:System.Speech.Recognition.SpeechRecognitionEngine>, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignisse, die von der Spracherkennung ausgelöst werden.  
  
 Gesprochene Eingaben, z. b. "Ich möchte von Chicago nach Miami übertragen werden", löst ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis aus. Mit dem Ausdruck "Fly me from Houston to Chicago" wird kein <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis auslöst.  
  
 Im Beispiel wird ein Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis verwendet, um erfolgreich erkannte Ausdrücke und die in der Konsole enthaltenen Semantik anzuzeigen.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Entlädt alle <see cref="T:System.Speech.Recognition.Grammar" />-Objekte aus des Erkennungsmodul.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung derzeit eine <xref:System.Speech.Recognition.Grammar> asynchron lädt, wartet diese Methode, bis der <xref:System.Speech.Recognition.Grammar> geladen ist, bevor alle <xref:System.Speech.Recognition.Grammar> Objekte aus der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz entladen werden.  
  
 Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>-Methode, um eine bestimmte Grammatik zu entladen.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die das synchrone laden und Entladen von sprach Erkennungs Grammatiken veranschaulicht.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadGrammar (grammar As Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Das zu entladene Grammatikobjekt.</param>
        <summary>Entlädt ein angegebenes <see cref="T:System.Speech.Recognition.Grammar" />-Objekt aus der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Instanz.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> verwenden, um die <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz anzuhalten, bevor Sie ein <xref:System.Speech.Recognition.Grammar> Objekt laden, entladen, aktivieren oder deaktivieren. Um alle <xref:System.Speech.Recognition.Grammar> Objekte zu entladen, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A>-Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die das synchrone laden und Entladen von sprach Erkennungs Grammatiken veranschaulicht.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> ist <see langword="null" /></exception>
        <exception cref="T:System.InvalidOperationException">Die Grammatik wird nicht in dieses Erkennungsmodul geladen, oder das Erkennungsmodul lädt gerade die Grammatik asynchron.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Aktualisiert den Wert einer Einstellung für das Erkennungsmodul.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennungseinstellungen können Zeichen folgen-, 64-Bit-ganzzahlige oder Speicher Adressdaten enthalten. In der folgenden Tabelle werden die Einstellungen beschrieben, die für eine SAPI-kompatible Erkennung (Microsoft Speech API) definiert sind. Die folgenden Einstellungen müssen für jede Erkennung, die die-Einstellung unterstützt, denselben Bereich aufweisen. Ein SAPI-kompatibles Erkennungs Modul ist nicht erforderlich, um diese Einstellungen zu unterstützen, und kann andere Einstellungen unterstützen.  
  
|Name|BESCHREIBUNG|  
|----------|-----------------|  
|`ResourceUsage`|Gibt die CPU-Auslastung des Erkennungs Moduls an. Der Bereich liegt zwischen 0 und 100. Der Standardwert lautet "50".|  
|`ResponseSpeed`|Gibt die Länge der Stille am Ende der eindeutigen Eingabe an, bevor die Spracherkennung einen Erkennungs Vorgang abschließt. Der Bereich liegt zwischen 0 und 10.000 Millisekunden (MS). Diese Einstellung entspricht der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>-Eigenschaft der Erkennung. Standardwert: 150 ms.|  
|`ComplexResponseSpeed`|Gibt die Länge der Stille in Millisekunden (MS) am Ende der mehrdeutigen Eingabe an, bevor die Spracherkennung einen Erkennungs Vorgang abschließt. Der Bereich liegt zwischen 0 und 10.000 ms. Diese Einstellung entspricht der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>-Eigenschaft der Erkennung. Standardwert = 500 ms.|  
|`AdaptationOn`|Gibt an, ob die Anpassung des Akustik Modells on (Value = `1`) oder Off (Value = `0`) erfolgt. Der Standardwert ist `1` (on).|  
|`PersistedBackgroundAdaptation`|Gibt an, ob die Hintergrund Anpassung on (Value = `1`) oder Off (Value = `0`) ist, und speichert die Einstellung in der Registrierung. Der Standardwert ist `1` (on).|  
  
 Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A>-Methode, um eine der Einstellungen der Erkennungsfunktion zurückzugeben.  
  
 Mit Ausnahme von `PersistedBackgroundAdaptation`bleiben Eigenschaftswerte, die mithilfe der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden festgelegt wurden, nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>in Kraft, nachdem Sie die Standardeinstellungen wieder hergestellt haben.  
  
 Mithilfe der Eigenschaften <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> können Sie ändern, wie die Spracherkennung auf nicht Spracheingaben antwortet.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der zu aktualisierenden Einstellung.</param>
        <param name="updatedValue">Der neue Wert für die Einstellung.</param>
        <summary>Aktualisiert die angegebene Einstellung für die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> mit dem angegebenen ganzzahligen Wert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Mit Ausnahme von `PersistedBackgroundAdaptation`bleiben Eigenschaftswerte, die mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>-Methode festgelegt wurden, nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>in Kraft, nachdem Sie die Standardeinstellungen wieder hergestellt haben. Unter <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> finden Sie Beschreibungen der unterstützten Einstellungen.  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die die Werte für eine Reihe von Einstellungen ausgibt, die für die Erkennung definiert sind, die das Gebiets Schema "en-US" unterstützt. Im Beispiel werden die Einstellungen für den Vertrauensgrad aktualisiert, und anschließend wird die Erkennung abgefragt, um die aktualisierten Werte zu überprüfen. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> ist <see langword="null" /></exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> ist die leere Zeichenfolge („“).</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Die Erkennung verfügt über keine Einstellung mit diesem Namen.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der zu aktualisierenden Einstellung.</param>
        <param name="updatedValue">Der neue Wert für die Einstellung.</param>
        <summary>Aktualisiert die angegebene Spracherkennungs-Engine-Einstellung mit dem angegebenen Zeichenfolgenwert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Mit Ausnahme von `PersistedBackgroundAdaptation`bleiben Eigenschaftswerte, die mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>-Methode festgelegt wurden, nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>in Kraft, nachdem Sie die Standardeinstellungen wieder hergestellt haben. Unter <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> finden Sie Beschreibungen der unterstützten Einstellungen.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> ist <see langword="null" /></exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> ist die leere Zeichenfolge („“).</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Die Erkennung verfügt über keine Einstellung mit diesem Namen.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>
