<Type Name="RecognizedAudio" FullName="System.Speech.Recognition.RecognizedAudio">
  <Metadata><Meta Name="ms.openlocfilehash" Value="15617af50fc8c8f4e68a84ea1756a2d6ba918251" /><Meta Name="ms.sourcegitcommit" Value="88014e1c5440e3df4f66ef04393854d15b1fd534" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="de-DE" /><Meta Name="ms.lasthandoff" Value="09/05/2019" /><Meta Name="ms.locfileid" Value="70560331" /></Metadata><TypeSignature Language="C#" Value="public class RecognizedAudio" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi serializable beforefieldinit RecognizedAudio extends System.Object" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.RecognizedAudio" />
  <TypeSignature Language="VB.NET" Value="Public Class RecognizedAudio" />
  <TypeSignature Language="C++ CLI" Value="public ref class RecognizedAudio" />
  <TypeSignature Language="F#" Value="type RecognizedAudio = class" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces />
  <Attributes>
    <Attribute>
      <AttributeName>System.Serializable</AttributeName>
    </Attribute>
  </Attributes>
  <Docs>
    <summary>Stellt eine Audioeingabe dar, die mit einem <see cref="T:System.Speech.Recognition.RecognitionResult" /> zugeordnet ist.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Eine Spracherkennung generiert im Rahmen des Erkennungs Vorgangs Informationen über die Audioeingabe. Verwenden Sie die <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> -Eigenschaft oder die <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> -Methode von <xref:System.Speech.Recognition.RecognitionResult>, um auf das erkannte Audiodaten zuzugreifen.  
  
 Ein Erkennungs Ergebnis kann durch die folgenden Ereignisse und Methoden der <xref:System.Speech.Recognition.SpeechRecognizer> -Klasse und der- <xref:System.Speech.Recognition.SpeechRecognitionEngine> Klasse erzeugt werden:  
  
-   Fall  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=nameWithType> und <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=nameWithType> und <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> und <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=nameWithType> und <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=nameWithType>  
  
-   Methoden:  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=nameWithType> und <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=nameWithType> und <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=nameWithType>  
  
> [!IMPORTANT]
>  Ein Erkennungs Ergebnis, das von der emulierten Spracherkennung erzeugt wurde, enthält keine erkannte Audiodaten. Für ein derartiges Erkennungs Ergebnis gibt <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> die- `null` Eigenschaft zurück <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> , und die-Methode löst eine Ausnahme aus. Weitere Informationen zu emulierten sprach Erkennungsmöglichkeiten finden Sie `EmulateRecognize` in `EmulateRecognizeAsync` <xref:System.Speech.Recognition.SpeechRecognizer> den-und- <xref:System.Speech.Recognition.SpeechRecognitionEngine> Methoden der-Klasse und der-Klasse.  
  
   
  
## Examples  
 Im folgenden Beispiel werden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType>- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType>,- <xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=nameWithType> oder-Ereignisse und-Ausgaben an die Konsolen Informationen über die erkannte Audiodatei verarbeitet, die dem Erkennungs Ergebnis zugeordnet ist.  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
    <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.RecognizedAudio.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die Position im Eingabeaudiostream für den Anfang des erkannten Audios ab.</summary>
        <value>Die Position im Eingabeaudiostream für den Anfang des erkannten Audios.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Eigenschaft verweist auf die Position am Anfang des erkannten Ausdrucks im generierten Audiostream des Eingabe Geräts. Im Gegensatz dazu verweist `RecognizerAudioPosition` die-Eigenschaft <xref:System.Speech.Recognition.SpeechRecognitionEngine> der <xref:System.Speech.Recognition.SpeechRecognizer> -Klasse und der-Klasse auf die Position der Erkennung innerhalb ihrer Audioeingabe. Diese Positionen können unterschiedlich sein. Weitere Informationen finden Sie unter [Verwenden von sprach Erkennungs Ereignissen](https://msdn.microsoft.com/library/01c598ca-2e0e-4e89-b303-cd1cef9e8482).  
  
 Die <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> -Eigenschaft ruft die Systemzeit ab, zu der der Erkennungs Vorgang gestartet wird.  
  
   
  
## Examples  
 Im folgenden Beispiel werden das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> - <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType> Ereignis oder das-Ereignis und Ausgaben an die Konsolen Informationen über die erkannte Audiodatei verarbeitet, die dem Erkennungs Ergebnis zugeordnet ist.  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.Duration" />
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.StartTime" />
      </Docs>
    </Member>
    <Member MemberName="Duration">
      <MemberSignature Language="C#" Value="public TimeSpan Duration { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan Duration" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.Duration" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Duration As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan Duration { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.Duration : TimeSpan" Usage="System.Speech.Recognition.RecognizedAudio.Duration" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die Dauer des Eingabeaudiostreams für das erkannte Audio ab.</summary>
        <value>Die Dauer innerhalb des Eingabeaudiostreams für das erkannte Audio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Im folgenden Beispiel werden das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> - <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType> Ereignis oder das-Ereignis und Ausgaben an die Konsolen Informationen über die erkannte Audiodatei verarbeitet, die dem Erkennungs Ergebnis zugeordnet ist.  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.StartTime" />
      </Docs>
    </Member>
    <Member MemberName="Format">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo Format { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo Format" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.Format" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Format As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ Format { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Format : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.RecognizedAudio.Format" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Audioformat ab, das von einer Erkennungs-Engine verarbeitet wird.</summary>
        <value>Das Format für das Audio-Element, dass von der Spracherkennung verarbeitet wird.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Im folgenden Beispiel werden das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> - <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType> Ereignis oder das-Ereignis und Ausgaben an die Konsolen Informationen über die erkannte Audiodatei verarbeitet, die dem Erkennungs Ergebnis zugeordnet ist.  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="N:System.Speech.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="GetRange">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizedAudio GetRange (TimeSpan audioPosition, TimeSpan duration);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognizedAudio GetRange(valuetype System.TimeSpan audioPosition, valuetype System.TimeSpan duration) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function GetRange (audioPosition As TimeSpan, duration As TimeSpan) As RecognizedAudio" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognizedAudio ^ GetRange(TimeSpan audioPosition, TimeSpan duration);" />
      <MemberSignature Language="F#" Value="member this.GetRange : TimeSpan * TimeSpan -&gt; System.Speech.Recognition.RecognizedAudio" Usage="recognizedAudio.GetRange (audioPosition, duration)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizedAudio</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioPosition" Type="System.TimeSpan" />
        <Parameter Name="duration" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="audioPosition">Der Anfangspunkt der Audiodaten, die zurückgegeben werden sollen.</param>
        <param name="duration">Die Länge des Segments, das zurückgegeben werden soll.</param>
        <summary>Wählt aus und gibt einen Abschnitt des aktuellen erkannten Audio als Binärdaten zurück.</summary>
        <returns>Gibt einen Unterabschnitt des erkannten Audio zurück, wie durch <paramref name="audioPosition" /> und <paramref name="duration" /> definiert.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Im folgenden Beispiel wird eine sprach Erkennungs Grammatik für namens Eingaben erstellt, ein Handler für das <xref:System.Speech.Recognition.Grammar.SpeechRecognized> -Ereignis hinzugefügt und die Grammatik in eine Prozess interne Spracherkennung geladen. Anschließend werden die Audioinformationen für den Namensteil der Eingabe in eine Audiodatei geschrieben. Die Audiodatei wird als Eingabe für ein <xref:System.Speech.Synthesis.SpeechSynthesizer> -Objekt verwendet, das einen Ausdruck mit der aufgezeichneten Audiodatei enthält.  
  
```  
private static void AddNameGrammar(SpeechRecognitionEngine recognizer)  
{  
  GrammarBuilder builder = new GrammarBuilder();  
  builder.Append("My name is");  
  builder.AppendWildcard();  
  
  Grammar nameGrammar = new Grammar(builder);  
  nameGrammar.Name = "Name Grammar";  
  nameGrammar.SpeechRecognized +=  
    new EventHandler<SpeechRecognizedEventArgs>(  
      NameSpeechRecognized);  
  
  recognizer.LoadGrammar(nameGrammar);  
}  
  
// Handle the SpeechRecognized event of the name grammar.  
private static void NameSpeechRecognized(  
  object sender, SpeechRecognizedEventArgs e)  
{  
  Console.WriteLine("Grammar ({0}) recognized speech: {1}",  
    e.Result.Grammar.Name, e.Result.Text);  
  
  try  
  {  
  
    // The name phrase starts after the first three words.  
    if (e.Result.Words.Count < 4)  
    {  
  
      // Add code to check for an alternate that contains the wildcard.  
      return;  
    }  
  
    RecognizedAudio audio = e.Result.Audio;  
    TimeSpan start = e.Result.Words[3].AudioPosition;  
    TimeSpan duration = audio.Duration - start;  
  
    // Add code to verify and persist the audio.  
    string path = @"C:\temp\nameAudio.wav";  
    using (Stream outputStream = new FileStream(path, FileMode.Create))  
    {  
      RecognizedAudio nameAudio = audio.GetRange(start, duration);  
      nameAudio.WriteToWaveStream(outputStream);  
      outputStream.Close();  
    }  
  
    Thread testThread =  
      new Thread(new ParameterizedThreadStart(TestAudio));  
    testThread.Start(path);  
  }  
  catch (Exception ex)  
  {  
    Console.WriteLine("Exception thrown while processing audio:");  
    Console.WriteLine(ex.ToString());  
  }  
}  
  
// Use the speech synthesizer to play back the .wav file  
// that was created in the SpeechRecognized event handler.  
  
private static void TestAudio(object item)  
{  
  string path = item as string;  
  if (path != null && File.Exists(path))  
  {  
    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  
    PromptBuilder builder = new PromptBuilder();  
    builder.AppendText("Hello");  
    builder.AppendAudio(path);  
    synthesizer.Speak(builder);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException"><paramref name="audioPosition" /> und <paramref name="duration" /> definieren ein Audio-Segment außerhalb des Bereichs des aktuellen Segments.</exception>
        <exception cref="T:System.InvalidOperationException">Das aktuelle erkannte Audiodatei enthält keine Daten.</exception>
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)" />
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="StartTime">
      <MemberSignature Language="C#" Value="public DateTime StartTime { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.DateTime StartTime" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.StartTime" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property StartTime As DateTime" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property DateTime StartTime { DateTime get(); };" />
      <MemberSignature Language="F#" Value="member this.StartTime : DateTime" Usage="System.Speech.Recognition.RecognizedAudio.StartTime" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.DateTime</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die Systemzeit zu Beginn des Erkennungsvorgangs ab.</summary>
        <value>Die Systemzeit am Anfang des Erkennungsvorgangs.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> -Eigenschaft ruft die Systemzeit zu Beginn des Erkennungs Vorgangs ab, der für Latenz-und Leistungsberechnungen nützlich sein kann.  
  
 Die <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A> -Eigenschaft ruft die Position im generierten Audiostream des Eingabe Geräts ab.  
  
   
  
## Examples  
 Im folgenden Beispiel werden das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> - <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType> Ereignis oder das-Ereignis und Ausgaben an die Konsolen Informationen über die erkannte Audiodatei verarbeitet, die dem Erkennungs Ergebnis zugeordnet ist.  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="WriteToAudioStream">
      <MemberSignature Language="C#" Value="public void WriteToAudioStream (System.IO.Stream outputStream);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void WriteToAudioStream(class System.IO.Stream outputStream) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub WriteToAudioStream (outputStream As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void WriteToAudioStream(System::IO::Stream ^ outputStream);" />
      <MemberSignature Language="F#" Value="member this.WriteToAudioStream : System.IO.Stream -&gt; unit" Usage="recognizedAudio.WriteToAudioStream outputStream" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="outputStream" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="outputStream">Der Stream, der die Audiodaten empfängt.</param>
        <summary>Schreibt das gesamte Audio als Rohdaten in einen Stream.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Audiodaten werden `outputStream` in Binär Form geschrieben. Es sind keine Header Informationen enthalten.  
  
 Die <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> -Methode verwendet das Wave-Format, enthält jedoch nicht den Wave-Header. Um den Wave-Header einzuschließen, <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> verwenden Sie die-Methode.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)" />
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="WriteToWaveStream">
      <MemberSignature Language="C#" Value="public void WriteToWaveStream (System.IO.Stream outputStream);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void WriteToWaveStream(class System.IO.Stream outputStream) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub WriteToWaveStream (outputStream As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void WriteToWaveStream(System::IO::Stream ^ outputStream);" />
      <MemberSignature Language="F#" Value="member this.WriteToWaveStream : System.IO.Stream -&gt; unit" Usage="recognizedAudio.WriteToWaveStream outputStream" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="outputStream" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="outputStream">Der Stream, der die Audiodaten empfängt.</param>
        <summary>Schreibt Audio in einen Stream im Waveformat.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Audiodaten werden `outputStream` in das Wave-Format geschrieben, das einen Riff-Header (Resource Interchange File Format) enthält.  
  
 Die <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> -Methode verwendet das gleiche Binärformat, enthält jedoch nicht den Wave-Header.  
  
   
  
## Examples  
 Im folgenden Beispiel wird eine sprach Erkennungs Grammatik für namens Eingaben erstellt, ein Handler für das <xref:System.Speech.Recognition.Grammar.SpeechRecognized> -Ereignis hinzugefügt und die Grammatik in eine Prozess interne Spracherkennung geladen. Anschließend werden die Audioinformationen für den Namensteil der Eingabe in eine Audiodatei geschrieben. Die Audiodatei wird als Eingabe für ein <xref:System.Speech.Synthesis.SpeechSynthesizer> -Objekt verwendet, das einen Ausdruck mit der aufgezeichneten Audiodatei enthält.  
  
```  
private static void AddNameGrammar(SpeechRecognitionEngine recognizer)  
{  
  GrammarBuilder builder = new GrammarBuilder();  
  builder.Append("My name is");  
  builder.AppendWildcard();  
  
  Grammar nameGrammar = new Grammar(builder);  
  nameGrammar.Name = "Name Grammar";  
  nameGrammar.SpeechRecognized +=  
    new EventHandler<SpeechRecognizedEventArgs>(  
      NameSpeechRecognized);  
  
  recognizer.LoadGrammar(nameGrammar);  
}  
  
// Handle the SpeechRecognized event of the name grammar.  
private static void NameSpeechRecognized(  
  object sender, SpeechRecognizedEventArgs e)  
{  
  Console.WriteLine("Grammar ({0}) recognized speech: {1}",  
    e.Result.Grammar.Name, e.Result.Text);  
  
  try  
  {  
    // The name phrase starts after the first three words.  
    if (e.Result.Words.Count < 4)  
    {  
  
      // Add code to check for an alternate that contains the   
wildcard.  
      return;  
    }  
  
    RecognizedAudio audio = e.Result.Audio;  
    TimeSpan start = e.Result.Words[3].AudioPosition;  
    TimeSpan duration = audio.Duration - start;  
  
    // Add code to verify and persist the audio.  
    string path = @"C:\temp\nameAudio.wav";  
    using (Stream outputStream = new FileStream(path, FileMode.Create))  
    {  
      RecognizedAudio nameAudio = audio.GetRange(start, duration);  
      nameAudio.WriteToWaveStream(outputStream);  
      outputStream.Close();  
    }  
  
    Thread testThread =  
      new Thread(new ParameterizedThreadStart(TestAudio));  
    testThread.Start(path);  
  }  
  catch (Exception ex)  
  {  
    Console.WriteLine("Exception thrown while processing audio:");  
    Console.WriteLine(ex.ToString());  
  }  
}  
  
// Use the speech synthesizer to play back the .wav file  
// that was created in the SpeechRecognized event handler.  
  
private static void TestAudio(object item)  
{  
  string path = item as string;  
  if (path != null && File.Exists(path))  
  {  
    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  
    PromptBuilder builder = new PromptBuilder();  
    builder.AppendText("Hello");  
    builder.AppendAudio(path);  
    synthesizer.Speak(builder);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)" />
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)" />
      </Docs>
    </Member>
  </Members>
</Type>
