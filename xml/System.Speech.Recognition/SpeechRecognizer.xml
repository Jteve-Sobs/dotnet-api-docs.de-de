<Type Name="SpeechRecognizer" FullName="System.Speech.Recognition.SpeechRecognizer">
  <Metadata><Meta Name="ms.openlocfilehash" Value="da0051c072e25c0156ffd31379feba54dfc8cae7" /><Meta Name="ms.sourcegitcommit" Value="397961a0164281b579f68064c3bb66c071f374d9" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="de-DE" /><Meta Name="ms.lasthandoff" Value="07/14/2020" /><Meta Name="ms.locfileid" Value="78786613" /></Metadata><TypeSignature Language="C#" Value="public class SpeechRecognizer : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognizer extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognizer" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognizer&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognizer : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognizer = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Bietet Zugriff auf den freigegebenen Spracherkennungsdienst, der auf dem Windows-Desktop verfügbar ist.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Anwendungen verwenden die freigegebene Erkennung, um auf die Windows-Spracherkennung zuzugreifen. Verwenden Sie das- <xref:System.Speech.Recognition.SpeechRecognizer> Objekt, um die Benutzer Darstellung von Windows Speech hinzuzufügen.  
  
 Diese Klasse bietet Kontrolle über verschiedene Aspekte des sprach Erkennungsprozesses:  
  
-   Verwenden Sie zum Verwalten von sprach Erkennungs Grammatiken <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> , <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> , <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A> , <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> und <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A> .  
  
-   Um Informationen zu aktuellen sprach Erkennungs Vorgängen zu erhalten, abonnieren Sie die <xref:System.Speech.Recognition.SpeechRecognizer> -,-,- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> und-Ereignisse von <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> .  
  
-   Um die Anzahl der alternativen Ergebnisse anzuzeigen oder zu ändern, die die Erkennung zurückgibt, verwenden Sie die- <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> Eigenschaft. Die Erkennung gibt Erkennungsergebnisse in einem- <xref:System.Speech.Recognition.RecognitionResult> Objekt zurück.  
  
-   Verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> Eigenschaften,, <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> ,, <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> ,, und <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> und die <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated> <xref:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred> <xref:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged> <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> -,-,-,-und-Ereignisse, um auf den Status der freigegebenen Erkennung zuzugreifen oder diese zu überwachen.  
  
-   Verwenden Sie die-Methode, um Änderungen an der Erkennung zu synchronisieren <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> . Die freigegebene Erkennung verwendet mehr als einen Thread, um Aufgaben auszuführen.  
  
-   Um die Eingabe für die freigegebene Erkennung zu emulieren, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> Methoden und.  
  
 Die Konfiguration der Windows-Spracherkennung wird durch die Verwendung des Dialog Felds **Spracheigenschaften** in der **Systemsteuerung**verwaltet. Diese Schnittstelle wird verwendet, um die standardmäßige Desktop sprach Erkennungs-Engine und-Sprache, das Audioeingabegerät und das standbyverhalten der Spracherkennung auszuwählen. Wenn die Konfiguration der Windows-Spracherkennung geändert wird, während die Anwendung ausgeführt wird (beispielsweise, wenn die Spracherkennung deaktiviert ist oder die Eingabe Sprache geändert wird), wirkt sich die Änderung auf alle <xref:System.Speech.Recognition.SpeechRecognizer> Objekte aus.  
  
 Verwenden Sie die-Klasse, um eine Prozess interne Spracherkennung zu erstellen, die von der Windows-Spracherkennung unabhängig ist <xref:System.Speech.Recognition.SpeechRecognitionEngine> .  
  
> [!NOTE]
>  Wird immer aufgerufen <xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A> , bevor Sie den letzten Verweis auf die Spracherkennung freigeben. Andernfalls werden die verwendeten Ressourcen nicht freigegeben, bis die Garbage Collector die-Methode des Erkennungs Moduls aufruft `Finalize` .  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die eine sprach Erkennungs Grammatik lädt und die asynchrone emulierten Eingaben, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignisse veranschaulicht, die von der Spracherkennung ausgelöst werden.  Wenn die Windows-Spracherkennung nicht ausgeführt wird, wird durch das Starten dieser Anwendung auch die Windows-Spracherkennung gestartet. Wenn **die Windows** -Spracherkennung im Ruhezustand ist, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> gibt immer NULL zurück.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.   
        // This matches the grammar and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.  
        // This does not match the grammar or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the SpeechRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
  </Docs>
  <Members>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognizer ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognizer();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Initialisiert eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognizer" />-Klasse.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jedes- <xref:System.Speech.Recognition.SpeechRecognizer> Objekt verwaltet einen separaten Satz von sprach Erkennungs Grammatiken.  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die eine sprach Erkennungs Grammatik lädt und die asynchrone emulierten Eingaben, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignisse veranschaulicht, die von der Spracherkennung ausgelöst werden. Wenn die Windows-Spracherkennung nicht ausgeführt wird, wird durch das Starten dieser Anwendung auch die Windows-Spracherkennung gestartet. Wenn **die Windows** -Spracherkennung im Ruhezustand ist, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> gibt immer NULL zurück.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.   
        // This matches the grammar and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.  
        // This does not match the grammar or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the SpeechRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Audioformat ab, das von der Spracherkennung empfangen wird.</summary>
        <value>Das Audioeingabeformat für die Spracherkennung oder die <see langword="null" />, wenn die Eingabe für die Erkennung nicht konfiguriert ist.</value>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den Level des Audiosignals ab, das von der Spracherkennung empfangen wird.</summary>
        <value>Der Audiopegel der Eingabe an die Spracherkennung, von 0 bis 100.</value>
        <remarks>To be added.</remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt auf, wenn die freigegebene Erkennung die Ebene ihrer Audioeingabe meldet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennung löst dieses Ereignis mehrmals pro Sekunde aus. Die Häufigkeit, mit der das Ereignis ausgelöst wird, hängt von dem Computer ab, auf dem die Anwendung ausgeführt wird.  
  
 Um die Audioebene zum Zeitpunkt des Ereignisses zu erhalten, verwenden Sie die- <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs> . Um die aktuelle Audioebene der Eingabe für die Erkennung zu erhalten, verwenden Sie die-Eigenschaft des Erkennungs Moduls <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> .  
  
 Wenn Sie einen Delegaten für ein `AudioLevelUpdated` Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel wird einem-Objekt ein Handler für das-Ereignis hinzugefügt `AudioLevelUpdated` <xref:System.Speech.Recognition.SpeechRecognizer> . Der Handler gibt die neue Audioebene an der Konsole aus.  
  
```csharp  
private SpeechRecognizer recognizer;  
  
// Initialize the SpeechRecognizer object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognizer();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
    new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die aktuelle Position im Audiostream ab, die durch das Gerät generiert wird, das die Spracherkennung mit Eingaben versorgt.</summary>
        <value>Die aktuelle Position im Audioeingabestream der Spracherkennung, durch den die Eingabe empfangen wurde.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die freigegebene Erkennung empfängt Eingaben, während die Spracherkennung der Desktop Ausführung ausgeführt wird.  
  
 Die `AudioPosition` -Eigenschaft verweist auf die Position des Eingabe Geräts im generierten Audiostream. Im Gegensatz dazu <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> verweist die-Eigenschaft auf die Position des Erkennungs Moduls bei der Verarbeitung von Audioeingaben. Diese Positionen können unterschiedlich sein.  Wenn die Erkennung z. b. Eingaben erhalten hat, für die Sie noch kein Erkennungs Ergebnis generiert hat, ist der Wert der <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> Eigenschaft kleiner als der Wert der <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> Eigenschaft.  
  
   
  
## Examples  
 Im folgenden Beispiel verwendet die freigegebene Spracherkennung eine Diktat Grammatik, um Spracheingaben abzugleichen. Ein Handler für das <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> Ereignis schreibt in die Konsole <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> , <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> , und, <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> Wenn die Spracherkennung bei der Eingabe eine Sprache erkennt.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add handlers for events.  
      recognizer.LoadGrammarCompleted +=   
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
      recognizer.SpeechRecognized +=   
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.StateChanged +=   
        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
      recognizer.SpeechDetected +=   
        new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load the grammar object to the recognizer.  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Recognizer audio position: " + recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Write the name of the loaded grammar to the console.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt auf, wenn in der Erkennung ein Problem beim Audiosignal auftritt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Um das Problem zu beheben, verwenden Sie die- <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs> .  
  
 Wenn Sie einen Delegaten für ein `AudioSignalProblemOccurred` Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel wird ein Ereignishandler definiert, der Informationen zu einem `AudioSignalProblemOccurred` Ereignis sammelt.  
  
```  
private SpeechRecognizer recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognizer();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognizer.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den Status des von der Spracherkennung empfangenen Audiosignals ab.</summary>
        <value>Der Zustand der Audioeingabe für die Spracherkennung.</value>
        <remarks>To be added.</remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt bei Zustandsänderungen im Audio auf, das von der Erkennung empfangen wird.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Um den audiozustand zum Zeitpunkt des Ereignisses zu erhalten, verwenden Sie die- <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioStateChangedEventArgs> . Um den aktuellen audiozustand der Eingabe für die Erkennung zu erhalten, verwenden Sie die-Eigenschaft des Erkennungs Moduls <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> . Weitere Informationen zum Audiostatus finden Sie unter der- <xref:System.Speech.Recognition.AudioState> Enumeration.  
  
 Wenn Sie einen Delegaten für ein `AudioStateChanged` Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel wird ein Handler für das `AudioStateChanged` -Ereignis verwendet, um <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> jedes Mal, wenn er sich mit einem Member der-Enumeration ändert, das neue Erkennungs Modul in die Konsole zu schreiben <xref:System.Speech.Recognition.AudioState> .  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.StateChanged +=  
          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Put the recognizer into Listening mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        Console.WriteLine();  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Gibt das <see cref="T:System.Speech.Recognition.SpeechRecognizer" />-Objekt frei.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognizer.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Gibt das <see cref="T:System.Speech.Recognition.SpeechRecognizer" />-Objekt frei.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognizer.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing"><see langword="true" />, um sowohl verwaltete als auch nicht verwaltete Ressourcen freizugeben, <see langword="false" />, um ausschließlich nicht verwaltete Ressourcen freizugeben.</param>
        <summary>Verwirft das <see cref="T:System.Speech.Recognition.SpeechRecognizer" />-Objekt und gibt Ressourcen frei, die während der Sitzung verwendet werden.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuliert die Eingabe für die freigegebene Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden umgehen die systemaudioeingabe. Dies kann hilfreich sein, wenn Sie eine Anwendung oder eine Grammatik testen oder Debuggen.  
  
> [!NOTE]
>  Wenn sich die Windows-Spracherkennung **im Ruhe** Zustand befindet, geben diese Methoden zurück `null` .  
  
 Die freigegebene Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> Ereignisse,, und aus, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> als ob der Erkennungs Vorgang nicht emuliert wird. Die Erkennung ignoriert neue Zeilen und zusätzliche Leerzeichen und behandelt Interpunktions Zeichen als literaleingaben.  
  
> [!NOTE]
>  Das <xref:System.Speech.Recognition.RecognitionResult> Objekt, das von der freigegebenen Erkennung als Antwort auf die emulierten Eingaben generiert wird, hat den Wert `null` für seine- <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> Eigenschaft.  
  
 Um die asynchrone Erkennung zu emulieren, verwenden Sie die- <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe für den Erkennungsvorgang.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die freigegebene Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet.</summary>
        <returns>Das Erkennungsergebnis des Erkennungsvorgangs oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder wenn sich die Windows-Spracherkennung im **Sleeping**-Zustand befindet.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren groß-und Kleinschreibung und Zeichenbreite beim Anwenden von Grammatikregeln auf den Eingabe Ausdruck. Weitere Informationen zu diesem Vergleichstyp finden Sie unter den <xref:System.Globalization.CompareOptions> Enumerationswerten <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> und <xref:System.Globalization.CompareOptions.IgnoreWidth> . Die Erkennungs Tools ignorieren auch neue Zeilen und zusätzliche Leerzeichen und behandeln Interpunktions Zeichen als literaleingaben.  
  
   
  
## Examples  
 Im folgenden Beispiel wird eine Beispiel Grammatik in die freigegebene Erkennung geladen und die Eingabe für die Erkennung emuliert. Wenn die Windows-Spracherkennung nicht ausgeführt wird, wird durch das Starten dieser Anwendung auch die Windows-Spracherkennung gestartet. Wenn **die Windows** -Spracherkennung im Ruhezustand ist, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> gibt immer NULL zurück.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        RecognitionResult result;  
  
        // This EmulateRecognize call matches the grammar and returns a  
        // recognition result.  
        result = recognizer.EmulateRecognize("testing testing");  
        OutputResult(result);  
  
        // This EmulateRecognize call does not match the grammar and   
        // returns null.  
        result = recognizer.EmulateRecognize("testing one two three");  
        OutputResult(result);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Output information about a recognition result to the console.  
    private static void OutputResult(RecognitionResult result)  
    {  
      if (result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (wordUnits As RecognizedWordUnit(), compareOptions As CompareOptions) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Ein Array von Worteinheiten, das die Eingabe für den Erkennungsvorgang enthält.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe bestimmter Wörter für die freigegebene Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen den Wörtern und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <returns>Das Erkennungsergebnis des Erkennungsvorgangs oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder wenn sich die Windows-Spracherkennung im **Sleeping**-Zustand befindet.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode erstellt ein- <xref:System.Speech.Recognition.RecognitionResult> Objekt anhand der Informationen, die im-Parameter bereitgestellt werden `wordUnits` .  
  
 Die Erkennung verwendet den `compareOptions` , wenn Sie Grammatikregeln auf den Eingabe Ausdruck anwendet. Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren Case, wenn der- <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Wert oder der- <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die Erkennungs Tools ignorieren immer die Zeichenbreite und ignorieren den Kana-Typ nie. Die erkennern ignorieren auch neue Zeilen und zusätzliche Leerzeichen und behandeln Interpunktions Zeichen als literaleingaben. Weitere Informationen zu Zeichenbreite und Kana-Typ finden Sie unter der- <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String, compareOptions As CompareOptions) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Der Eingabebegriff für den Erkennungsvorgang.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die freigegebene Spracherkennung. Dabei wird Text statt Audio für die synchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen dem Ausdruck und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <returns>Das Erkennungsergebnis des Erkennungsvorgangs oder <see langword="null" />, wenn der Vorgang nicht erfolgreich war oder wenn sich die Windows-Spracherkennung im **Sleeping**-Zustand befindet.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennung verwendet den `compareOptions` , wenn Sie Grammatikregeln auf den Eingabe Ausdruck anwendet. Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren Case, wenn der- <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Wert oder der- <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die Erkennungs Tools ignorieren immer die Zeichenbreite und ignorieren den Kana-Typ nie. Die erkennern ignorieren auch neue Zeilen und zusätzliche Leerzeichen und behandeln Interpunktions Zeichen als literaleingaben. Weitere Informationen zu Zeichenbreite und Kana-Typ finden Sie unter der- <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuliert die Eingabe für die freigegebene Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden umgehen die systemaudioeingabe. Dies kann hilfreich sein, wenn Sie eine Anwendung oder eine Grammatik testen oder Debuggen.  
  
 Die freigegebene Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> Ereignisse,, und aus, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> als ob der Erkennungs Vorgang nicht emuliert wird. Wenn die Erkennung den asynchronen Erkennungs Vorgang abschließt, wird das- <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> Ereignis ausgelöst. Die Erkennung ignoriert neue Zeilen und zusätzliche Leerzeichen und behandelt Interpunktions Zeichen als literaleingaben.  
  
> [!NOTE]
>  Wenn sich die Windows-Spracherkennung im **Ruhezustand befindet** , verarbeitet die freigegebene Erkennung keine Eingaben und löst nicht die <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> -und-bezogenen Ereignisse aus, löst aber trotzdem das-Ereignis aus <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> .  
  
> [!NOTE]
>  Das <xref:System.Speech.Recognition.RecognitionResult> Objekt, das von der freigegebenen Erkennung als Antwort auf die emulierten Eingaben generiert wird, hat den Wert `null` für seine- <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> Eigenschaft.  
  
 Verwenden Sie zum Emulieren der synchronen Erkennung die- <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe für den Erkennungsvorgang.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die freigegebene Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren groß-und Kleinschreibung und Zeichenbreite beim Anwenden von Grammatikregeln auf den Eingabe Ausdruck. Weitere Informationen zu diesem Vergleichstyp finden Sie unter den <xref:System.Globalization.CompareOptions> Enumerationswerten <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> und <xref:System.Globalization.CompareOptions.IgnoreWidth> . Die Erkennungs Tools ignorieren auch neue Zeilen und zusätzliche Leerzeichen und behandeln Interpunktions Zeichen als literaleingaben.  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die eine sprach Erkennungs Grammatik lädt und die asynchrone emulierten Eingaben, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignisse veranschaulicht, die von der Spracherkennung ausgelöst werden. Wenn die Windows-Spracherkennung nicht ausgeführt wird, wird durch das Starten dieser Anwendung auch die Windows-Spracherkennung gestartet. Wenn **die Windows** -Spracherkennung im Ruhezustand ist, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> gibt immer NULL zurück.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar   
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.   
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (wordUnits As RecognizedWordUnit(), compareOptions As CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Ein Array von Worteinheiten, das die Eingabe für den Erkennungsvorgang enthält.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe bestimmter Wörter für die freigegebene Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen den Wörtern und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode erstellt ein- <xref:System.Speech.Recognition.RecognitionResult> Objekt anhand der Informationen, die im-Parameter bereitgestellt werden `wordUnits` .  
  
 Die Erkennung verwendet den `compareOptions` , wenn Sie Grammatikregeln auf den Eingabe Ausdruck anwendet. Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren Case, wenn der- <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Wert oder der- <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die Erkennungs Tools ignorieren immer die Zeichenbreite und ignorieren den Kana-Typ nie. Die erkennern ignorieren auch neue Zeilen und zusätzliche Leerzeichen und behandeln Interpunktions Zeichen als literaleingaben. Weitere Informationen zu Zeichenbreite und Kana-Typ finden Sie unter der- <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String, compareOptions As CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Der Eingabebegriff für den Erkennungsvorgang.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die den Typ des Vergleichs beschreiben, der für den emulierten Erkennungsvorgang verwendet wird.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die freigegebene Spracherkennung. Dabei wird Text statt Audio für die asynchrone Spracherkennung verwendet und festgelegt, wie die Erkennung Unicode-Vergleich zwischen dem Ausdruck und den geladenen Spracherkennungsgrammatiken behandelt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennung verwendet den `compareOptions` , wenn Sie Grammatikregeln auf den Eingabe Ausdruck anwendet. Die Erkennungs Tools, die mit Vista und Windows 7 ausgeliefert werden, ignorieren Case, wenn der- <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> Wert oder der- <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Die Erkennungs Tools ignorieren immer die Zeichenbreite und ignorieren den Kana-Typ nie. Die erkennern ignorieren auch neue Zeilen und zusätzliche Leerzeichen und behandeln Interpunktions Zeichen als literaleingaben. Weitere Informationen zu Zeichenbreite und Kana-Typ finden Sie unter der- <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt auf, wenn die freigegebene Erkennung einen asynchronen Erkennungsvorgang für emulierte Eingabe abgeschlossen hat.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> Methode startet einen asynchronen Erkennungs Vorgang. Die Erkennung löst das- `EmulateRecognizeCompleted` Ereignis aus, wenn der asynchrone Vorgang abgeschlossen wird.  
  
 Der asynchrone Erkennungs Vorgang kann die <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> Ereignisse, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> , <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> und aufrichten <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> . Das <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> Ereignis ist das letzte Ereignis, das die Erkennung für einen bestimmten Vorgang auslöst.  
  
 Wenn Sie einen Delegaten für ein `EmulateRecognizeCompleted` Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die eine sprach Erkennungs Grammatik lädt und die asynchrone emulierten Eingaben, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignisse veranschaulicht, die von der Spracherkennung ausgelöst werden. Wenn die Windows-Spracherkennung nicht ausgeführt wird, wird durch das Starten dieser Anwendung auch die Windows-Spracherkennung gestartet. Wenn **die Windows** -Spracherkennung im inaktiven Modus ist, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> gibt immer NULL zurück.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=   
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="Enabled">
      <MemberSignature Language="C#" Value="public bool Enabled { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance bool Enabled" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
      <MemberSignature Language="VB.NET" Value="Public Property Enabled As Boolean" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property bool Enabled { bool get(); void set(bool value); };" />
      <MemberSignature Language="F#" Value="member this.Enabled : bool with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.Enabled" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Boolean</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft einen Wert ab bzw. legt einen Wert fest, der angibt, ob dieses <see cref="T:System.Speech.Recognition.SpeechRecognizer" />-Objekt für die Sprachverarbeitung bereit ist.</summary>
        <value><see langword="true" />, wenn dieses <see cref="T:System.Speech.Recognition.SpeechRecognizer" />-Objekt Spracherkennung ausführt; andernfalls <see langword="false" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Änderungen an dieser Eigenschaft wirken sich nicht auf andere Instanzen der- <xref:System.Speech.Recognition.SpeechRecognizer> Klasse aus.  
  
 Standardmäßig ist der Wert der- <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> Eigenschaft `true` für eine neu instanziierte Instanz von <xref:System.Speech.Recognition.SpeechRecognizer> . Obwohl die Erkennung deaktiviert ist, sind keine der sprach Erkennungs Grammatiken für Erkennungs Vorgänge verfügbar. Das Festlegen der-Eigenschaft des Erkennungs Moduls <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> hat keine Auswirkung auf die-Eigenschaft des Erkennungs Moduls <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> .  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognizer.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft eine Auflistung der <see cref="T:System.Speech.Recognition.Grammar" />-Objekte ab, die in diese <see cref="T:System.Speech.Recognition.SpeechRecognizer" />-Instanz geladen werden.</summary>
        <value>Eine Auflistung der <see cref="T:System.Speech.Recognition.Grammar" />-Objekte, die die Anwendung in die aktuelle Instanz der freigegebenen Erkennung geladen hat.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Eigenschaft gibt keine sprach Erkennungs Grammatiken zurück, die von einer anderen Anwendung geladen wurden.  
  
   
  
## Examples  
 Im folgenden Beispiel werden Informationen für jede sprach Erkennungs Grammatik, die in die freigegebene Spracherkennung geladen wurde, an die Konsole ausgegeben.  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        Grammar sampleGrammar = new Grammar(new GrammarBuilder("sample phrase"));  
        sampleGrammar.Name = "Sample Grammar";  
        recognizer.LoadGrammar(sampleGrammar);  
  
        OutputGrammarList(recognizer);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void OutputGrammarList(SpeechRecognizer recognizer)  
    {  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      if (grammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in grammars)  
        {  
          Console.WriteLine("  Grammar: {0}",  
            (g.Name != null) ? g.Name : "<no name>");  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
    }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="VB.NET" Value="Public Sub LoadGrammar (grammar As Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Die zu ladende Spracherkennungsgrammatik.</param>
        <summary>Lädt eine Spracherkennungsgrammatik.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die freigegebene Erkennung löst eine Ausnahme aus, wenn die Spracherkennungs-Grammatik bereits geladen wurde, asynchron geladen wird oder nicht in eine Erkennung geladen werden konnte. Wenn die Erkennung ausgeführt wird, müssen Anwendungen zum Anhalten <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> der sprach Erkennungs-Engine verwenden, bevor Sie eine Grammatik laden, entladen, aktivieren oder deaktivieren.  
  
 Verwenden Sie die-Methode, um eine sprach Erkennungs Grammatik asynchron zu laden <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> .  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die eine sprach Erkennungs Grammatik lädt und die asynchrone emulierten Eingaben, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignisse veranschaulicht, die von der Spracherkennung ausgelöst werden. Wenn die Windows-Spracherkennung nicht ausgeführt wird, wird durch das Starten dieser Anwendung auch die Windows-Spracherkennung gestartet. Wenn **die Windows** -Spracherkennung im Ruhezustand ist, <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> gibt immer NULL zurück.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar   
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }   
  
    // Handle the EmulateRecognizeCompleted event.   
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="VB.NET" Value="Public Sub LoadGrammarAsync (grammar As Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Die zu ladende Spracherkennungsgrammatik.</param>
        <summary>Lädt asynchron eine Spracherkennungsgrammatik.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung diesen asynchronen Vorgang abschließt, löst Sie ein- <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> Ereignis aus. Die Erkennung löst eine Ausnahme aus, wenn die sprach Erkennungs Grammatik bereits geladen wurde, asynchron geladen wird oder nicht in eine Erkennung geladen werden konnte. Wenn die Erkennung ausgeführt wird, müssen Anwendungen zum Anhalten <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> der sprach Erkennungs-Engine verwenden, bevor Sie eine Grammatik laden, entladen, aktivieren oder deaktivieren.  
  
 Verwenden Sie die-Methode, um eine sprach Erkennungs Grammatik synchron zu laden <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> .  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt auf, wenn die Erkennung das asynchrone Laden einer Spracherkennungsgrammatik beendet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die-Methode der Erkennungs <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> Methode initiiert einen asynchronen Vorgang. Die Erkennung löst das- `LoadGrammarCompleted` Ereignis aus, wenn der Vorgang abgeschlossen ist. Um das Objekt zu erhalten, das <xref:System.Speech.Recognition.Grammar> von der Erkennung geladen wurde, verwenden Sie die- <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs> . Um die aktuellen Objekte zu erhalten <xref:System.Speech.Recognition.Grammar> , die die Erkennung geladen hat, verwenden Sie die-Eigenschaft des Erkennungs Moduls <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A> .  
  
 Wenn Sie einen Delegaten für ein- `LoadGrammarCompleted` Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel wird eine freigegebene Spracherkennung erstellt, und anschließend werden zwei Arten von Grammatiken zum Erkennen bestimmter Wörter und zum Akzeptieren der kostenlosen Diktat Erstellung erstellt. Im Beispiel werden alle erstellten Grammatiken asynchron in die Erkennung geladen. Handler für die-und-Ereignisse des Erkennungs Moduls <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> Schreiben in die Konsole den Namen der Grammatik, die zum Durchführen der Erkennung verwendet wurde, bzw. den Text des Erkennungs Ergebnisses.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Add a handler for the StateChanged event.  
        recognizer.StateChanged +=  
          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
        // Create "yesno" grammar.  
        Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah}" });  
        SemanticResultValue yesValue =  
            new SemanticResultValue(yesChoices, (bool)true);  
        Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
        SemanticResultValue noValue =  
            new SemanticResultValue(noChoices, (bool)false);  
        SemanticResultKey yesNoKey =  
            new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
        Grammar yesnoGrammar = new Grammar(yesNoKey);  
        yesnoGrammar.Name = "yesNo";  
  
        // Create "done" grammar.  
        Grammar doneGrammar =  
          new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
        doneGrammar.Name = "Done";  
  
        // Create dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation";  
  
        // Load grammars to the recognizer.  
        recognizer.LoadGrammarAsync(yesnoGrammar);  
        recognizer.LoadGrammarAsync(doneGrammar);  
        recognizer.LoadGrammarAsync(dictation);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Put the shared speech recognizer into "listening" mode.   
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die Höchstzahl alternativer Erkennungsergebnisse ab, welche die gemeinsame Erkennung für jeden Erkennungsvorgang zurückgibt, oder legt diese fest.</summary>
        <value>Die maximale Anzahl alternativer Ergebnisse, die die Spracherkennung für jeden Erkennungsvorgang zurückgibt.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die- <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> Eigenschaft der- <xref:System.Speech.Recognition.RecognitionResult> Klasse enthält die Auflistung von- <xref:System.Speech.Recognition.RecognizedPhrase> Objekten, die andere Kandidaten Interpretationen der Eingabe darstellen.  
  
 Der Standardwert für <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> ist 10.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognitionResult.Alternates" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="PauseRecognizerOnRecognition">
      <MemberSignature Language="C#" Value="public bool PauseRecognizerOnRecognition { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance bool PauseRecognizerOnRecognition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      <MemberSignature Language="VB.NET" Value="Public Property PauseRecognizerOnRecognition As Boolean" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property bool PauseRecognizerOnRecognition { bool get(); void set(bool value); };" />
      <MemberSignature Language="F#" Value="member this.PauseRecognizerOnRecognition : bool with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Boolean</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft einen Wert ab, bzw. legt diesen fest, der angibt, ob das gemeinsame Erkennungsmodul Erkennungsvorgänge anhält, während eine Anwendung ein <see cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />-Ereignis verarbeitet.</summary>
        <value><see langword="true" />, wenn die freigegebene Erkennungsmodul wartet, um Eingaben zu verarbeiten, während eine beliebige Anwendung das Ereignis <see cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" /> behandelt; andernfalls <see langword="false" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Legen Sie diese Eigenschaft auf fest `true` , wenn die Anwendung innerhalb des <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis Handlers den Status des sprach Erkennungs Dienstanbieter ändern oder die geladenen oder aktivierten sprach Erkennungs Grammatiken ändern muss, bevor der sprach Erkennungsdienst mehr Eingaben verarbeitet.  
  
> [!NOTE]
>  Das Festlegen der- <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Eigenschaft auf bewirkt, dass `true` jeder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignishandler in jeder Anwendung den Windows-sprach Erkennungsdienst blockiert.  
  
 Verwenden Sie die-Methode, um die Änderungen mit dem freigegebenen Erkennungs Modul mit dem Anwendungs Status zu synchronisieren <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> .  
  
 Wenn den Wert <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> `true` hat, hält <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> der sprach Erkennungsdienst während der Ausführung des Handlers eine Pause an und puffert neue Audioeingaben, sobald sie eintreffen. Nachdem der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignishandler beendet wurde, setzt der sprach Erkennungsdienst die Erkennung fort und beginnt mit der Verarbeitung von Informationen aus dem Eingabepuffer.  
  
 Verwenden Sie die-Eigenschaft, um den sprach Erkennungsdienst zu aktivieren oder zu deaktivieren <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> .  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die aktuelle Position der Erkennung in der Audioeingabe ab, die verarbeitet wird.</summary>
        <value>Die Position der Erkennung in der Audioeingabe, die sie verarbeitet.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die `RecognizerAudioPosition` -Eigenschaft verweist auf die Position der Erkennung bei der Verarbeitung der Audioeingabe. Im Gegensatz dazu <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> verweist die-Eigenschaft auf die Position des Eingabe Geräts im generierten Audiostream. Diese Positionen können unterschiedlich sein. Wenn die Erkennung z. b. Eingaben erhalten hat, für die Sie noch kein Erkennungs Ergebnis generiert hat, ist der Wert der <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> Eigenschaft kleiner als der Wert der <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> Eigenschaft.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognizer.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft Informationen über die freigegebene Spracherkennung ab.</summary>
        <value>Informationen über die freigegebene Spracherkennung.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Eigenschaft gibt Informationen über die Spracherkennung zurück, die von der Windows-Spracherkennung verwendet wird.  
  
   
  
## Examples  
 Im folgenden Beispiel werden Informationen über die freigegebene Erkennung an die Konsole gesendet.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        Console.WriteLine("Recognizer information for the shared recognizer:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.State" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt auf, wenn die Erkennung anhält, um Erkennungs- und andere Vorgänge zu synchronisieren.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Anwendungen müssen verwenden <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> , um eine laufende Instanz von anzuhalten, <xref:System.Speech.Recognition.SpeechRecognizer> bevor Sie Ihre Objekte ändern können <xref:System.Speech.Recognition.Grammar> . Wenn z. b <xref:System.Speech.Recognition.SpeechRecognizer> . angehalten ist, können Sie Objekte laden, entladen, aktivieren und deaktivieren <xref:System.Speech.Recognition.Grammar> . Das löst <xref:System.Speech.Recognition.SpeechRecognizer> Dieses Ereignis aus, wenn es bereit ist, Änderungen zu akzeptieren.  
  
 Wenn Sie einen Delegaten für ein- <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt eine Konsolenanwendung, die Objekte lädt und entlädt <xref:System.Speech.Recognition.Grammar> . Die Anwendung verwendet die- <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> Methode, um die sprach Erkennungs-Engine anzufordern, damit Sie ein Update empfangen kann. Die Anwendung lädt oder entlädt ein- <xref:System.Speech.Recognition.Grammar> Objekt.  
  
 Bei jedem Update schreibt ein Handler für das <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis den Namen und den Status der aktuell geladenen <xref:System.Speech.Recognition.Grammar> Objekte in die Konsole. Wenn Grammatiken geladen und entladen werden, erkennt die Anwendung zuerst die Namen von Farm-animals, dann die Namen der Farm-und die Namen der Früchte und dann nur die Namen der Früchte.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Create the first grammar - Farm.  
      Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
      GrammarBuilder farm = new GrammarBuilder(animals);  
      Grammar farmAnimals = new Grammar(farm);  
      farmAnimals.Name = "Farm";  
  
      // Create the second grammar - Fruit.  
      Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
      GrammarBuilder favorite = new GrammarBuilder(fruit);  
      Grammar favoriteFruit = new Grammar(favorite);  
      favoriteFruit.Name = "Fruit";  
  
      // Attach event handlers.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.RecognizerUpdateReached +=  
        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
      recognizer.StateChanged +=   
        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Load the Farm grammar.  
      recognizer.LoadGrammar(farmAnimals);  
      Console.WriteLine("Grammar Farm is loaded");  
  
      // Pause to recognize farm animals.  
      Thread.Sleep(7000);  
      Console.WriteLine();  
  
      // Request an update and load the Fruit grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.LoadGrammarAsync(favoriteFruit);  
      Thread.Sleep(5000);  
  
      // Request an update and unload the Farm grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.UnloadGrammar(farmAnimals);  
      Thread.Sleep(5000);  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  Grammar {0} is loaded and is {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Fordert an, dass die freigegebene Erkennung anhält und ihren Zustand aktualisiert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie diese Methode, um Änderungen an der freigegebenen Erkennung zu synchronisieren. Wenn Sie z. b. eine sprach Erkennungs Grammatik laden oder entladen, während die Erkennung Eingaben verarbeitet, verwenden Sie diese Methode und das- <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis, um das Anwendungsverhalten mit dem Zustand der Erkennung zu synchronisieren.  
  
 Wenn diese Methode aufgerufen wird, wird die Erkennung angehalten oder beendet, und es wird ein- <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis generiert. Ein- <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignishandler kann dann den Zustand der Erkennung in zwischen Erkennungs Vorgängen ändern.  
  
 Wenn diese Methode aufgerufen wird:  
  
-   Wenn die Erkennung keine Eingaben verarbeitet, generiert die Erkennung sofort das <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis.  
  
-   Wenn die Erkennung Eingaben verarbeitet, die aus Ruhe-oder Hintergrundrauschen bestehen, hält die Erkennung den Erkennungs Vorgang an und generiert das <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis.  
  
-   Wenn die Erkennung Eingaben verarbeitet, die nicht aus Ruhe-oder Hintergrundrauschen bestehen, schließt die Erkennung den Erkennungs Vorgang ab und generiert dann das <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis.  
  
 Während die Erkennung das <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis behandelt:  
  
-   Die Erkennung verarbeitet die Eingabe nicht, und der Wert der <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> Eigenschaft bleibt unverändert.  
  
-   Die Erkennung erfasst weiterhin Eingaben, und der Wert der <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> Eigenschaft kann sich ändern.  
  
 Verwenden Sie die-Eigenschaft, um zu ändern, ob die freigegebene Erkennung Erkennungs Vorgänge anhält, während eine Anwendung ein- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> Ereignis verarbeitet <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> .  
  
   
  
## Examples  
 Das folgende Beispiel zeigt eine Konsolenanwendung, die Objekte lädt und entlädt <xref:System.Speech.Recognition.Grammar> . Die Anwendung verwendet die- <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> Methode, um die sprach Erkennungs-Engine anzufordern, damit Sie ein Update empfangen kann. Die Anwendung lädt oder entlädt ein- <xref:System.Speech.Recognition.Grammar> Objekt.  
  
 Bei jedem Update schreibt ein Handler für das <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis den Namen und den Status der aktuell geladenen <xref:System.Speech.Recognition.Grammar> Objekte in die Konsole. Wenn Grammatiken geladen und entladen werden, erkennt die Anwendung zuerst die Namen von Farm-animals, dann die Namen der Farm-und die Namen der Früchte und dann nur die Namen der Früchte.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      recognizer = new SpeechRecognizer();  
  
      // Create the first grammar - Farm.  
      Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
      GrammarBuilder farm = new GrammarBuilder(animals);  
      Grammar farmAnimals = new Grammar(farm);  
      farmAnimals.Name = "Farm";  
  
      // Create the second grammar - Fruit.  
      Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
      GrammarBuilder favorite = new GrammarBuilder(fruit);  
      Grammar favoriteFruit = new Grammar(favorite);  
      favoriteFruit.Name = "Fruit";  
  
      // Attach event handlers.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.RecognizerUpdateReached +=  
        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
  
      // Check to see if recognizer is loaded, wait if it is not loaded.  
      if (recognizer.State != RecognizerState.Listening)  
      {  
        Thread.Sleep(5000);  
  
        // Put recognizer in listening state.  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
  
      // Load the Farm grammar.  
      recognizer.LoadGrammar(farmAnimals);  
      Console.WriteLine("Grammar Farm is loaded");  
  
      // Pause to recognize farm animals.  
      Thread.Sleep(7000);  
      Console.WriteLine();  
  
      // Request an update and load the Fruit grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.LoadGrammarAsync(favoriteFruit);  
      Thread.Sleep(5000);  
  
      // Request an update and unload the Farm grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.UnloadGrammar(farmAnimals);  
      Thread.Sleep(5000);  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    public static void recognizer_RecognizerUpdateReached(object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      // At the update, get the names and enabled status of the currently loaded grammars.  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  Grammar {0} is loaded and is {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Fordert an, dass die freigegebene Erkennung anhält und ihren Zustand aktualisiert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung das- <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis generiert, ist die- <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> von `null` .  
  
 Verwenden Sie die-oder-Methode, um ein Benutzer Token bereitzustellen <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> . Um einen audiopositions Offset anzugeben, verwenden Sie die- <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> Methode.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Benutzerdefinierte Informationen, die Informationen für den Vorgang enthalten.</param>
        <summary>Fordert an, dass die freigegebene Erkennung anhält, um den Zustand zu aktualisieren und stellt ein Benutzertoken für das zugeordnete Ereignis bereit.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung das- <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis generiert, enthält die- <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft des- <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> Parameters den Wert des- `userToken` Parameters.  
  
 Um einen audiopositions Offset anzugeben, verwenden Sie die- <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> Methode.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Benutzerdefinierte Informationen, die Informationen für den Vorgang enthalten.</param>
        <param name="audioPositionAheadToRaiseUpdate">Der Offset von der aktuellen <see cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />, um die Anforderung zu verzögern.</param>
        <summary>Fordert an, dass die freigegebene Erkennung anhält, um den Zustand zu aktualisieren und stellt ein Offset für das zugeordnete Ereignis bereit.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennung initiiert die Aktualisierungs Anforderung des Erkennungs Moduls erst, wenn das Erkennungs Modul <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> dem aktuellen <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> plus dem Wert des- `audioPositionAheadToRaiseUpdate` Parameters entspricht.  
  
 Wenn die Erkennung das- <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Ereignis generiert, enthält die- <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft des- <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> Parameters den Wert des- `userToken` Parameters.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt auf, wenn die Erkennung eine Eingabe erkennt, die sie als Sprache identifizieren kann.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die freigegebene Erkennung kann dieses Ereignis als Antwort auf die Eingabe erhöhen. Die- <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> Eigenschaft des zugeordneten- <xref:System.Speech.Recognition.SpeechDetectedEventArgs> Objekts gibt den Speicherort im Eingabestream an, in dem die Erkennung Sprache erkannt hat. Weitere Informationen finden Sie in den <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> -und- <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> Eigenschaften sowie in den <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> -und- <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> Methoden.  
  
 Wenn Sie einen Delegaten für ein- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, mit der die Ursprungs-und Zielstädte für einen Flug ausgewählt werden. Die Anwendung erkennt Ausdrücke, z. b. "Ich möchte von Miami nach Chicago fliegen."  Im Beispiel wird das- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> Ereignis verwendet, um den Zeitpunkt der Erkennung der Sprache zu melden <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> .  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=   
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt auf, wenn die Erkennung ein Wort oder Wörter erkannt hat, die möglicherweise eine Komponente von mehreren vollständigen Ausdrücken in einer Grammatik sind.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die freigegebene Erkennung kann dieses Ereignis hervorrufen, wenn die Eingabe mehrdeutig ist. Beispielsweise ist für eine sprach Erkennungs Grammatik, die das Erkennen von "New Game gefällt" oder "New Game" unterstützt, "New Game bitte" eine eindeutige Eingabe, und "New Game" ist eine mehrdeutige Eingabe.  
  
 Wenn Sie einen Delegaten für ein- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel werden Ausdrücke wie "Anzeigen der Liste der Künstler in der Kategorie" Jazz "erkannt. Im Beispiel wird das- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> Ereignis verwendet, um unvollständige Ausdrucks Fragmente in der-Konsole anzuzeigen, sobald diese erkannt werden.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=   
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechHypothesizedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt auf, wenn die Erkennung Eingaben empfängt, die mit keiner der Spracherkennungsgrammatiken übereinstimmen, die sie geladen hat.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die freigegebene Erkennung löst dieses Ereignis aus, wenn es feststellt, dass die Eingabe nicht mit ausreichendem Vertrauen zu den geladenen sprach Erkennungs Grammatiken zu vergleichen ist. Die- <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft des- <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> Objekts enthält das abgelehnte- <xref:System.Speech.Recognition.RecognitionResult> Objekt.  
  
 Die Vertrauens Schwellenwerte für die von verwaltete freigegebene Erkennung <xref:System.Speech.Recognition.SpeechRecognizer> werden einem Benutzerprofil zugeordnet und in der Windows-Registrierung gespeichert. Anwendungen sollten keine Änderungen an der Registrierung für die Eigenschaften der freigegebenen Erkennung schreiben.  
  
 Wenn Sie einen Delegaten für ein- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel werden Ausdrücke wie "Anzeigen der Liste der Künstler in der Kategorie" Jazz "oder" Anzeigen von Alben (Gospel) "erkannt. Das Beispiel verwendet einen Handler für das- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> Ereignis, um eine Benachrichtigung in der-Konsole anzuzeigen, wenn die Spracheingabe nicht mit dem Inhalt der Grammatik abgeglichen werden kann, um eine erfolgreiche Erkennung zu gewährleisten.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=   
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt auf, wenn die Erkennung Eingaben empfängt, die mit einer ihrer Spracherkennungsgrammatiken übereinstimmen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Erkennung löst das- `SpeechRecognized` Ereignis aus, wenn es mit ausreichendem Vertrauen feststellt, dass die Eingabe mit einer der geladenen und aktivierten sprach Erkennungs Grammatiken übereinstimmt. Die- <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft des- <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> Objekts enthält das akzeptierte- <xref:System.Speech.Recognition.RecognitionResult> Objekt.  
  
 Die Vertrauens Schwellenwerte für die von verwaltete freigegebene Erkennung <xref:System.Speech.Recognition.SpeechRecognizer> werden einem Benutzerprofil zugeordnet und in der Windows-Registrierung gespeichert. Anwendungen sollten keine Änderungen an der Registrierung für die Eigenschaften der freigegebenen Erkennung schreiben.  
  
 Wenn die Erkennung Eingaben empfängt, die mit einer Grammatik übereinstimmen, kann das- <xref:System.Speech.Recognition.Grammar> Objekt das- <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis erhöhen. Das <xref:System.Speech.Recognition.Grammar> -Ereignis des-Objekts <xref:System.Speech.Recognition.Grammar.SpeechRecognized> wird vor dem-Ereignis der Spracherkennung ausgelöst <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> .  
  
 Wenn Sie einen Delegaten für ein- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die eine sprach Erkennungs Grammatik lädt und die Spracheingabe für die freigegebene Erkennung, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignisse veranschaulicht, die von der Spracherkennung ausgelöst werden. Wenn die Windows-Spracherkennung nicht ausgeführt wird, wird durch das Starten dieser Anwendung auch die Windows-Spracherkennung gestartet.  
  
 Gesprochene Eingaben, z. b. "Ich möchte von Chicago nach Miami übertragen werden", löst ein- <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> Ereignis aus. Mit dem Ausdruck "Fly me from Houston to Chicago" wird kein Ereignis auslöst <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> .  
  
 Das Beispiel verwendet einen Handler für das <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> -Ereignis, um erfolgreich erkannte Ausdrücke und die in der-Konsole enthaltenen Semantik anzuzeigen.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="T:System.Speech.Recognition.SpeechRecognizedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
      </Docs>
    </Member>
    <Member MemberName="State">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerState State { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.RecognizerState State" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.State" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property State As RecognizerState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerState State { System::Speech::Recognition::RecognizerState get(); };" />
      <MemberSignature Language="F#" Value="member this.State : System.Speech.Recognition.RecognizerState" Usage="System.Speech.Recognition.SpeechRecognizer.State" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den Zustand eines <see cref="T:System.Speech.Recognition.SpeechRecognizer" />-Objekts ab.</summary>
        <value>Der Zustand des <see langword="SpeechRecognizer" />-Objekts.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese schreibgeschützte Eigenschaft gibt an, ob die freigegebene Erkennung in Windows den `Stopped` Zustand oder aufweist `Listening` . Weitere Informationen finden Sie unter der <xref:System.Speech.Recognition.RecognizerState>-Enumeration.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
      </Docs>
    </Member>
    <Member MemberName="StateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; StateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.StateChangedEventArgs&gt; StateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event StateChanged As EventHandler(Of StateChangedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::StateChangedEventArgs ^&gt; ^ StateChanged;" />
      <MemberSignature Language="F#" Value="member this.StateChanged : EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; " Usage="member this.StateChanged : System.EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event StateChanged As EventHandler(Of StateChangedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Tritt auf, wenn sich der Ausführzustand der Erkennungs-Engine von Windows Desktop Speech Technology ändert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die freigegebene Erkennung löst dieses Ereignis aus, wenn sich der Zustand der Windows-Spracherkennung in den <xref:System.Speech.Recognition.RecognizerState.Listening> Zustand oder ändert <xref:System.Speech.Recognition.RecognizerState.Stopped> .  
  
 Um den Zustand der freigegebenen Erkennung zum Zeitpunkt des Ereignisses zu erhalten, verwenden Sie die- <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.StateChangedEventArgs> . Um den aktuellen Status der freigegebenen Erkennung zu erhalten, verwenden Sie die-Eigenschaft des Erkennungs Moduls <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> .  
  
 Wenn Sie einen Delegaten für ein- <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> Ereignis erstellen, wird die Methode identifiziert, die das Ereignis behandelt. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen zu Ereignishandlerdelegaten finden Sie unter [Ereignisse und](https://go.microsoft.com/fwlink/?LinkId=162418)Delegaten.  
  
   
  
## Examples  
 Im folgenden Beispiel wird eine freigegebene Spracherkennung erstellt, und anschließend werden zwei Arten von Grammatiken zum Erkennen bestimmter Wörter und zum Akzeptieren der kostenlosen Diktat Erstellung erstellt. Im Beispiel werden alle erstellten Grammatiken asynchron in die Erkennung geladen.  Ein Handler für das- <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> Ereignis verwendet die- <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> Methode, um die Windows-Erkennung in den Modus "lauschen" zu versetzen.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted += new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized += new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Add a handler for the StateChanged event.  
      recognizer.StateChanged += new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Create "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yah}" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "nah" });  
      SemanticResultValue noValue = new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void  recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
     if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void  recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
     Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void  recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
     string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
      }  
  
      // Add exception handling code here.  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerState" />
        <altmember cref="T:System.Speech.Recognition.StateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.State" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognizer.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Entlädt alle Spracherkennungsgrammatiken aus dem freigegebenen Erkennungsmodul.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung zurzeit eine Grammatik asynchron lädt, wartet diese Methode, bis die Grammatik geladen ist, bevor alle Grammatiken der Erkennungsfunktion entlädt werden.  
  
 Verwenden Sie die-Methode, um eine bestimmte Grammatik zu entladen <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A> .  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadGrammar (grammar As Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Die zu entladene Grammatik.</param>
        <summary>Entlädt eine angegebene Spracherkennungsgrammatik aus dem freigegebenen Erkennungsmodul.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen zum Anhalten <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> der sprach Erkennungs-Engine verwenden, bevor Sie eine Grammatik laden, entladen, aktivieren oder deaktivieren. Verwenden Sie die-Methode, um alle Grammatiken zu entladen <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> .  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
      </Docs>
    </Member>
  </Members>
</Type>
